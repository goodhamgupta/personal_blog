{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"GPU goes brrr with Mojo\"\n",
        "draft: true\n",
        "toc: true\n",
        "bibliography: citations/mojo_citations.bib\n",
        "csl: citations/ieee.csl\n",
        "footnotes-hover: false\n",
        "execute:\n",
        "  echo: false\n",
        "format:\n",
        "  html:\n",
        "    highlight-style: github\n",
        "    code-fold: true\n",
        "    code-summary: \"Solution\"\n",
        "    syntax-definitions:\n",
        "      - mojo_gpu_puzzles/mojo.xml\n",
        "---"
      ],
      "id": "87d9fc67"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "This blog will be updated with puzzle solutions as they’re released. Stay tuned for more updates!\n",
        ":::\n",
        "\n",
        "After a super-long break from writing blogs, I've decided to get back to it this year.\n",
        "\n",
        "I've always been interested in systems programming, but somehow never _really_ picked it up. The rate of progress in the GenAI space has been exponential recently, with players like Google [@Google] reportedly processing 9.7 trillion tokens a month. Companies are now investing more time and resources interest in making these Large Language Modelsas fast and cheap as possible, by improving training and inference efficiency using \"moar\" compute.\n",
        "\n",
        "I briefly spoke about [GPU computing last year](https://www.figma.com/deck/Sq9frEEoTFgFWthOJ4EM5w/intro_gpu_cuda?node-id=1-37&t=VNzh9p2qKrHNSTJj-1), and finally decided to learn it this summer. The goal is to eventually be able to implement kernels for fast matmuls, softmax, and FlashAttention.\n",
        "\n",
        "## Why Mojo?\n",
        "\n",
        "I've tried learning Rust [multiple](https://github.com/goodhamgupta/rustlings) [times](https://github.com/goodhamgupta/100-exercises-to-learn-rust/), along with a few stints of trying C, C++ and Zig, but I never really felt as comfortable in these languages as I do in Python and Elixir.\n",
        "\n",
        "In early 2023, Modular announced Mojo🔥, a new systems-programming language promising:\n",
        "\n",
        "- Python-like syntax\n",
        "- Support for both CPU and GPU architectures\n",
        "- Kernel autofusion\n",
        "- Builds on MLIR\n",
        "- Traits and bounds checking\n",
        "- Interopeability with PTX, Python, C\n",
        "\n",
        "Modular has since announced Max, their AI inference platform, built on Mojo. The released [all kernels](https://github.com/modular/modular/tree/main/max/kernels) available as part of the platform, along with their own version[@modularpuzzles] of Sasha Rush's GPU Puzzles [@GPUPuzzles] in Mojo. IMO, their kernels were much easier to read compared to CUDA/Triton implementations, so I decided to learn a bit more about how to write these kernels.\n",
        "\n",
        "## GPUs 101 {#gpu-memory}\n",
        "\n",
        "Not sure what to put in here. Skipping for now.\n",
        "\n",
        "![](mojo_gpu_puzzles/gpu_memory.png)\n",
        "\n",
        "![](mojo_gpu_puzzles/gpu_flow_hierachy.png){fig-align=\"center\"}\n",
        "\n",
        "![](mojo_gpu_puzzles/program_flow.png)\n",
        "\n",
        "## Infrastructure\n",
        "\n",
        "If you plan on solving these puzzles, remember to pick a [compatible GPU](https://docs.modular.com/max/faq/#gpu-requirements) and follow the [setup instructions](https://builds.modular.com/puzzles/howto.html)\n",
        "\n",
        "I completed the puzzles on a instance with a RTX4090 Ti chip, rented via [Prime Intellect](https://www.primeintellect.ai/) at **0.22 $/hr**!\n",
        "\n",
        "## Part 1: GPU Fundamentals\n",
        "\n",
        "The Modular team has created beautiful [Manim](https://github.com/ManimCommunity/manim) visualizations for each puzzle, making the concepts much more intuitive. I'll walk through these visualizations as we tackle each problem.\n",
        "\n",
        "### [Puzzle 1: Map](https://builds.modular.com/puzzles/puzzle_01/puzzle_01.html) {#puzzle-01}\n",
        "\n",
        "In this puzzle, we aim to add a scalar to a vector. Specifically, we want to use a separate thread for each element in the vector, add the scalar, and write the result to the output memory.\n",
        "\n",
        "When we create the kernel, the scalar will be effectively \"broadcast\" or expanded to match the shape of the input vector. This allows each element of the vector to be independently added with the scalar value in parallel by its dedicated thread, following the [broadcasting rules](https://docs.pytorch.org/docs/stable/notes/broadcasting.html).\n",
        "\n",
        "![](mojo_gpu_puzzles/p01_vector_addition.png){fig-align=\"middle\"}\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "```{.mojo filename=\"p01.mojo\"}\n",
        "fn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):\n",
        "    i = thread_idx.x\n",
        "    out[i] = a[i] + 10\n",
        "```\n",
        "\n",
        "```bash\n",
        "pixi run p01\n",
        "# out: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "# expected: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 2: Zip](https://builds.modular.com/puzzles/puzzle_02/puzzle_02.html) {#puzzle-02}\n",
        "\n",
        "This is an extension of the map puzzle. Now, we aim to add 2 tensors together.\n",
        "\n",
        "![](mojo_gpu_puzzles/p02.png)\n",
        "\n",
        "As in puzzle 1, the aim is to use one individual thread for elements at a specific index in both vectors.\n",
        "\n",
        "![](mojo_gpu_puzzles/p02_thread.png)\n",
        "\n",
        "Note that we assume the entire array will fit within a single block, which is why there is no code for boundary checking, edge cases, etc.\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "```{.mojo filename=\"p02.mojo\"}\n",
        "fn add(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    b: UnsafePointer[Scalar[dtype]],\n",
        "):\n",
        "    i = thread_idx.x\n",
        "    out[i] = a[i] + b[i]\n",
        "```\n",
        "\n",
        "```bash\n",
        "pixi run p02\n",
        "# a: HostBuffer([0.0, 1.0, 2.0, 3.0])\n",
        "# b: HostBuffer([0.0, 1.0, 2.0, 3.0])\n",
        "# out: HostBuffer([0.0, 2.0, 4.0, 6.0])\n",
        "# expected: HostBuffer([0.0, 2.0, 4.0, 6.0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 3: Guards](https://builds.modular.com/puzzles/puzzle_03/puzzle_03.html) {#puzzle-03}\n",
        "\n",
        "The only difference between this puzzle and [Puzzle 1](#puzzle-01) is that now it's possible that the size of the GPU block is larger than the given input.\n",
        "\n",
        "In GPU programming, \"guards\" refer to conditional statements that check if a thread should perform work based on its index. GPUs launch threads in fixed-size groups (blocks), and often these blocks contain more threads than elements in our array.\n",
        "\n",
        "In this case, we need to check if the current thread index is valid before applying our computation on the vector. Without this guard, threads with indices beyond our array bounds would cause memory access violations.\n",
        "\n",
        "![](mojo_gpu_puzzles/p03.png)\n",
        "\n",
        "The image above illustrates how some threads have indices that exceed the array size and must be prevented from accessing memory.\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "```{.mojo filename=\"p03.mojo\"}\n",
        "fn add_10_guard(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    i = thread_idx.x\n",
        "    if i < size:\n",
        "        out[i] = a[i] + 10\n",
        "```\n",
        "\n",
        "Note that the size of the array is also sent as input to the kernel, as computing it in the kernel would defeat the purpose of parallelisation. While these conditional checks are necessary for correctness, they can introduce some performance overhead due to thread divergence within warps. We'll cover this in more detail shortly.\n",
        "\n",
        "```bash\n",
        "pixi run p03\n",
        "# in: HostBuffer([0.0, 1.0, 2.0, 3.0])\n",
        "# out: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "# expected: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 4: 2D Map](https://builds.modular.com/puzzles/puzzle_04/puzzle_04.html) {#puzzle-04}\n",
        "\n",
        "Similar to [Puzzle 2](#puzzle-02), instead of operating on scalars with 1D tensors, we will now use 2D tensors.\n",
        "\n",
        "Mojo, similar to CUDA, typically uses [row-major](https://en.wikipedia.org/wiki/Row-_and_column-major_order) order for array storage, meaning data is stored sequentially by rows in memory.\n",
        "\n",
        "![](mojo_gpu_puzzles/p04_row_col_major.png)\n",
        "\n",
        "Given the row-major format, the elements are accessed using the formula:\n",
        "\n",
        "$$\n",
        "A_{R,C} = R*\\text{size\\_of\\_array} + C\n",
        "$$\n",
        "\n",
        "#### Raw Memory Approach\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "```{.mojo filename=\"p04.mojo\"}\n",
        "fn add_10_2d(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    row = thread_idx.y\n",
        "    col = thread_idx.x\n",
        "    if row < size and col < size:\n",
        "        out[row * size + col] = a[row*size+col] + 10\n",
        "\n",
        "````\n",
        "\n",
        "```bash\n",
        "pixi run p04\n",
        "# in: HostBuffer([0.0, 1.0, 2.0, 3.0]) -- shaped as 2x2 row-major\n",
        "# out: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "# expected: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "````\n",
        "\n",
        "</details>\n",
        "\n",
        "#### LayoutTensor\n",
        "\n",
        "LayoutTensor[@llvmlayouttensor] is Mojo's abstraction to work on a Tensor.\n",
        "\n",
        "Specifically, LayoutTensor aims to provide:\n",
        "\n",
        "- High level primitive to perform operations on tiles.\n",
        "- Flexible memory layouts, with support for row-based, column-based and tiled organisation of data in memory.\n",
        "- Expose functions/parameters to enable auto-tuning or manual experimentation.\n",
        "- Access to hardware without inline assembly.\n",
        "\n",
        "Mojo(and LayoutTensor) follow this \"parameter syntax\"[@mojotalk], which is similar to how C++ templates are defined. This was a bit difficult for me to grasp since I don't have a C++ background, and caused a few troubles in the upcoming puzzles. I was happy to learn that I'm not the only one struggling with it though![@jeffniutriton] .\n",
        "\n",
        "![](mojo_gpu_puzzles/p04_parameter_syntax.png)\n",
        "\n",
        "The features that looked most interesting to me are:\n",
        "\n",
        "- Natural Indexing: Index a element using the format `A[row, col]`\n",
        "- Automatic Bounds Checking: I've (ab)used this feature in the upcoming puzzles.\n",
        "\n",
        "Some examples of [LayoutTensor in practice](https://builds.modular.com/puzzles/puzzle_04/introduction_layout_tensor.html#basic-usage-example):\n",
        "\n",
        "```{.mojo filename=layout_tensor.mojo}\n",
        "from layout import Layout, LayoutTensor\n",
        "\n",
        "# Define layout\n",
        "alias HEIGHT = 2\n",
        "alias WIDTH = 3\n",
        "alias layout = Layout.row_major(HEIGHT, WIDTH)\n",
        "\n",
        "# Create tensor\n",
        "tensor = LayoutTensor[dtype, layout](buffer.unsafe_ptr())\n",
        "\n",
        "# Access elements naturally\n",
        "tensor[0, 0] = 1.0  # First element\n",
        "tensor[1, 2] = 2.0  # Last element\n",
        "\n",
        "# Column-major layout\n",
        "layout_col = Layout.col_major(HEIGHT, WIDTH)\n",
        "\n",
        "# Tiled layout (for better cache utilization)\n",
        "layout_tiled = tensor.tiled[4, 4](HEIGHT, WIDTH)\n",
        "```\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "```{.mojo filename=\"p04.mojo\"}\n",
        "fn add_10_2d(\n",
        "    out: LayoutTensor[mut=True, dtype, layout],\n",
        "    a: LayoutTensor[mut=True, dtype, layout],\n",
        "    size: Int,\n",
        "):\n",
        "    row = thread_idx.y\n",
        "    col = thread_idx.x\n",
        "    # NOTE: With layout tensor, this is not really necessary, but it helps prevent unwanted memory access\n",
        "    if row < size and col < size: \n",
        "        out[row, col] = a[row, col] + 10.0\n",
        "\n",
        "````\n",
        "\n",
        "```bash\n",
        "pixi run p04_layout_tensor\n",
        "# in: HostBuffer([0.0, 1.0, 2.0, 3.0])\n",
        "# out shape: 2 x 2\n",
        "# out: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "# expected: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "````\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 5: Broadcast](https://builds.modular.com/puzzles/puzzle_05/puzzle_05.html) {#puzzle-05}\n",
        "\n",
        "We aim to broadcast the addition operation over two vectors. Following the [broadcasting rules](https://docs.pytorch.org/docs/stable/notes/broadcasting.html), the result will be an outer-product of the given vectors.\n",
        "\n",
        "![](mojo_gpu_puzzles/p05_vector_addition.png){fig-align=\"center\" height=600}\n",
        "\n",
        "#### Raw Memory Version\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "```{.mojo filename=p05.mojo}\n",
        "fn broadcast_add(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    b: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    row = thread_idx.y\n",
        "    col = thread_idx.x\n",
        "    if row < size and col < size:\n",
        "        out[row*size + col] = a[row] + b[col]\n",
        "\n",
        "````\n",
        "\n",
        "```bash\n",
        "pixi run p05\n",
        "# in a: HostBuffer([0.0, 1.0])\n",
        "# in b: HostBuffer([0.0, 1.0])\n",
        "# out: HostBuffer([0.0, 1.0, 1.0, 2.0])\n",
        "# expected: HostBuffer([0.0, 1.0, 1.0, 2.0])\n",
        "````\n",
        "\n",
        "</details>\n",
        "\n",
        "#### Layout Tensor\n",
        "\n",
        "Since we know the inputs are 1D vectors, we use only one dimension from each of the vectors, and set the other to 0 i.e the first element.\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "```{.mojo filename=p05_layout_tensor.mojo}\n",
        "fn broadcast_add[\n",
        "    out_layout: Layout,\n",
        "    a_layout: Layout,\n",
        "    b_layout: Layout,\n",
        "](\n",
        "    out: LayoutTensor[mut=True, dtype, out_layout],\n",
        "    a: LayoutTensor[mut=False, dtype, a_layout],\n",
        "    b: LayoutTensor[mut=False, dtype, b_layout],\n",
        "    size: Int,\n",
        "):\n",
        "    row = thread_idx.y\n",
        "    col = thread_idx.x\n",
        "    if row < size and col < size:\n",
        "        out[row, col] = a[0, row] + b[col, 0]\n",
        "\n",
        "````\n",
        "\n",
        "```bash\n",
        "pixi run p05_layout_tensor\n",
        "# in a: HostBuffer([0.0, 1.0])\n",
        "# in b: HostBuffer([0.0, 1.0])\n",
        "# out shape: 2 x 2\n",
        "# out: HostBuffer([0.0, 1.0, 1.0, 2.0])\n",
        "# expected: HostBuffer([0.0, 1.0, 1.0, 2.0])\n",
        "````\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 6: Blocks](https://builds.modular.com/puzzles/puzzle_06/puzzle_06.html) {#puzzle-06}\n",
        "\n",
        "Building on Puzzles 4[#puzzle-04] and 5[#puzzle-5], we now aim to add a scalar to a tensor. We also have the addtional restriction around having fewer threads than the elements in our array, per block. This means that now apart from using the local indices of the current thread(`thread_idx.y` and `thread_idx.x`), we now also need to identify the current block, using `block_idx.y` and `block_idx.x`. The formula for calculating the index, in row-major format, is:\n",
        "\n",
        "$$\n",
        "idx = block\\_idx.x * block\\_dim.x + thread\\_idx.x\n",
        "$$\n",
        "\n",
        "![](mojo_gpu_puzzles/p06.png){fig-align=\"center\" height=600}\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "```{.mojo filename=p06.mojo}\n",
        "alias SIZE = 9\n",
        "alias BLOCKS_PER_GRID = (3, 1)\n",
        "alias THREADS_PER_BLOCK = (4, 1)\n",
        "alias dtype = DType.float32\n",
        "\n",
        "\n",
        "fn add_10_blocks(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    i = block_dim.x * block_idx.x + thread_idx.x\n",
        "    if i < size:\n",
        "        out[i] = a[i] + 10\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 7: 2D Blocks](https://builds.modular.com/puzzles/puzzle_07/puzzle_07.html) {#puzzle-07}\n",
        "\n",
        "As the title suggests, we now have a 2D structure for both blocks and grids, and the number of threads per block is lesser than the total number of elements in the input tensor.\n",
        "\n",
        "![](mojo_gpu_puzzles/p07.png){fig-align=\"center\"}\n",
        "\n",
        "#### Raw Memory\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "````{.mojo filename=p07.mojo}\n",
        "alias SIZE = 5\n",
        "alias BLOCKS_PER_GRID = (2, 2)\n",
        "alias THREADS_PER_BLOCK = (3, 3)\n",
        "alias dtype = DType.float32\n",
        "\n",
        "\n",
        "fn add_10_blocks_2d(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    row = block_dim.y * block_idx.y + thread_idx.y\n",
        "    col = block_dim.x * block_idx.x + thread_idx.x\n",
        "    if row < size and col < size:\n",
        "        out[row * size + col] = a[row * size + col] + 10.0\n",
        "\n",
        "```bash\n",
        "pixi run p07\n",
        "# out: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "# expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "````\n",
        "\n",
        "````\n",
        "\n",
        "</details>\n",
        "\n",
        "#### Layout Tensor\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "```{.mojo filename=p07.mojo}\n",
        "alias SIZE = 9\n",
        "alias BLOCKS_PER_GRID = (3, 1)\n",
        "alias THREADS_PER_BLOCK = (4, 1)\n",
        "alias dtype = DType.float32\n",
        "\n",
        "\n",
        "fn add_10_blocks(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    i = block_dim.x * block_idx.x + thread_idx.x\n",
        "    if i < size:\n",
        "        out[i] = a[i] + 10\n",
        "````\n",
        "\n",
        "```bash\n",
        "pixi run p07_layout_tensor\n",
        "# out: 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "# expected: 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "# 11.0 11.0 11.0 11.0 11.0\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 8: Shared Memory](https://builds.modular.com/puzzles/puzzle_08/puzzle_08.html) {#puzzle-08}\n",
        "\n",
        "In this puzzle we leverage shared memory (SRAM). Like [Puzzle 7](#puzzle-07), we add a scalar to a 2D tensor, but now each block has fewer threads than there are input elements.\n",
        "\n",
        "As shown [above](#gpu-memory), SRAM is orders of magnitude faster than DRAM. Accessing global memory directly is slow, so we first load data into shared memory—then perform our computations for much faster access.\n",
        "\n",
        "Although this input is too small to reveal a noticeable speedup, the advantage of shared memory becomes substantial as array sizes increase.\n",
        "\n",
        "Now, because our operations depend on all records being available in shared memory, we need to wait for all threads in a block to write data to the shared memory before we can access it. Failure to do this can lead to deadlocks or undefined behaviour. Hence, we need **synchronisation**!\n",
        "\n",
        "Mojo has support for all the common [synchronisation primitives](https://docs.modular.com/mojo/stdlib/gpu/sync/#functions), similar to [CUDA primitives](https://nvidia.github.io/cccl/libcudacxx/extended_api/synchronization_primitives.html). For this puzzle, we need to use the `barrier` synchronisation, which is the same as `_syncThreads()` in CUDA: Ensure all threads within a thread block reach the barrier before any can proceed.\n",
        "\n",
        "![](mojo_gpu_puzzles/p08.png){fig-align=\"center\"}\n",
        "\n",
        "#### Raw memory\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "```{.mojo filename=p08.mojo}\n",
        "alias TPB = 4\n",
        "alias SIZE = 8\n",
        "alias BLOCKS_PER_GRID = (2, 1)\n",
        "alias THREADS_PER_BLOCK = (TPB, 1)\n",
        "alias dtype = DType.float32\n",
        "\n",
        "\n",
        "fn add_10_shared(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    shared = stack_allocation[\n",
        "        TPB,\n",
        "        Scalar[dtype],\n",
        "        address_space = AddressSpace.SHARED,\n",
        "    ]()\n",
        "    global_i = block_dim.x * block_idx.x + thread_idx.x\n",
        "    local_i = thread_idx.x\n",
        "    # local data into shared memory\n",
        "    if global_i < size:\n",
        "        shared[local_i] = a[global_i]\n",
        "\n",
        "    # wait for all threads to complete\n",
        "    # works within a thread block\n",
        "    barrier()\n",
        "\n",
        "    if global_i < size:\n",
        "        out[global_i] = shared[local_i] + 10.0\n",
        "```\n",
        "\n",
        "```bash\n",
        "pixi run p08\n",
        "# out: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "# expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "#### LayoutTensor\n",
        "\n",
        "Key difference here is to use [LayoutTensorBuild instead of stack_allocation](https://builds.modular.com/puzzles/puzzle_08/layout_tensor.html#key-differences-from-raw-approach) to allocate shared memory.\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "```{.mojo filename=p08_layout_tensor.mojo}\n",
        "alias TPB = 4\n",
        "alias SIZE = 8\n",
        "alias BLOCKS_PER_GRID = (2, 1)\n",
        "alias THREADS_PER_BLOCK = (TPB, 1)\n",
        "alias dtype = DType.float32\n",
        "alias layout = Layout.row_major(SIZE)\n",
        "\n",
        "\n",
        "fn add_10_shared_layout_tensor[\n",
        "    layout: Layout\n",
        "](\n",
        "    out: LayoutTensor[mut=True, dtype, layout],\n",
        "    a: LayoutTensor[mut=True, dtype, layout],\n",
        "    size: Int,\n",
        "):\n",
        "    # Allocate shared memory using tensor builder\n",
        "    shared = tb[dtype]().row_major[TPB]().shared().alloc()\n",
        "\n",
        "    global_i = block_dim.x * block_idx.x + thread_idx.x\n",
        "    local_i = thread_idx.x\n",
        "\n",
        "    if global_i < size:\n",
        "        shared[local_i] = a[global_i]\n",
        "\n",
        "    barrier()\n",
        "\n",
        "    if global_i < size:\n",
        "        out[global_i] = shared[local_i] + 10.0\n",
        "```\n",
        "\n",
        "```bash\n",
        "pixi run p08_layout_tensor\n",
        "# out: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "# expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 2: GPU Algorithms\n",
        "\n",
        "This section primarily aims to implement basic algorithms used in building models, such as pooling, convolutions, etc.\n",
        "\n",
        "### [Puzzle 9: Pooling](https://builds.modular.com/puzzles/puzzle_09/puzzle_09.html) {#puzzle-09}\n",
        "\n",
        "Pooling is a classic trick in neural networks for shrinking down your data—think of it as a way to \"summarize\" regions of an image or tensor. Instead of looking at every single pixel, pooling (like max or average pooling) slides a window over your data and grabs just the most important info from each patch. On GPUs, pooling is a perfect fit: each thread can independently process a window, so you get massive parallelism and a big speedup compared to CPUs.\n",
        "\n",
        "This puzzle is a bit different compared to traditional pooling: Instead of having a \"kernel\", each output element is the running sum of the all the elements in the current window.\n",
        "\n",
        "![](mojo_gpu_puzzles/p09.png)\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "```{.mojo filename=p09.mojo}\n",
        "alias TPB = 8\n",
        "alias SIZE = 8\n",
        "alias BLOCKS_PER_GRID = (1, 1)\n",
        "alias THREADS_PER_BLOCK = (TPB, 1)\n",
        "alias dtype = DType.float32\n",
        "\n",
        "\n",
        "fn pooling(\n",
        "    out: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    shared = stack_allocation[\n",
        "        TPB,\n",
        "        Scalar[dtype],\n",
        "        address_space = AddressSpace.SHARED,\n",
        "    ]()\n",
        "    global_i = block_dim.x * block_idx.x + thread_idx.x\n",
        "    local_i = thread_idx.x\n",
        "    if global_i < size:\n",
        "        shared[local_i] = a[global_i]\n",
        "\n",
        "    barrier()\n",
        "\n",
        "    if global_i < size:\n",
        "        if local_i - 2 >= 0:\n",
        "            out[global_i] = (\n",
        "                shared[local_i - 2] + shared[local_i - 1] + shared[local_i]\n",
        "            )\n",
        "        elif local_i - 1 >= 0:\n",
        "            out[global_i] = shared[local_i - 1] + shared[local_i]\n",
        "        else:\n",
        "            out[global_i] = shared[local_i]\n",
        "```\n",
        "\n",
        "```bash\n",
        "pixi run p09\n",
        "# out: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "# expected: HostBuffer([11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "The LayoutTensor version is nearly identical to the Raw Memory approach, so we'll omit the code here for brevity.\n",
        "\n",
        "### [Puzzle 10: Dot Product](https://builds.modular.com/puzzles/puzzle_10/puzzle_10.html) {#puzzle-10}\n",
        "\n",
        "The Dot Product of two vectors $a$ and $b$ is defined as [@wikipediadotproduct]:\n",
        "\n",
        "$$\n",
        "c = a \\cdot b = \\sum_{i=0}^{n-1} a_i b_i\n",
        "$$\n",
        "\n",
        "Similar to the previous puzzles, we can implement the dot-product by copying data to the shared memory, and running our operations on it.\n",
        "\n",
        "![](mojo_gpu_puzzles/p10.png){fig-align='center'}\n",
        "\n",
        "To implement dot product efficiently on a GPU, we will use **parallel reduction**. This is a classic pattern for aggregating values (sum, min, max, etc.) across a large array using many threads. The general flow is:\n",
        "\n",
        "- Each thread computes a partial sum of the dot product (multiplying its assigned elements from `a` and `b`).\n",
        "- All threads store their partial results in shared memory.\n",
        "- In each reduction step:\n",
        "  - The number of active threads is halved.\n",
        "  - Each active thread adds its value to the value from another thread (usually its \"neighbor\" at a fixed offset).\n",
        "  - A barrier ensures all threads have updated shared memory before the next step.\n",
        "- This repeats for log₂(n) steps, until only one thread holds the final result.\n",
        "\n",
        "This pattern is fast, highly parallel, and used everywhere in GPU programming for reductions (sum, min, max, etc).\n",
        "\n",
        "::: {layout-nrow=3}\n",
        "\n",
        "![](mojo_gpu_puzzles/pr_p1.png){fig-align='center' width=\"500\"}\n",
        "\n",
        "![](mojo_gpu_puzzles/pr_p2.png){fig-align='center' width=\"500\"}\n",
        "\n",
        "![](mojo_gpu_puzzles/pr_p3.png){fig-align='center' width=\"500\"}\n",
        "\n",
        ":::\n",
        "\n",
        "#### Raw Memory\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "```{.mojo filename=p10.mojo}\n",
        "fn dot_product(\n",
        "    output: UnsafePointer[Scalar[dtype]],\n",
        "    a: UnsafePointer[Scalar[dtype]],\n",
        "    b: UnsafePointer[Scalar[dtype]],\n",
        "    size: Int,\n",
        "):\n",
        "    shared = stack_allocation[\n",
        "        TPB, Scalar[dtype], address_space = AddressSpace.SHARED\n",
        "    ]()\n",
        "\n",
        "    global_idx = block_dim.x * block_idx.x + thread_idx.x\n",
        "    local_idx = thread_idx.x\n",
        "    if global_idx < size:\n",
        "        shared[local_idx] = a[global_idx] * b[global_idx]\n",
        "\n",
        "    barrier()\n",
        "\n",
        "    stride = TPB // 2\n",
        "    while(stride > 0):\n",
        "        if local_idx < stride:\n",
        "            shared[local_idx] += shared[local_idx + stride]\n",
        "        \n",
        "        barrier()\n",
        "        stride = stride // 2\n",
        "    \n",
        "    # only allow thread 0 to write result\n",
        "    if local_idx == 0:\n",
        "        output[0] = shared[0]\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "**Note**: Instead of doing the parallel reduction, we could also implement the solution using a loop:\n",
        "\n",
        "```diff\n",
        "-    stride = TPB // 2\n",
        "-    while(stride > 0):\n",
        "-        if local_idx < stride:\n",
        "-            shared[local_idx] += shared[local_idx + stride]\n",
        "-        \n",
        "-        barrier()\n",
        "-        stride = stride // 2\n",
        "-    \n",
        "-    # only allow thread 0 to write result\n",
        "-    if local_idx == 0:\n",
        "-        output[0] = shared[0]\n",
        "+    if global_idx < size:\n",
        "+        for idx in range(size):\n",
        "+            output[0] = output[0] + shared[idx]\n",
        "```\n",
        "\n",
        "\n",
        "While this approach also gives the correct answer for this puzzle, it has multiple problems:\n",
        "\n",
        "- **Race conditions**: Multiple threads would simultaneously try to update output[0] without synchronization, causing lost updates.\n",
        "- **Thread divergence**: When threads in a warp take different execution paths (some running the loop, others not), the GPU must serialize execution, destroying parallelism.\n",
        "- **Redundant computation**: Every qualifying thread would compute the exact same sum over the entire array, wasting compute resources.\n",
        "- **Memory bottleneck**: Repeated atomic operations to the same memory location (output[0]) create severe contention.\n",
        "\n",
        "\n",
        "#### LayoutTensor\n",
        "\n",
        "alias TPB = 8\n",
        "alias SIZE = 8\n",
        "alias BLOCKS_PER_GRID = (1, 1)\n",
        "alias THREADS_PER_BLOCK = (SIZE, 1)\n",
        "alias dtype = DType.float32\n",
        "alias layout = Layout.row_major(SIZE)\n",
        "alias out_layout = Layout.row_major(1)\n",
        "\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "\n",
        "```{.mojo filename=p10.mojo}\n",
        "fn dot_product[\n",
        "    in_layout: Layout, out_layout: Layout\n",
        "](\n",
        "    output: LayoutTensor[mut=True, dtype, out_layout],\n",
        "    a: LayoutTensor[mut=True, dtype, in_layout],\n",
        "    b: LayoutTensor[mut=True, dtype, in_layout],\n",
        "    size: Int,\n",
        "):\n",
        "    # Use LayoutTensorBuilder instead of stack_allocation\n",
        "    shared = tb[dtype]().row_major[TPB]().shared().alloc()\n",
        "    global_idx = block_dim.x * block_idx.x + thread_idx.x\n",
        "    local_idx = thread_idx.x\n",
        "\n",
        "    if global_idx < size:\n",
        "        shared[local_idx] = a[global_idx] * b[global_idx]\n",
        "\n",
        "    barrier()\n",
        "\n",
        "    stride = TPB // 2\n",
        "    while(stride > 0):\n",
        "        if local_idx < stride:\n",
        "            shared[local_idx] += shared[local_idx + stride]\n",
        "        \n",
        "        barrier()\n",
        "        stride = stride // 2\n",
        "    \n",
        "    # only allow thread 0 to write result\n",
        "    if local_idx == 0:\n",
        "        output[0] = shared[0]\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "### [Puzzle 11: 1D Convolution](https://builds.modular.com/puzzles/puzzle_11/puzzle_11.html) {#puzzle-11}\n",
        "\n",
        "Picture sliding a magnifying glass along a long strip of film.\n",
        "That’s exactly what a 1-D convolution does to any 1-D signal—audio samples, DNA bases, even bytes of log data.\n",
        "\n",
        "- The kernel (a small weight vector) glides over the sequence one step at a time (or more if you set stride > 1).\n",
        "- At each stop it multiplies the local window by its weights, sums the result, and drops a single number into the output map.\n",
        "- Stack layers and you grow the “what can I see at once?” window (the receptive field) without blowing up parameters.\n",
        "\n",
        "**Why bother?**\n",
        "\n",
        "- **Speed**: A conv layer is just a batched matrix-mul—GPU catnip.\n",
        "- **Locality first, context later**: Early layers grab short-range patterns (phonemes, k-mers). Deeper layers stitch them into bigger motifs (words, promoters).\n",
        "- **Channels generalize it**: You convolve along length, but for each input channel you keep separate weights, sum across channels, and spit out new feature maps. Same trick as 2-D CNNs, just flattened.\n",
        "\n",
        "For a better picture, see Ayush's blog[@thakur_convolutions] on convolutions.\n",
        "\n",
        "The convolution operation can be defined as:\n",
        "$$\n",
        "    (input\\_signal\\_a * kernel\\_b)[i] = \\sum_{j=0}^{\\text{kernel\\_size}-1} input\\_signal\\_a[i + j] * kernel\\_b[j]\n",
        "$$\n",
        "\n",
        "#### Simple: Single Block with Shared Memory\n",
        "\n",
        "For this version, we assume that we only have a single block, and both the input data and the kernel fit within a block.\n",
        "\n",
        "<details open>\n",
        "<summary> **Solution** </summary>\n",
        "\n",
        "```{.mojo filename=p11.mojo}\n",
        "alias TPB = 8\n",
        "alias SIZE = 6\n",
        "alias CONV = 3\n",
        "alias BLOCKS_PER_GRID = (1, 1)\n",
        "alias THREADS_PER_BLOCK = (TPB, 1)\n",
        "alias dtype = DType.float32\n",
        "alias in_layout = Layout.row_major(SIZE)\n",
        "alias out_layout = Layout.row_major(SIZE)\n",
        "alias conv_layout = Layout.row_major(CONV)\n",
        "\n",
        "\n",
        "fn conv_1d_simple[\n",
        "    in_layout: Layout, out_layout: Layout, conv_layout: Layout\n",
        "](\n",
        "    output: LayoutTensor[mut=False, dtype, out_layout],\n",
        "    a: LayoutTensor[mut=False, dtype, in_layout],\n",
        "    b: LayoutTensor[mut=False, dtype, conv_layout],\n",
        "):\n",
        "    global_i = block_dim.x * block_idx.x + thread_idx.x\n",
        "    local_i = thread_idx.x\n",
        "    shared_a = tb[dtype]().row_major[TPB]().shared().alloc()\n",
        "    # This is oversized! I've explained it later :)\n",
        "    shared_b = tb[dtype]().row_major[TPB]().shared().alloc()\n",
        "\n",
        "    # This can also be optimised, as shown later.\n",
        "    if global_i < SIZE:\n",
        "        shared_a[local_i] = a[global_i]\n",
        "        shared_b[local_i] = b[global_i]\n",
        "    \n",
        "\n",
        "    barrier()\n",
        "\n",
        "    if global_i < SIZE:\n",
        "\n",
        "        # Ensure the local var has the same type as the output\n",
        "        # to avoid type casting errors.\n",
        "        var local_sum: output.element_type = 0\n",
        "\n",
        "        # Perform loop unrolling.\n",
        "        @parameter\n",
        "        for j in range(CONV):\n",
        "            if local_i + j < SIZE:\n",
        "                local_sum += shared_a[local_i + j] * shared_b[j]\n",
        "            barrier()\n",
        "        \n",
        "        output[global_i] = local_sum\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "I deliberately allocate `shared_b` with the block width (`TPB`) instead of the filter length (`CONV`). The extra space isn’t needed for correctness—the kernel only touches the first `CONV` elements—but it nicely demonstrates `LayoutTensor`’s masking: out-of-range indices are silently ignored. This trick keeps the buffer shape uniform across puzzles without cluttering the code with edge-case branches. The flip side is a bit of wasted shared memory, which can pinch if your kernel is already pushing the SRAM limit.  \n",
        "\n",
        "```diff\n",
        "-    shared_b = tb[dtype]().row_major[TPB]().shared().alloc()\n",
        "+    # Allocate exactly CONV elements → smaller shared-mem footprint\n",
        "+    shared_b = tb[dtype]().row_major[CONV]().shared().alloc()\n",
        "...\n",
        "\n",
        "-    if global_i < SIZE:\n",
        "-        shared_a[local_i] = a[global_i]\n",
        "-        shared_b[local_i] = b[global_i]\n",
        "+    if global_i < SIZE:\n",
        "+        shared_a[local_i] = a[global_i]\n",
        "+    if global_i < CONV:\n",
        "+        shared_b[local_i] = b[global_i]\n",
        "```\n",
        "\n",
        "#### \\@parameter : Loop Unrolling\n",
        "\n",
        "[`@parameter`](https://docs.modular.com/mojo/manual/decorators/parameter/) is Mojo's implementation of **loop unrolling**. This has the same functionality as `pragma unroll(N)` in CUDA.\n",
        "\n",
        "When unroll is in effect, the optimizer determines and applies the best unrolling factor for each loop; in some cases, the loop control might be modified to avoid unnecessary branching. The compiler remains the final arbiter of whether the loop is unrolled[@nvidiapragmaunroll].\n",
        "\n",
        "`@parameter` isn’t limited to loops/branches—you can slap it on an inner\n",
        "function and Mojo will build a **parametric closure**, defined as[@mojoparameter]: \n",
        "\n",
        "> A parametric closure is a nested function decorated with `@parameter`.\n",
        "> Any values it captures from the surrounding scope are treated as\n",
        "> compile-time constants.  The compiler materialises one specialised\n",
        "> version of the closure for every distinct set of captured values\n",
        "> (docs: decorators/parameter → “Parametric closure”).\n",
        "\n",
        "Example:\n",
        "\n",
        "```{.mojo filename=parametric_closure.mojo}\n",
        "fn make_shift(off: Int):\n",
        "    @parameter            # ← specialised per ‘off’\n",
        "    fn shift(x: Int) -> Int:\n",
        "        return x + off\n",
        "    return shift\n",
        "\n",
        "let s1 = make_shift(1)    # emits shift-$off=1\n",
        "let s4 = make_shift(4)    # emits shift-$off=4\n",
        "```\n",
        "\n",
        "No runtime captures, no heap boxing—the constant `off` is literally\n",
        "spliced into the generated IR, so calls to `s1`/`s4` inline like normal\n",
        "code and can be further unrolled or constant-folded.\n",
        "\n",
        "Why is this safe?  Mojo’s *origin* system[@mojo_lifetimes] assigns each compile-time constant its own immutable origin.\n",
        "The closure therefore can’t outlive or mutate the thing it captured;\n",
        "once the surrounding scope ends those origins die too, guaranteeing that\n",
        "the specialised code never touches expired storage.\n",
        "\n",
        "**Bottom line**: you get closure ergonomics plus “zero-cost abstraction”[@zero_cost_abstractions]\n",
        "performance—ideal for GPU kernels where every cycle and register matters.\n",
        "\n",
        "## Citation\n",
        "\n",
        "Please cite this work as:\n",
        "\n",
        "```\n",
        "Gupta, Shubham. “Mojo GPU Puzzles — Solutions & Explanations”. shubhamg.in (June 2025). https://shubhamg.in/posts/mojo-gpu-puzzles\n",
        "```\n",
        "\n",
        "Or use the BibTeX citation:\n",
        "\n",
        "\n",
        "```{citation}\n",
        "#| code-fold: true\n",
        "#| code-summary: \"Show the code\"\n",
        "@article{gupta2025mojopuzzles,\n",
        "  title   = {Mojo GPU Puzzles — Solutions \\& Explanations},\n",
        "  author  = {Gupta, Shubham},\n",
        "  journal = {shubhamg.in},\n",
        "  year    = {2025},\n",
        "  month   = {June},\n",
        "  url     = {https://shubhamg.in/posts/mojo-gpu-puzzles},\n",
        "  note    = {Original puzzles by the Modular team; this blog provides personal explanations and solutions}\n",
        "}\n",
        "```"
      ],
      "id": "24b61edd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}