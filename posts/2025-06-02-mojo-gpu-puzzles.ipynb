{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: GPU goes brrr with Mojo\n",
        "draft: true\n",
        "toc: true\n",
        "bibliography: citations/mojo_citations.bib\n",
        "csl: citations/ieee.csl\n",
        "footnotes-hover: false\n",
        "execute:\n",
        "  echo: false\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-summary: Solution\n",
        "    syntax-definitions:\n",
        "      - mojo_gpu_puzzles/mojo.xml\n",
        "---"
      ],
      "id": "e939eb5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After a super-long break from writing blogs, I've decided to get back to it this year.\n",
        "\n",
        "I've always been interested in systems programming, but somehow never _really_ picked it up. The rate of progress in the GenAI space has been exponential recently, with players like Google [@Google] reportedly processing 9.7 trillion tokens a month. Companies are now investing more time and resources interest in making these Large Language Modelsas fast and cheap as possible, by improving training and inference efficiency using \"moar\" compute.\n",
        "\n",
        "I briefly spoke about [GPU computing last year](https://www.figma.com/deck/Sq9frEEoTFgFWthOJ4EM5w/intro_gpu_cuda?node-id=1-37&t=VNzh9p2qKrHNSTJj-1), and finally decided to learn it this summer. The goal is to eventually be able to implement kernels for fast matmuls, softmax, and FlashAttention.\n",
        "\n",
        "## Why Mojo?\n",
        "\n",
        "I've tried learning Rust [multiple](https://github.com/goodhamgupta/rustlings) [times](https://github.com/goodhamgupta/100-exercises-to-learn-rust/), along with a few stints of trying C, C++ and Zig, but I never really felt as comfortable in these languages as I did in Python.\n",
        "\n",
        "In early 2023, Modular announced MojoðŸ”¥, a new systems-programming language promising:\n",
        "\n",
        "- Python-like syntax\n",
        "- Support for both CPU and GPU architectures\n",
        "- Kernel autofusion\n",
        "- Builds on MLIR\n",
        "- Traits and bounds checking\n",
        "- Interopeability with PTX\n",
        "\n",
        "Modular has since announced Max, their AI inference platform, built on Mojo. The released [all kernels](https://github.com/modular/modular/tree/main/max/kernels) available as part of the platform, along with their own version[@modularpuzzles] of Sasha Rush's GPU Puzzles [@GPUPuzzles] in Mojo. IMO, their kernels were much easier to read compared to CUDA/Triton implementations, so I decided to learn a bit more about how to write these kernels.\n",
        "\n",
        "## GPUs 101\n",
        "\n",
        "Not sure what to put in here. Skipping for now.\n",
        "\n",
        "## Infrastructure\n",
        "\n",
        "If you plan on solving these puzzles, remember to pick a [compatible GPU](https://docs.modular.com/max/faq/#gpu-requirements) and follow the [setup instructions](https://builds.modular.com/puzzles/howto.html)\n",
        "\n",
        "I completed the puzzles on a instance with a RTX4090 Ti chip, rented via [Prime Intellect](https://www.primeintellect.ai/) at **0.22 $/hr**!\n",
        "\n",
        "## Part 1: GPU Fundamentals\n",
        "\n",
        "The Modular team has created beautiful [Manim](https://github.com/ManimCommunity/manim) visualizations for each puzzle, making the concepts much more intuitive. I'll walk through these visualizations as we tackle each problem.\n",
        "\n",
        "### Puzzle 1: Map\n",
        "\n",
        "In this puzzle, we aim to add a scalar to a vector. Specifically, we want to use a separate thread for each element in the vector, add the scalar, and write the result to the output memory.\n",
        "\n",
        "When we create the kernel, the scalar will be effectively \"broadcast\" or expanded to match the shape of the input vector. This allows each element of the vector to be independently added with the scalar value in parallel by its dedicated thread, following the [broadcasting rules](https://docs.pytorch.org/docs/stable/notes/broadcasting.html).\n",
        "\n",
        "![](mojo_gpu_puzzles/p01_vector_addition.png){fig-align=\"middle\"}\n",
        "\n",
        "<details open>\n",
        "<summary> Solution </summary>\n",
        "```{.mojo filename=\"p01.mojo\"}\n",
        "fn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):\n",
        "    i = thread_idx.x\n",
        "    # FILL ME IN (roughly 1 line)\n",
        "    out[i] = a[i] + 10\n",
        "```\n",
        "\n",
        "```bash\n",
        "pixi run p01\n",
        "# out: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "# expected: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "### Puzzle 2: Zip\n",
        "\n",
        "This is an extension of the map puzzle. Now, we aim to add 2 tensors together.\n",
        "\n",
        "![](mojo_gpu_puzzles/p02.png)\n",
        "\n",
        "As in puzzle 1, the aim is to use one individual thread for elements at a specific index in both vectors.\n",
        "\n",
        "![](https://builds.modular.com/puzzles/puzzle_02/media/videos/720p30/puzzle_02_viz.gif)\n",
        "\n",
        "\n",
        "<details open>\n",
        "<summary> Solution </summary>\n",
        "```{.mojo filename=\"p02.mojo\"}\n",
        "fn add_10(out: UnsafePointer[Scalar[dtype]], a: UnsafePointer[Scalar[dtype]]):\n",
        "    i = thread_idx.x\n",
        "    # FILL ME IN (roughly 1 line)\n",
        "    out[i] = a[i] + 10\n",
        "```\n",
        "\n",
        "```bash\n",
        "pixi run p01\n",
        "# out: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "# expected: HostBuffer([10.0, 11.0, 12.0, 13.0])\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "id": "ad916769"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}