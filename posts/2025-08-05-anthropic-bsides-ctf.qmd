---
aliases:
- /ctf/2025/08/05/bsides-ctf
author: Shubham Gupta
categories:
- ctf
date: '2025-08-05'
description: ... and how to win API credits!
image: bsides-ctf/bsides-ctf.png
layout: post
title: 'Anthropic BSides CTF 2025'
toc: true
bibliography: citations/anthropic_bsides_ctf.bib
csl: citations/ieee.csl
footnotes-hover: false
execute:
  echo: false
format:
  html:
    highlight-style: gruvbox
    code-overflow: wrap
    code-fold: true
    code-summary: "Solution"

---

In May last year, I spotted a tweet from an Anthropic engineer announcing the BSides CTF:

![](anthropic_bsides_ctf/tweet.png)

I'm no CTF expert/practitioner, but I tackled a few during undergrad, with a focus on forensic puzzles. This event mixed steganography with some neural-network trivia, so it sounded like the perfect weekend project.

The challenge is available via the Wayback Machine at [Anthropic AI Bsides](https://web.archive.org/web/20240603200949/https://anthropic-at-bsides.com). Do try it out before reading further!

# Introduction

Feel free to skip ahead to the [start of the challenge](#exploration) if you're already familiar with CTFs.

## CTF?

CTF stands for **Capture The Flag**, a style of cybersecurity competition where solving technical puzzles reveals a short secret stringâ€”the *flag* - that you submit for points.  
Think of it as digital hide-and-seek: organizers hide vulnerabilities, encrypted messages, or cleverly obfuscated code; competitors hunt them down.

## CTF Styles

Not every competition runs the same playbook. Most events fall into one of three flavours:

### Jeopardy

A scoreboard with dozens of stand-alone puzzles. Solve any task in any order to reveal a _static_ flag worth fixed points.   This style is workshop / learning-friendly. 

![Jeopardy format example](anthropic_bsides_ctf/jeopardy.png)  

### Attack-Defense

Each team gets an identical (and intentionally vulnerable) service. Every "tick" you:

- Keep _your_ instance alive for _defence points_
- Exploit everyone else to steal a fresh dynamic flag for _attack points_

Patch too aggressively and you might brick your own service. Patch too cautiously and youâ€™ll bleed flags. Itâ€™s a fast-paced, chaotic, team-centric format.

![Attack-Defense format example](anthropic_bsides_ctf/ad.png)  

### Mixed / Custom

Organisers mash the two together or add story-driven twists (e.g., live red-team/blue-team, lock-picking, hardware).
This is a YOLO mode CTF. 

Most community events, including BSides, opt for classic Jeopardy because it scales well and newcomers can jump straight in. But if you ever see "A/D" or "King-of-the-Hill" on the announcement banner, expect the wilder second style.

## Flag anatomy & submission

Flags are just short strings that prove you solved or exploited something. They come in many house stylesâ€”`CTF{leet_hax0r}`, `FLAG-6f7b5eâ€¦`, `BSides[you_found_it]`â€”but the portal will show an example on each challenge page. 

![Flags](anthropic_bsides_ctf/flag.png)

In A/D games a new flag is generated every round, so automation matters.

---

With that primer out of the way, let's dive into the challengesâ€¦ 


# Initial Exploration {#exploration}

As soon as we open the website, we're greeted with a fairly simple page. 

![Landing Page](anthropic_bsides_ctf/landing.png)

After clicking around the page a bit, I looked through the source code and found this:

```html
<div class="fixed w-full h-full bg-[url('stego.png')] opacity-50 -z-10"></div>
<!-- sometimes, the answers you seek are in plain sight -->
```

Once the above image `stego.png` is downloaded, it looks as follows:

![stego.png](anthropic_bsides_ctf/stego.png)

While it looks like the image doesn't really have anything interesting, the name of the image gives us a hint: "stego" = "steganography"

## Steganography?

Steganography is the practice of hiding data in plain sight. Steganography is often embedded in images or audio[@ctf101_steganography].  

In graphics the most common trick is Least-Significant-Bit (LSB) encoding: flip only the lowest bit of each pixel's colour value.  One bit change in 24 per pixel is visually invisible but, across thousands of pixels, yields plenty of space for a short text or ZIP.

![LSB Encoding for an alphabet](anthropic_bsides_ctf/lsb.png)

**zsteg** is the CTF scalpel for pictures. It bruteforces every sane combo of  
bit-plane Ã— channel Ã— endianness Ã— encoding (ASCII/UTF-8/hex/deflate/â€¦) and flags what smells like data.

Running zsteg on the image gives the following:

```bash
docker run -it --rm -v $PWD:/data:Z sjourdan/zsteg stego.png
# b1,a,lsb,xy         .. text: "According to all known laws of aviation, there is no way a bee should be able to fly.\nIts wings are too small to get its fat little body off the ground.\nThe bee, of course, flies anyway because bees don't care what humans think is impossible.\nYellow, black"
# b3,rgba,msb,xy      .. file: MPEG ADTS, AAC, v4 LTP, 8 kHz, surround + side
# b4,rgb,msb,xy       .. file: MPEG ADTS, layer I, v2, 112 kbps, Monaural
```

The `zsteg` output format follows: `bit-plane, channels, lsb|msb, scan-order â†’ payload`

- `b1,a,lsb,xy` â€“ bit-plane 1 (the very least-significant bit) of **only the alpha channel**, read left-to-right/top-to-bottom (`xy`). Those bits spell the "Bee Movie"[@imdb_bee_movie_barry] opening monologue.
- `b3,rgba,msb,xy` â€“ 3rd bit of every RGBA channel (MSB). This is an 8 kHz AAC clip.  
- `b4,rgb,msb,xy` â€“ 4th bit of RGB (MSB). Turns into a 112 kbps MP1 file.

`zsteg` is pinpointing exactly which bits to extract, and from which colour plane to recover each hidden payload. We can extract the files using:

```bash
zsteg stego.png -E b1,a,lsb,xy > transcript.txt
zsteg stego.png -E b3,rgba,msb,xy > bee_movie.mp3
zsteg stego.png -E b4,rgb,msb,xy > bee_movie.mp1
```

## File Analysis

I spent _way too_ much time digging through the audio files, and processing them with everyone's favorite audio processing tool: **FFmpeg**

However, I couldn't obtain any results from the audio files. Based on my analysis:

- `audio1.mp3` is a file of size 0 bytes, and doesn't contain any content.
- `audio2.mp3` also seems to have a missing header. Furthermore, running `hexdump` on the file gives repeated occurences of the string "ff f7 7f".


<details closed>
<summary> **Commands** </summary>

```bash
ffmpeg -i audio2.mp3 -f null
# [mp3 @ 0x136e06be0] Format mp3 detected only with low score of 1, misdetection possible!
# [mp3float @ 0x136e07310] Header missing
# [mp3 @ 0x136e06be0] Could not find codec parameters for stream 0 (Audio: mp3 (mp3float), 0 channels, fltp): unspecified frame size
# Consider increasing the value for the 'analyzeduration' (0) and 'probesize' (5000000) options
# Input #0, mp3, from 'audio2.mp3':
#   Duration: N/A, start: 0.000000, bitrate: N/A
#   Stream #0:0: Audio: mp3 (mp3float), 0 channels, fltp
# Stream mapping:
#   Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))
# Press [q] to stop, [?] for help
# [mp3float @ 0x136e096e0] Header missing
# [aist#0:0/mp3 @ 0x136e07310] [dec:mp3float @ 0x136e09130] Error submitting packet to decoder: Invalid data found when processing input
# [aist#0:0/mp3 @ 0x136e07310] [dec:mp3float @ 0x136e09130] Decode error rate 1 exceeds maximum 0.666667
# [aist#0:0/mp3 @ 0x136e07310] [dec:mp3float @ 0x136e09130] Task finished with error code: -1145393733 (Error number -1145393733 occurred)
# [aist#0:0/mp3 @ 0x136e07310] [dec:mp3float @ 0x136e09130] Terminating thread with return code -1145393733 (Error number -1145393733 occurred)
# [graph_0_in_0:0 @ 0x600000be80b0] Neither number of channels nor channel layout specified
# Error initializing filters!
# [af#0:0 @ 0x600000eca0a0] Task finished with error code: -22 (Invalid argument)
# [af#0:0 @ 0x600000eca0a0] Terminating thread with return code -22 (Invalid argument)
# [aost#0:0/pcm_s16le @ 0x136e07fe0] Could not open encoder before EOF
# [aost#0:0/pcm_s16le @ 0x136e07fe0] Task finished with error code: -22 (Invalid argument)
# [aost#0:0/pcm_s16le @ 0x136e07fe0] Terminating thread with return code -22 (Invalid argument)
# [out#0/null @ 0x6000009c8480] Nothing was written into output file, because at least one of its streams received no packets.
# size=       0KiB time=N/A bitrate=N/A speed=N/A
# Conversion failed! 
```

```bash
hexdump -C audio2.mp3
# 00000000  ff f7 7f ff f7 7f ff f7  7f ff f7 7f ff f7 7f ff  |................|
# 00000010  f7 7f ff f7 7f ff f7 7f  ff f7 7f ff f7 7f ff f7  |................|
# 00000020  7f ff f7 7f ff f7 7f ff  f7 7f ff f7 7f ff f7 7f  |................|
# ...
```


</details>

Disappointed with the above experiments, I finally started looking at the text file for some hints, and found the following text:

```
BREAKING OUT OF THE SCRIPT
the thing you are looking for is at the regular website the challenge is on slash 
8471c9e7c8e8e5722c2c41d68575b5f3 dot zip
END BREAKING OUT OF THE SCRIPT
```

Hah! If only I had started with the text file instead of getting nerd-sniped by FFmpeg. Oh well, we move on.

# Zip File Exploration

The zipfile contains a `model.pkl`, `model.py` file and some instructions for the next task:

```txt
The next and final part of this puzzle relies on some understanding of simple
multilayer perceptron behaviors. The other file in this ZIP archive is a Python
Pickle file that contains a PyTorch model:

1. The model has been trained to just repeat any lowercase ASCII you give it
2. Except it has also been trained to output a special "flag" given the right
   password

The input to the model is one-hot encoded and shaped (B, N, V) where:

- B is the batch size
- N is the length of the sequence (which is stored in `seq_length`)
- V is the vocabulary size (this dimension contains the one-hot encoding)

Your goal is to reverse engineer, crack, or otherwise manipulate the model to
extract the password.
```

The `model.py` file has the following content:

```{.python filename=model.py}
import torch
import torch.nn as nn

import string


vocab = " " + string.ascii_lowercase


class ASCIIModel(nn.Module):
    def __init__(self, vocab_size: int, hidden_dim: int, seq_length: int):
        super(ASCIIModel, self).__init__()
        self.vocab_size = vocab_size
        self.seq_length = seq_length
        self.final = nn.Linear(seq_length * vocab_size, vocab_size * seq_length)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.view(-1, self.seq_length * self.vocab_size)

        logits = self.final.forward(x)

        logits = logits.view(-1, self.seq_length, self.vocab_size)
        return logits
```

First, let's analyze the pickle file to find out more about the model parameters:

```{.python filename=loader.py}
import torch

model_data = torch.load("model.pkl", map_location="cpu")

print(model_data.__dict__)
# {
#     '_backward_hooks': OrderedDict(),
#     '_backward_pre_hooks': OrderedDict(),
#     '_buffers': OrderedDict(),
#     '_forward_hooks': OrderedDict(),
#     '_forward_hooks_always_called': OrderedDict(),
#     '_forward_hooks_with_kwargs': OrderedDict(),
#     '_forward_pre_hooks': OrderedDict(),
#     '_forward_pre_hooks_with_kwargs': OrderedDict(),
#     '_is_full_backward_hook': None,
#     '_load_state_dict_post_hooks': OrderedDict(),
#     '_load_state_dict_pre_hooks': OrderedDict(),
#     '_modules': OrderedDict([('final',
#                             Linear(in_features=864, out_features=864, bias=True))]),
#     '_non_persistent_buffers_set': set(),
#     '_parameters': OrderedDict(),
#     '_state_dict_hooks': OrderedDict(),
#     '_state_dict_pre_hooks': OrderedDict(),
#     'seq_length': 32,
#     'training': True,
#     'vocab_size': 27
# }
```

Both of the above outputs confirm that:

- The sequence length(N) is 32
- The vocabulary size(V) is 27

We can also write a quick script to generate some ouputs from the model:

```{.python filename=loader.py}

import argparse

import torch
from model import ASCIIModel, vocab

checkpoint = torch.load("model.pkl", map_location="cpu")
state_dict = checkpoint.state_dict()

V = 27
N = 32

model = ASCIIModel(V, 0, N)
model.load_state_dict(state_dict)
model.eval()


def one_hot_encode(sequence, vocab_size, seq_length):
    """
    One-hot encode a string sequence into a (seq_length, vocab_size) tensor.

    Args:
        sequence (str): Input string to encode. Only characters in `vocab` are encoded.
        vocab_size (int): Size of the vocabulary (should match len(vocab)).
        seq_length (int): Length of the output sequence. Input is truncated/padded as needed.

    Returns:
        torch.Tensor: Tensor of shape (seq_length, vocab_size) with one-hot rows.
    """
    tensor = torch.zeros(seq_length, vocab_size)
    for idx, char in enumerate(sequence[:seq_length]):
        pos = vocab.find(char)
        if pos != -1:
            tensor[idx][pos] = 1
    return tensor


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Test a candidate password against the model."
    )
    parser.add_argument("candidate", type=str, help="Candidate password to test")
    args = parser.parse_args()

    input_tensor = one_hot_encode(args.candidate, V, N).unsqueeze(0)
    with torch.no_grad():
        output = model(input_tensor)
    if torch.any(output != input_tensor):
        output_string = "".join(
            [vocab[char_prob.argmax().item()] for char_prob in output[0]]
        )
        print(f"Output for {args.candidate}: {output_string}")
```

We can run the script with a few inputs:

```sh
python loader.py password
# password

python loader.py whatistheflag
# whatistheflag
```

The behaviour of the model is as expected. It echoes the input.

Next, we know the the vocabulary has 27 characters(26 alphabets + space). Let's check if using an out-of-vocabulary character will cause the model to predict the flag. In terms of tokenisation, and OOV character will just be marked as 0.

```sh
python loader.py password1111111111111111111111
# passworddamn nice traininy da

python loader.py password@@@@@@@@@@@@@@@@@@@@@@
# passworddamn nice traininy da
```

Aha! We are able to obtain a partial flag from the OOV inputs.

At this point, I tried a bunch of approaches, ranging from varied inputs to analyzing the activations for many characters. Unfortunately, none of them worked.

I then realised one key assumption made in the initial script: I always assumed that the flag would be the first token with the _highest_ probability i.e I performed greedy decoding. 

A quick primer on decoding methods:

## Decoding methods primer

### Greedy decoding  

Pick the single most likely token at every time-step (`argmax`). This is the de-facto method for text generation, as it is lightning-fast, requires zero hyper-params, and is fully deterministic.

The greatest downside of greedy decoding is that it is _myopic_: one bad early choice dooms the entire sequence. Effectively `top_k = 1`.

![Greedy vs Beam decoding. Source: [Niklas Heidloff](https://heidloff.net/article/greedy-beam-sampling/)](./anthropic_bsides_ctf/greedy.jpeg)

### Beam decoding  

Beam decoding tries to solve the problem of greedy decoding by tracking the *B* best partial sequences instead of just one. For every step:  

- Extend each candidate with every vocab token â†’ |vocab| Ã— B branches.  
- Add cumulative log-probs (optionally apply a length penalty).  
- Keep the top *B* overall.  

By exploring multiple paths, beam search chases the globally most probable sentence, not just the best next token. Compute grows linearly with *B*; memory with *B Ã— length*.

However, this method is generally slower than greedy decoding, and requires more hyperparameters to tune.

### Nudging `top_k` from 1 â†’ 2  

`top_k` sampling is a quick middle groundâ€”sample only from the top *k* tokens.  

- `k = 1`: identical to greedy.  
- `k = 2`: injects just enough randomness to escape the echo loop while staying in high-probability territory. I suspected the runner-up tokens would reveal the hidden flag. 

## Top-k sampling

The only change to our code is:

```diff
-    if torch.any(output != input_tensor):
-        output_string = "".join(
-            [vocab[char_prob.argmax().item()] for char_prob in output[0]]
-        )
-        print(f"Output for {args.candidate}: {output_string}")
+    if torch.any(output != input_tensor):
+        output_string = "".join(
+            [vocab[char_prob.topk(2).indices[1].item()] for char_prob in output[0]]
+        )
+        print(f"Output for {args.candidate}: {output_string}")
```

Running the script now, we get:

```bash
python loader.py passwordzzzzzzzzzzzzzzzzzzzzzzzz
# Output for passwordzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz: flag is damn nice training datas
```

Success! We've successfully extracted the flag.

## Reward

I submitted the flag to the Anthropic team, and received the following message a few days later:

![API Credits ðŸ¤‘](./anthropic_bsides_ctf/credits.png)

I would've loved to get the credits now since my Claude Code bills are going through the roof!

# Where to next?

Overall, this was a super-fun challenge! If you're looking to gain more experience with general CTF style contests, I recommend the following:

- [picoCTF](https://picoctf.com/)
  - One of the first websites I discovered during my undergrad, and arguably one of the best CTFs out there for beginners.
- [HackTheBox](https://www.hackthebox.com/)
  - CTF-style labs and puzzles, great for sharpening practical skills.
- [VulnHub](https://www.vulnhub.com/)
  - Free repository of intentionally vulnerable VMs you can download and hack offlineâ€”realistic boxes with community write-ups.

Thanks for sticking around! Spotted a bug or have feedback? Open an issue in the repo, or ping me on [Twitter/X](https://x.com/shubhamg2208). Happy hacking!

# References

::: {#refs}
:::