@misc{Google,
  author = {Google},
  title = {{I/O 2025 Keynote}},
  year = {2025},
  url = {https://blog.google/technology/ai/io-2025-keynote/},
} 
@misc{GPUPuzzles,
  author = {Rush, Sasha},
  title = {{GPU Puzzles}},
  year = {2023},
  url = {https://github.com/srush/GPU-Puzzles},
  note = {A collection of GPU programming puzzles}
}
@misc{siboehmcuda,
  author = {Boehm, Sebastian},
  title = {{Matrix Multiplication in CUDA}},
  year = {2022},
  url = {https://siboehm.com/articles/22/CUDA-MMM},
  note = {A detailed guide on implementing matrix multiplication in CUDA}
}
@misc{maharshicuda,
  author = {Maharshi, },
  title = {{Optimizing SGEMV in CUDA}},
  year = {2023},
  url = {https://maharshi.bearblog.dev/optimizing-sgemv-cuda/},
  note = {A detailed worklog on optimizing SGEMV operations in CUDA}
}
@misc{modularpuzzles,
  author = {Modular},
  title = {{GPU Puzzles in Mojo}},
  year = {2025},
  url = {https://builds.modular.com/puzzles/introduction.html},
  note = {A collection of GPU programming puzzles implemented in Mojo}
}
@misc{llvmlayouttensor,
  author = {Taei, },
  title = {{Simplifying GPU Programming with Parametric Tile-Level Tensors In Mojo}},
  year = {2024},
  url = {https://llvm.org/devmtg/2024-10/slides/techtalk/Taei-Simplifying-GPU-Programming-with-Parametric-Tile-Level-Tensors-In-Mojo.pdf},
  note = {Technical talk on GPU programming simplification using Mojo}
}
@misc{mojotalk,
  author = {Modular},
  title = {{Mojo: The Future of Systems Programming}},
  year = {2025},
  url = {https://youtu.be/5gPG7SXoBag?si=H_kbkzbqfZHvNQSy&t=1731},
  note = {A talk on the future of systems programming with Mojo}
}
@misc{jeffniutriton,
  author = {Niu, Jeff},
  title = {{Triton Clone in Mojo}},
  year = {2025},
  url = {https://youtu.be/zUwyO2PZisw?si=QLdX_cAXDBcJH4mu},
  note = {Triton Clone in Mojo}
}
@misc{wikipediadotproduct,
  author = {Wikipedia},
  title = {Dot product},
  year = {2024},
  howpublished = {\url{https://en.wikipedia.org/wiki/Dot_product}},
}
@misc{thakur_convolutions,
  author = {Ayush Thakur},
  title = {Intuitive Understanding of 1D, 2D, and 3D Convolutions in Convolutional Neural Networks},
  year = {2020},
  url = {https://wandb.ai/ayush-thakur/dl-question-bank/reports/Intuitive-understanding-of-1D-2D-and-3D-convolutions-in-convolutional-neural-networks---VmlldzoxOTk2MDA},
  note = {A report on convolution operations in deep neural networks.}
}
@misc{nvidiapragmaunroll,
  author = {NVIDIA},
  title = {{\#pragma unroll Compiler Directive (CUDA C Programming Guide)}},
  year = {2025},
  url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=unroll#pragma-unroll},
  note = {Reference for the \#pragma unroll compiler directive in CUDA}
}
@misc{mojoparameter,
  author = {Modular},
  title = {Parametric closure (\texttt{@parameter}) in Mojo},
  year = {2025},
  url = {https://docs.modular.com/mojo/manual/decorators/parameter/#parametric-closure},
  note = {Official documentation for parametric closures using @parameter in Mojo}
}
@misc{mojo_lifetimes,
  author = {Modular},
  title = {Lifetimes in Mojo},
  year = {2025},
  url = {https://docs.modular.com/mojo/manual/values/lifetimes/},
  note = {Official documentation for value lifetimes and the origin system in Mojo}
}
@misc{zero_cost_abstractions,
  author = {saoirse},
  title = {Zero Cost Abstractions},
  year = {2019},
  url = {https://without.boats/blog/zero-cost-abstractions/},
  note = {Blog post discussing the meaning and implications of zero-cost abstractions}
}
@misc{iitd_parallel_convolution,
  author = {Rijurekha Sen},
  title = {Parallel Convolution},
  year = {2022},
  url = {https://www.cse.iitd.ac.in/~rijurekha/col730_2022/parallelconvolution_aug29.pdf},
  note = {Lecture notes on parallel convolution, COL730, IIT Delhi}
}
@misc{zeno_dichotomy_paradox,
  author = {Wikipedia},
  title = {Zeno's Paradoxes — Dichotomy paradox},
  year = {2024},
  howpublished = {\url{https://en.wikipedia.org/wiki/Zeno%27s_paradoxes#Dichotomy_paradox}},
}

@inproceedings{blelloch_prefix_sum,
  author = {Guy E. Blelloch},
  title = {Prefix Sums and Their Applications},
  booktitle = {Synthesis of Parallel Algorithms},
  year = {1993},
  pages = {35--60},
  url = {https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf},
  note = {Seminal work introducing the all-prefix-sums (scan) operation and its parallel algorithms.}
}
@misc{modular_democratizing_ai_compute,
  author = {Chris Lattner and Modular},
  title = {Democratizing AI Compute},
  year = {2025},
  url = {https://www.modular.com/democratizing-ai-compute},
  note = {Blog series on the vision and technical approach to democratizing AI compute.}
}
@misc{wikipedia_simt,
  author = {Wikipedia},
  title = {Single instruction, multiple threads},
  howpublished = {\url{https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads}},
  note = {Wikipedia article on SIMT (Single instruction, multiple threads) architecture}
}
@article{dao_flashattention,
  author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
  title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  journal = {arXiv preprint arXiv:2205.14135},
  year = {2022},
  url = {https://arxiv.org/abs/2205.14135},
  note = {Introduces FlashAttention, an IO-aware algorithm for efficient attention computation.}
}
@misc{siboehm_cuda_mmm,
  author = {Simon Boehm},
  title = {CUDA Matrix Multiplication Madness},
  year = {2022},
  url = {https://siboehm.com/articles/22/CUDA-MMM},
  note = {Blog post exploring various CUDA matrix multiplication strategies and optimizations.}
}
@misc{mojo_layouttensor_setitem,
  author = {Modular, Inc.},
  title = {LayoutTensor.\_\_setitem\_\_ API Reference},
  year = {2024},
  url = {https://docs.modular.com/mojo/kernels/layout/layout_tensor/LayoutTensor/#__setitem__},
  note = {Official documentation for the LayoutTensor \_\_setitem\_\_ method in Mojo.}
}
@misc{nhrfau_roofline_model,
  author = {NHR@FAU},
  title = {Roofline Model: Performance Modeling for Modern Processors},
  year = {2022},
  howpublished = {\url{https://www.youtube.com/watch?v=IrkNZG8MJ64}},
  note = {Roofline performance model for CPUs and GPUs.}
}
@misc{rtx_ada_specs,
  author = {NVIDIA Corporation},
  title = {NVIDIA RTX 4000 Ada Generation Datasheet},
  year = {2023},
  url = {https://resources.nvidia.com/en-us-briefcase-for-datasheets/rtx-4000-ada-datashe-1?ncid=no-ncid},
  note = {Official datasheet for the RTX 4000 Ada}
}
@misc{amd_gpu_basics,
  author = {Oak Ridge National Laboratory},
  title = {AMD GPU Basics},
  year = {2019},
  url = {https://www.olcf.ornl.gov/wp-content/uploads/2019/10/ORNL_Application_Readiness_Workshop-AMD_GPU_Basics.pdf},
  note = {Workshop slides covering AMD GPU architecture and programming fundamentals.}
}
@misc{nvidia_shared_memory_stats,
  author = {NVIDIA Corporation},
  title = {CUDA Kernel-Level Shared Memory Statistics},
  url = {https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/memorystatisticsshared.htm},
  note = {Official documentation on shared memory statistics in CUDA kernels.}
}
@misc{leimao_cuda_shared_memory_bank,
  author = {Lei Mao},
  title = {CUDA Shared Memory Bank},
  year = {2022},
  url = {https://leimao.github.io/blog/CUDA-Shared-Memory-Bank/},
  note = {Blog post explaining CUDA shared memory bank structure and bank conflicts.}
}
@misc{mojo_layouttensor_tile,
  author = {Modular, Inc.},
  title = {LayoutTensor.tile API Reference},
  url = {https://docs.modular.com/mojo/kernels/layout/layout_tensor/LayoutTensor/#tile},
  note = {Official documentation for the LayoutTensor tile method in Mojo.}
}

@misc{mojo_copy_dram_to_sram_async,
  author = {Modular, Inc.},
  title = {copy\_dram\_to\_sram\_async API Reference},
  url = {https://docs.modular.com/mojo/kernels/layout/layout_tensor/copy_dram_to_sram_async/},
  note = {Official documentation for the copy\_dram\_to\_sram\_async method in Mojo.}
}
@misc{mojo_async_copy_wait_all,
  author = {Modular, Inc.},
  title = {async\_copy\_wait\_all API Reference},
  url = {https://docs.modular.com/mojo/stdlib/gpu/memory/async_copy_wait_all},
  note = {Official documentation for the async\_copy\_wait\_all method in Mojo.}
}
@misc{nvidia_ampere_unified_shared_memory,
  author = {NVIDIA Corporation},
  title = {Ampere Tuning Guide: Unified Shared Memory and L1/Texture Cache},
  year = {2023},
  url = {https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html#unified-shared-memory-l1-texture-cache},
  note = {Official documentation on unified shared memory and L1/texture cache in NVIDIA Ampere GPUs.}
}
@misc{nvidia_blackwell_unified_shared_memory,
  author = {NVIDIA Corporation},
  title = {Blackwell Tuning Guide: Unified Shared Memory and L1/Texture Cache},
  year = {2025},
  url = {https://docs.nvidia.com/cuda/blackwell-tuning-guide/index.html#unified-shared-memory-l1-texture-cache},
  note = {Official documentation on unified shared memory and L1/texture cache in NVIDIA Blackwell GPUs.}
}

