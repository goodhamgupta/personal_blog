@misc{Google,
  author = {Google},
  title = {{I/O 2025 Keynote}},
  year = {2025},
  url = {https://blog.google/technology/ai/io-2025-keynote/},
} 
@misc{GPUPuzzles,
  author = {Rush, Sasha},
  title = {{GPU Puzzles}},
  year = {2023},
  url = {https://github.com/srush/GPU-Puzzles},
  note = {A collection of GPU programming puzzles}
}
@misc{siboehmcuda,
  author = {Boehm, Sebastian},
  title = {{Matrix Multiplication in CUDA}},
  year = {2022},
  url = {https://siboehm.com/articles/22/CUDA-MMM},
  note = {A detailed guide on implementing matrix multiplication in CUDA}
}
@misc{maharshicuda,
  author = {Maharshi, },
  title = {{Optimizing SGEMV in CUDA}},
  year = {2023},
  url = {https://maharshi.bearblog.dev/optimizing-sgemv-cuda/},
  note = {A detailed worklog on optimizing SGEMV operations in CUDA}
}
@misc{modularpuzzles,
  author = {Modular},
  title = {{GPU Puzzles in Mojo}},
  year = {2025},
  url = {https://builds.modular.com/puzzles/introduction.html},
  note = {A collection of GPU programming puzzles implemented in Mojo}
}
@misc{llvmlayouttensor,
  author = {Taei, },
  title = {{Simplifying GPU Programming with Parametric Tile-Level Tensors In Mojo}},
  year = {2024},
  url = {https://llvm.org/devmtg/2024-10/slides/techtalk/Taei-Simplifying-GPU-Programming-with-Parametric-Tile-Level-Tensors-In-Mojo.pdf},
  note = {Technical talk on GPU programming simplification using Mojo}
}
@misc{mojotalk,
  author = {Modular},
  title = {{Mojo: The Future of Systems Programming}},
  year = {2025},
  url = {https://youtu.be/5gPG7SXoBag?si=H_kbkzbqfZHvNQSy&t=1731},
  note = {A talk on the future of systems programming with Mojo}
}
@misc{jeffniutriton,
  author = {Niu, Jeff},
  title = {{Triton Clone in Mojo}},
  year = {2025},
  url = {https://youtu.be/zUwyO2PZisw?si=QLdX_cAXDBcJH4mu},
  note = {Triton Clone in Mojo}
}
@misc{wikipediadotproduct,
  author = {Wikipedia},
  title = {Dot product},
  year = {2024},
  howpublished = {\url{https://en.wikipedia.org/wiki/Dot_product}},
}
@misc{thakur_convolutions,
  author = {Ayush Thakur},
  title = {Intuitive Understanding of 1D, 2D, and 3D Convolutions in Convolutional Neural Networks},
  year = {2020},
  url = {https://wandb.ai/ayush-thakur/dl-question-bank/reports/Intuitive-understanding-of-1D-2D-and-3D-convolutions-in-convolutional-neural-networks---VmlldzoxOTk2MDA},
  note = {A report on convolution operations in deep neural networks.}
}
@misc{nvidiapragmaunroll,
  author = {NVIDIA},
  title = {{\#pragma unroll Compiler Directive (CUDA C Programming Guide)}},
  year = {2025},
  url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=unroll#pragma-unroll},
  note = {Reference for the \#pragma unroll compiler directive in CUDA}
}
@misc{mojoparameter,
  author = {Modular},
  title = {Parametric closure (\texttt{@parameter}) in Mojo},
  year = {2025},
  url = {https://docs.modular.com/mojo/manual/decorators/parameter/#parametric-closure},
  note = {Official documentation for parametric closures using @parameter in Mojo}
}
@misc{mojo_lifetimes,
  author = {Modular},
  title = {Lifetimes in Mojo},
  year = {2025},
  url = {https://docs.modular.com/mojo/manual/values/lifetimes/},
  note = {Official documentation for value lifetimes and the origin system in Mojo}
}
@misc{zero_cost_abstractions,
  author = {saoirse},
  title = {Zero Cost Abstractions},
  year = {2019},
  url = {https://without.boats/blog/zero-cost-abstractions/},
  note = {Blog post discussing the meaning and implications of zero-cost abstractions}
}
@misc{iitd_parallel_convolution,
  author = {Rijurekha Sen},
  title = {Parallel Convolution},
  year = {2022},
  url = {https://www.cse.iitd.ac.in/~rijurekha/col730_2022/parallelconvolution_aug29.pdf},
  note = {Lecture notes on parallel convolution, COL730, IIT Delhi}
}
@misc{zeno_dichotomy_paradox,
  author = {Wikipedia},
  title = {Zeno's Paradoxes — Dichotomy paradox},
  year = {2024},
  howpublished = {\url{https://en.wikipedia.org/wiki/Zeno%27s_paradoxes#Dichotomy_paradox}},
}

@inproceedings{blelloch_prefix_sum,
  author = {Guy E. Blelloch},
  title = {Prefix Sums and Their Applications},
  booktitle = {Synthesis of Parallel Algorithms},
  year = {1993},
  pages = {35--60},
  url = {https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf},
  note = {Seminal work introducing the all-prefix-sums (scan) operation and its parallel algorithms.}
}
@misc{modular_democratizing_ai_compute,
  author = {Chris Lattner and Modular},
  title = {Democratizing AI Compute},
  year = {2025},
  url = {https://www.modular.com/democratizing-ai-compute},
  note = {Blog series on the vision and technical approach to democratizing AI compute.}
}
@misc{wikipedia_simt,
  author = {Wikipedia},
  title = {Single instruction, multiple threads},
  howpublished = {\url{https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads}},
  note = {Wikipedia article on SIMT (Single instruction, multiple threads) architecture}
}
@article{dao_flashattention,
  author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
  title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  journal = {arXiv preprint arXiv:2205.14135},
  year = {2022},
  url = {https://arxiv.org/abs/2205.14135},
  note = {Introduces FlashAttention, an IO-aware algorithm for efficient attention computation.}
}
@misc{siboehm_cuda_mmm,
  author = {Simon Boehm},
  title = {CUDA Matrix Multiplication Madness},
  year = {2022},
  url = {https://siboehm.com/articles/22/CUDA-MMM},
  note = {Blog post exploring various CUDA matrix multiplication strategies and optimizations.}
}
