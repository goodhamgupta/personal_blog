[
  {
    "objectID": "posts/2020-04-20-attention.html",
    "href": "posts/2020-04-20-attention.html",
    "title": "Attention is all you need",
    "section": "",
    "text": "This paper review is following the blog from Jay Alammar’s blog on the Illustrated Transformer. The blog can be found here."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#encoder-and-decoder-stacks",
    "href": "posts/2020-04-20-attention.html#encoder-and-decoder-stacks",
    "title": "Attention is all you need",
    "section": "Encoder and decoder stacks",
    "text": "Encoder and decoder stacks\n\nEncoder: 6 identical layers. 2 sub layers per layer\nFirst: multi-head self attention mechanism\nSecond: Fully connected feed forward network\nApply residual connection for each of the two laters\nApply layer normalization\nDecoder: 6 identical layers. 2 sub layers as above + 1 more which performs multi-head attention over output of encoder stack\nResidual blocks: Present around all 3 sub layers\nLayer normalization: Normalizes input across features instead of normalizing input features across batch dimension(i.e in batch normalization). There is a great overview of normalization layers available by Akash Bindal here.\nModify self-attention sub layer to prevent positions from attending to subsequent positions. Ensures that i output depends only on words before i."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#attention",
    "href": "posts/2020-04-20-attention.html#attention",
    "title": "Attention is all you need",
    "section": "Attention",
    "text": "Attention\n\n3 vectors: Query(Q), Key(K) and Value(V)\nOutput = Weighted sum of values. Weights assigned as a function of query with key.\nScaled dot-product attention and multi-head attention\n\n\n\nTypes of Attention\n\n\nAttention is calculated as:\n\\[\n        Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n   \\]\nDot product attention is faster and more space-efficient than additive attention."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#multi-head-attention",
    "href": "posts/2020-04-20-attention.html#multi-head-attention",
    "title": "Attention is all you need",
    "section": "Multi head attention",
    "text": "Multi head attention\n\nUsing multile q, k and v vectors. Get the final output, concatenate them and get another final projection \\(d_{v}\\).\n$$ MultiHead(Q,K,V) = Concat(head_1,…,head_h)W^O \\\n\\text{where } head_i = Attention(QW_{i}^{Q}, KW_{i}^{K},VW_{i}^{V})\n$$\nDimensions of the key and value matrices will be: \\(d_{k} = d_{v} = d_{model}/h = 64\\)"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#applications-of-attention",
    "href": "posts/2020-04-20-attention.html#applications-of-attention",
    "title": "Attention is all you need",
    "section": "Applications of attention",
    "text": "Applications of attention\n\nEncoder-decoder attention: Q from previours decoder, K and V from output of decoder. Attend to all positions in the input sequence.\nEncoder: Self attentnion laters. Q,K and V from output of previous layer in the encoder. Some talk about leftward flow, didn’t really understand this bit. Will come back to this in sometime."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#position-wise-feed-forward-networks",
    "href": "posts/2020-04-20-attention.html#position-wise-feed-forward-networks",
    "title": "Attention is all you need",
    "section": "Position-wise Feed-Forward Networks",
    "text": "Position-wise Feed-Forward Networks\n\nEach layer contains feed-forward network.\n\\[\n        FFN(x) = max(o, xW_1,+ b_1)W_2 + b_2\n\\]"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#embeddings-and-softmax",
    "href": "posts/2020-04-20-attention.html#embeddings-and-softmax",
    "title": "Attention is all you need",
    "section": "Embeddings and Softmax",
    "text": "Embeddings and Softmax\n\nConvert input and output string to vectors of dim \\(d_{model}\\)\nShare weight matrix between two embedding layers and the pre-softmaax linear transformation"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#positional-encoding",
    "href": "posts/2020-04-20-attention.html#positional-encoding",
    "title": "Attention is all you need",
    "section": "Positional Encoding",
    "text": "Positional Encoding\n\nEncode positions of the tokens for the input and output.\nSame vector size i.e \\(d_{model}\\)\n$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\\n    PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n$$\nMight allow approximation of longer sequence lenghts than seen in the training set"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#why-self-attention",
    "href": "posts/2020-04-20-attention.html#why-self-attention",
    "title": "Attention is all you need",
    "section": "Why self attention?",
    "text": "Why self attention?\n\nTotal computational complexity per layer\nParallel Computation\nPath length between long-range dependencies in the network."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#optimizer",
    "href": "posts/2020-04-20-attention.html#optimizer",
    "title": "Attention is all you need",
    "section": "Optimizer",
    "text": "Optimizer\n\nUse Adam. Vary learning rate according to formula: \\(lrate = d_{model}^{-0.5} . min(step_num^{-0.5}, step_num . warmupsteps^{-1.5})\\)\nIncrease LR for warmup steps, then decrease propotionally to inverse square root of step number. Warmup steps = 4000"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#regularization",
    "href": "posts/2020-04-20-attention.html#regularization",
    "title": "Attention is all you need",
    "section": "Regularization",
    "text": "Regularization\n\nResidual Dropout\nLabel Smoothing: Instead of using 0 and 1 as class labels, allow for some uncertainity in the prediction, and use values like 0.1 and 0.9 for the classes"
  },
  {
    "objectID": "posts/2020-03-14-realm.html",
    "href": "posts/2020-03-14-realm.html",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "",
    "text": "REALM is a paper mentioned in the T5 paper titled: How Much Knowledge Can You Pack Into The Parameters of a Language Model?\nTLDR: This paper retrieves documents that have the information present while solving Question-Answer type problems.\n\nNOTE: This post is more like my running notes while reading the paper than a comprehensive blog. I will update this blog once I learn a little more about the transformer architecture.\n\nIntroduced a latent knowledge retriever, which can attend and retrieve documents over large corpus and can be trained in unsupervised manner using masked language modelling technique and backprop through retreiver which considers lots of docs.\n\n\n\nTraining process for REALM\n\n\nKey point: Train retriever using a performance-based signal from unsupervised text.\nRetrieval based LM =&gt; Moar computational resources =&gt; Moar money\n\nSolution: Computation performed for each doc is cached and can be used again. Best doc selected using Maximum Inner Product Search(MIPS). Read the paper here.\n\nREALM retriever can be used on downstream tasks via transfer learning.\nREALM is SOTA on NQ-Open, WQ and CuratedTrec."
  },
  {
    "objectID": "posts/2020-03-14-realm.html#retreive-then-predict-generative-process",
    "href": "posts/2020-03-14-realm.html#retreive-then-predict-generative-process",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Retreive-then-predict generative process",
    "text": "Retreive-then-predict generative process\n\nTraining: Masked-LM. Fine-tuning: Open QA task\nComputing chance of the document given a question decomposed into two steps:\n\nFunction to be computed: \\[p(y\\|x)\\]\nGiven \\[x\\],retrive documents \\[z\\] from corpus \\[Z\\]. Modelled as: \\[p(z\\|x)\\]\nCondition of both \\[z\\] and \\[x\\] to generate output \\[y\\] i.e \\[p(y\\|z, x)\\]\nOverall likelihood \\[y\\] is generated by treating \\[z\\] as latent variable and marginalizing over all documents \\[z\\]\n\\[\np(y\\|x) = \\sum_{z \\epsilon Z} p(y\\|z, x) * p(z\\|x)\n\\]"
  },
  {
    "objectID": "posts/2020-03-14-realm.html#neural-knowledge-retriever",
    "href": "posts/2020-03-14-realm.html#neural-knowledge-retriever",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Neural Knowledge Retriever",
    "text": "Neural Knowledge Retriever\n\nDense inner product model.\n\\[\n\\begin{aligned}\n    p(z\\|x) = \\frac{exp(f(x,z))}{\\sum_{z'}{exp(f(x,z'))}} \\\\\n    f(x,z) = Embed_{input}(x)^TEmbed_{doc}(z)\n\\end{aligned}\n\\]\n\\[Embed_{input}\\] and \\[Embed_{doc}\\] are embedding functions\n\\[f(x,z)\\] is called relevance score. It is inner product of vector embeddings.\nRelevant Distribution is softmax over all relevance scores\nEmbedding implement using BERT-style transformers. Join using &lt;SEP&gt;, prefix using &lt;CLS&gt; and append &lt;SEP&gt; as the end token. \\[\\begin{aligned}\n        \\\\ join_{BERT}(x) = [CLS]x[SEP]\n        \\\\ join_{BERT}(x_1, x_2) = [CLS]x_1[SEP]x_2[SEP]\n    \\end{aligned}\\]\nPass above into transformer, which gives over vector for each token. Perform linear projection to reduce dimensionality of vector \\[\\begin{aligned}\n    \\\\ Embed_{input}(x) = W_{input}BERT_{CLS}(join_{BERT}(x))\n    \\\\ Embed_{doc}(z) = W_{doc}BERT_{CLS}(join_{BERT}(z_{title}, z_{body}))\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/2020-03-14-realm.html#knowledge-augmented-encoder",
    "href": "posts/2020-03-14-realm.html#knowledge-augmented-encoder",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Knowledge-Augmented Encoder",
    "text": "Knowledge-Augmented Encoder\n\nGiven input \\[x\\] and relevant doc \\[z\\], this defines \\[p(y\\|z,x)\\]\nJoin \\[x\\] and \\[z\\] into single sequence and feed into transformer\nHere, training is different for pre-training vs fine-tuning\n\nFor pre-training, predict [MASK] token. Use same Masked LM(MLM) loss as in Transformer(Devlin et al.)\nFor Open-QA, we need to produce string \\[y\\].\nAssumption: \\[y\\] occurs as sequence of tokens in some document in the corpus."
  },
  {
    "objectID": "posts/2020-03-14-realm.html#training",
    "href": "posts/2020-03-14-realm.html#training",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Training",
    "text": "Training\n\nCompute gradients in \\[\\theta\\] and \\[\\phi\\] and optimize using SGD.\nChallenge: Computing \\[p(y\\|x)\\]\nApprox by summing over top \\[k\\] documents with highest prob under \\[p(z\\|x)\\]\nQuestion: How to find top \\[k\\] docs? Answer: Use MIPS\nNeed to precompute \\[Embed_{doc}(x)\\] for all docs. Problem? It changes with each step of SGD.\nSolution: Async refresh \\(Embed_{doc}\\) every 500 steps\nUse MIPS to select top \\(k\\) docs. For these docs, recompute \\(p(z\\|x)\\) using new \\(\\theta\\).\n\n\nImplementing async MIPS refreshes\n\nTwo jobs running in parallel:\n\nPrimary trainer: Perform gradient updates on parameters\nSecondary index builder: Embeds and indexes the docs\n\n\n\nAsync MIPS implementation\n\n\nAsync refresh used only for pre-training\nFor fine tuning, build index once from pre-trained \\(\\theta\\) and use it.\n\n\n\n\nWhat does retriever learn?\n\nRetriever promotes docs that improve accuracy\nThis can be analyzed by analyzing gradient wrt the parameters\n\n\n\nInjecting inductive biases into pre-trianing\n\nSalient span masking: Some questions require only local context. Select named entities and dates and mask one of them. Performs better.\nNull document: Add null document to top \\[k\\] documents to allow answers even when no context is required\nProhibiting trivial retrievals: If knowledge corpus \\[Z\\] is the same as pre-training corpus \\[X\\], it can predict \\[y\\] by looking at \\[x\\] in \\[z\\]. Exclude trivial candidate\nInitialization: Warm up \\[Embed_{input}\\] and \\[Embed_{doc}\\] using Inverse Cloze Task(ICT) i.e model trained to retrieve the doc where the sentence came from."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html",
    "href": "posts/2020-05-11-longformer.html",
    "title": "LongFormer",
    "section": "",
    "text": "The NLP world had its ImageNet moment with the introduction of the Transformer in the paper Attention is All you Need.\nThe ability to be able to process multiple words/tokens in parallel and train models without labeled data(using self-attention) led to the creation of multiple models which gave us SOTA results on many interesting tasks such as Question Answering, Summarization, etc.\nHowever, the biggest drawback is the Transformer architecture is the limitation it has on the number of tokens it can process at a once, due to exponentially increasing memory and compute requirements(typically about 512 tokens), causing the performance to deteriorate over large documents.\nLongformer by the team at Allen AI aims to address this problem and demonstrate it’s application to do transfer learning for large documents.\nOther approaches to are described in recent work such as Transformer XL, Blockwise, Reformer, etc. Their characteristics are mentioned below:\n\n\n\n\nComparison"
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#sliding-window-attention",
    "href": "posts/2020-05-11-longformer.html#sliding-window-attention",
    "title": "LongFormer",
    "section": "Sliding Window Attention",
    "text": "Sliding Window Attention\n\nTLDR : Similar to kernels for CNN which apply a matrix operation to a set of pixels and move onto the next set, apply attention to tokens in current window only.\nIn this, we change the attention objective to only focus on the tokens that occur in a context window \\(w\\).\nEach token will be able to attend to \\(\\frac{1}{2}w\\) number of tokens to it’s left and right.\nQuestion: But doesn’t this limit the number of tokens being taken into account to only the tokens in the window?\n\nYes, it does. This is why we stack multiple layers of self-attention. As shown in the image below, the green neuron learns from the first 3 tokens(Lionel, Messi, is). However, the brown neuron learns from the green, yellow, and red neuron, who together learn from the first 5 tokens. This way, we can apply attention to long sequences(Lionel, Messi, is, the, true).\n\nAs with the CNN, we will have \\(l\\) layers to this sliding window attention(multi-head attention) implemented to learn low level and high-level features. A balance should be found between the number of layers \\(l\\)(efficiency) and the window size \\(w\\)(model representation capacity).\n\n\n\n\nSliding Window Attention\n\n\n\nPros: Reduces computation from \\(O(n^2)\\) to \\(O(n*w)\\) i.e the computation complexity will only scale linearly now.\nCons: To learn dependencies for a large sequence, we would either have to increase the window size \\(w\\) or increase the number of layers \\(l\\), both of which will cause an increase in the amount of memory and processing power required to train and test the model."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#dilated-sliding-window",
    "href": "posts/2020-05-11-longformer.html#dilated-sliding-window",
    "title": "LongFormer",
    "section": "Dilated Sliding Window",
    "text": "Dilated Sliding Window\n\nTLDR: Use dilation instead of window attention i.e for some particular window size, take alternate elements while performing self-attention.\nTo solve the problem for long sequences, the authors propose that instead of considering all tokens in window \\(w\\), consider alternate(or any number \\(d\\))tokens instead. The range of tokens will now be \\(l * d * w\\), which will be large for even a small value of \\(d\\).\nPros: This small change will allow us to cover a wider range of tokens without significant changes to the architecture.\nCons: Skipping tokens might lead to loss of information in the lower layers which will get propagated to the higher layers. This will lead to unstable training and poor model performance."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#global-attention",
    "href": "posts/2020-05-11-longformer.html#global-attention",
    "title": "LongFormer",
    "section": "Global Attention",
    "text": "Global Attention\n\nTLDR: Use full attention for certain tokens depending on the task. This is an engineering choice.\nIn BERT style models, optimal representation for input sequence varies by task.\n\nFor MLM, local context is used to predict the masked word\nFor classification, [CLS] token is used.\nFor QnA, the question is concatenated with the document to help model learn through self-attention.\n\nThe windowed and dilated attention is not flexible enough to learn task-specific representations.\nHence, for some tokens enable global tokens i.e at these tokens, all tokens in the sequence can attend to it. For classification, enable global attention on the [CLS] token.\nPros:\n\nAdding global attention improves performance for specific tasks. Since these tokens are limited in number, the complexity still stays at \\(O(n)\\).\nIt also increases the representational power of the model.\n\n\n\nLinear Projections\n\nTLDR: Use two sets of Q,K and V matrices, one for sliding window attention, one for global attention.\nAttention is defined as:\n\\[\n\\begin{aligned}\nAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n\\end{aligned}\n\\]\nWe will use two different sets of Q,K and V matrices for sliding window and global attention.\n\\(Q_g\\), \\(K_g\\), \\(V_g\\) are initialized with \\(Q_s\\), \\(K_s\\), \\(V_s\\)\n\n\n\n\nBanded Matrix\n\n\n\nBanded Matrix(Source)\n\n\n\n\nCompressed Banded Matrix\n\n\n\nCompressed Banded Matrix(Source)\n\n\n\nCUDA Kernels\n\nOne of the important and interesting contributions of this paper is the implementation of matrix multiplication via CUDA kernels.\nIn the dilated sliding window, the matrix formed is called a band matrix i.e there are diagonal bands of indices that have values and the other values are 0.\nImplementing matrix operations for band matrices using native for loops and via frameworks is not easy and optimized.\nThe authors have provided custom CUDA kernels implemented using TVM for this banded matrix operations.\nAs demonstrated in the image below, the custom CUDA kernels have a significant impact on the time and memory consumption of the model. The kernels and implementation for the longformer are available here. \n\nLongFormer Performance"
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#attention-pattern",
    "href": "posts/2020-05-11-longformer.html#attention-pattern",
    "title": "LongFormer",
    "section": "Attention Pattern",
    "text": "Attention Pattern\n\nIn multi-head attention, each head computes a different score.\nTo get a good representation of all tokens, the authors propose that normal sliding window attention can be used for the lower layers, and dilated sliding window attention can be used the higher layers(top 1-2 layers).\nThe reasoning for this approach is that in the lower layers, the local context is more important, and in the upper layers, the global context is more important. Hence, it is acceptable to skip over a few tokens in the upper layers."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#task-and-datasets",
    "href": "posts/2020-05-11-longformer.html#task-and-datasets",
    "title": "LongFormer",
    "section": "Task and Datasets",
    "text": "Task and Datasets\n\nThe authors focus on character level modeling because the sequences are naturally longer than those of word-level language modeling.\nDatasets that were used are text8 and enwik8."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#training-and-evaluation",
    "href": "posts/2020-05-11-longformer.html#training-and-evaluation",
    "title": "LongFormer",
    "section": "Training and Evaluation",
    "text": "Training and Evaluation\n\nThe model was trained in multiple phases.\n\nThe window and sequence length was increased in each phase. This is to allow local context from tokens to be learned efficiently.\nOverall five training phases used, starting from the token length of 2048 to 23040 (45x more than vanilla BERT).\nTwo models were created for evaluation:\n\nSmall model: 12 layers, 512 hidden size\nLarge model: 30 layers, 512 hidden sizes (2.5x larger)\n\nDuring the model evaluation, the model can run on a sequence length of 32256(63x more than vanilla BERT)."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#results",
    "href": "posts/2020-05-11-longformer.html#results",
    "title": "LongFormer",
    "section": "Results",
    "text": "Results\n\n\n\nResults\n\n\n\nLongformer achieves SOTA using the small models with BPC of 1.10 and 1.00 for text8 and enwik8.\nThe large model was only tested on enwik8 due to the computational cost of training.\nIt’s also important to note that, while the large model did not achieve SOTA, it performs much better than it’s counterparts who have almost 2x more parameters."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#copy-initialization-trick",
    "href": "posts/2020-05-11-longformer.html#copy-initialization-trick",
    "title": "LongFormer",
    "section": "Copy initialization trick",
    "text": "Copy initialization trick\n\nSince the MLM objective pretraining objective is expensive, the authors continue to train from the checkpoints of the RoBERTA model.\nThe attention mechanism is replaced with the new attention module.\nFor the position embeddings:\n\nRoBERTA has position embeddings for 512 tokens.\nLongFormer can support position embeddings for 4096 tokens(larger for larger GPU)\nTo use the weight checkpoints from RoBERTA, instead of random initialization, copy the 512 position embeddings multiple times as analysis of the BERT attention heads showed a strong learned bias to attend to the local context."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#pretraining",
    "href": "posts/2020-05-11-longformer.html#pretraining",
    "title": "LongFormer",
    "section": "Pretraining",
    "text": "Pretraining\n\nApart from the datasets(Books corpus + English Wikipedia) used in RoBERTA, \\(\\frac{1}{3}^{rd}\\) Realnews dataset was added with tokens larger than 1200.\nBoth models(small and large) trained with varying gradient updates.\n\n\n\n\nCopy init\n\n\n\nMLM BPC for RoBERTA with various model config"
  },
  {
    "objectID": "posts/2020-08-31-bison.html",
    "href": "posts/2020-08-31-bison.html",
    "title": "BERT + BM25 = BISON",
    "section": "",
    "text": "This paper aims to create a framework to map query and doc into semantic vectors via self-attention models.\nWe cant use prior knowledge about important tokens for models based on self-attention.\n\nWords are split into different tokens using a tokenization mechanism such as WordPiece. We cannot translate word-level knowledge into different tokens.\n\nHowever, from classical information retrieval, we know that prior knowledge about the word is important. For example, ERNIE used a Knowledge Graph to achieve SOTA on several GLUE tasks.\nFurthermore, documents have different fields with varying degrees of importance such as text, header, filetypes, etc. We cannot combine these fields directly because their importance varies for a task.\nKey takeaways:\n\nCombine BM25 to learn attention scores with Query(Q) and Key(K) matrices, which are used in self-attention.\nWord weight sharing to reduce knowledge discrepancy between tokens and words.\nCombine multiple fields by placing different fields in different segments using a BM25F, a variation of BM25."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#overview-of-bison",
    "href": "posts/2020-08-31-bison.html#overview-of-bison",
    "title": "BERT + BM25 = BISON",
    "section": "Overview of BISON",
    "text": "Overview of BISON\n\n\n\nBISON Architecture\n\n\n\nThe framework has 4 important parts:\n\nWord level BM25: In this, we prepend the CLS token to the query and use combined fields representation for the documents.\nToken level representation: As is the norm, we will use the token, position and segment embedding\nBISON Encoder: This will encode the query q and the document d into semantic spacy by siamese structure making it possible to serve the model online. The architecture consists of 3 stacked BISON layers.\nScoring: The documents are scored using the cosine similarity metric."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#bison-encoder-weighted-self-attention",
    "href": "posts/2020-08-31-bison.html#bison-encoder-weighted-self-attention",
    "title": "BERT + BM25 = BISON",
    "section": "BISON Encoder: Weighted Self Attention",
    "text": "BISON Encoder: Weighted Self Attention\n\nAs we know from the original “Attention” paper, attention is computed using the query, key, and value matrices.\nTo the above, we will add the importance of tokens via BM25. We will introduce w_i and multiply with above attention to get new attention score i.e Weighted Self Attention \\[ A_{ij}^w = w_j\\frac{q_i.k_j^T}{\\sqrt{d}} \\]\n\n\n\n\nWeighted Self Attention\n\n\n\nMathematically, it is represented as:\n\n\\[WeightedSelfAttention(Q,K,W,V) = softmax(W (.) \\frac{QK^T}{\\sqrt{d}}V\\]\n\nWSA is the main block unit. Multiple such units are tacked to get the multi-head structure.\nRescaling by \\(W^o\\), we get Complex Weighted Self Attention(CWSA).\nA fully connected layer is added. In both CWSA and fully connected layer, layer norm and residual connections are used\n\n\\[CWSA = Concat(WeightedSelfAttention1,... WeightedSelfAttention, n)W^o\\]\n\\[CWSA_{out}=LayerNorm(CWSA + X)\\]\n\\[BISONEncoder = LayerNorm(CWSA_{out} + FeedForward(CWSA_{out}))\\]"
  },
  {
    "objectID": "posts/2020-08-31-bison.html#bm25-weight-generation",
    "href": "posts/2020-08-31-bison.html#bm25-weight-generation",
    "title": "BERT + BM25 = BISON",
    "section": "BM25 Weight generation",
    "text": "BM25 Weight generation\n\nUse BM25 for weight scores in query and BM25F for weight scores in multi-field documents\nBM25F, a variation of BM25, is for documents with different fields, each having different importance in terms of relevance saturation and length normalization. Find additional details in the file here.\n\n\nInherent Query BM25\n\nFor a given query, BM25 is calculated within the query.\n\n\\(l_q\\): query length\n\\(avl_q\\): query average length along collection\n\n\n\\[\nw_i^{BM25} = idf_i \\frac{tf_i}{tf_i + k_1(1-b+b \\frac{l_q}{avl_q})}\n\\]\n\n\nInherent Document BM25F\n\nBM25F is implemented by assigning different degrees of importance to the different zones in a document such as title, header, footer, filetype, text, etc. For a \\(word_j\\) in a document field \\(c\\), it’s frequency \\(f_j^c\\) is defined as:\n\n\\[\natf_j^c = \\frac{fw_c . tf_j^c}{1.0 + fln_c . (\\frac{fl_c}{avl_c}-1.0)}\n\\]\n\nThe corresponding BM25F score is computed as\n\n\\[\nw_j^{BM25F} = idf_j\\frac{atf_j}{k_1 + atf_j}\n\\]"
  },
  {
    "objectID": "posts/2020-08-31-bison.html#whole-word-weight-sharing",
    "href": "posts/2020-08-31-bison.html#whole-word-weight-sharing",
    "title": "BERT + BM25 = BISON",
    "section": "Whole word weight sharing",
    "text": "Whole word weight sharing\n\nBERT uses wordpiece to produce tokens from raw text. However, because of this, we cannot directly apply the prior knowledge we obtained from B-52.\nSolution: Assign the same word weight to all tokens for a given word. This way, a token might have a different weight depending on the context of the given word."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#combined-fields-representation",
    "href": "posts/2020-08-31-bison.html#combined-fields-representation",
    "title": "BERT + BM25 = BISON",
    "section": "Combined Fields Representation",
    "text": "Combined Fields Representation\n\nDocuments typically consist of different fields, each of which provides complementary information. Thus, these fields need to be taken into consideration. Typical fields considered are:\n\nPrimitive Fields(Title, URL, header, etc.)\nOther fields(anchor, click signal via parsing search log, etc)\n\nFor the experiment, only the following fields were picked for performance reasons(the body has too much text to encode in a single space):\n\nTitle\nAnchor\nURL\nClicked query\n\nFor each field, we learn their representation individually and combine them. Further, we also restrict the number of tokens for each of the above fields to a total of 128 tokens.\n\n20 tokens each for Title, Anchor and URL.\nOnly consider the top 5 clicked queries for a maximum of 68 tokens.\n\nFor given fields, the document representation \\(\\phi(D)\\) is given by:\n\n\\[\n\\phi(D) = A_{f_i}(\\phi_{F_1}(F_1)+ \\phi_{F_2}(F_2)+...+\\phi_{F_n}(F_n))\n\\]\n\nHere,\n\n\\(F_i\\) is the field\n\\(\\phi(F_1)\\) denotes the representation learned for each field \\(F_i\\)\n\\(A_{f_i}\\) is a function to aggregate all representations\n\nThe remaining tokens(512-128=384) are used to encode the query."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#optimization",
    "href": "posts/2020-08-31-bison.html#optimization",
    "title": "BERT + BM25 = BISON",
    "section": "Optimization",
    "text": "Optimization\n\nThe [CLS] token from the last layer is used as a representation for the query and the document\nMatching score \\(s\\) is computed as: \\[\ns = cos(BISON(query)_{last cls}, BISON(document)_{last cls})\n\\]\nCross entropy loss is used to determine if the retrieved document is relevant or not. \\[\nLoss = -ylog(\\delta(w.s+b))-(1-y)log(1 - \\delta(w.s+b))\n\\]"
  },
  {
    "objectID": "posts/2020-08-31-bison.html#intrinsic-evaluation",
    "href": "posts/2020-08-31-bison.html#intrinsic-evaluation",
    "title": "BERT + BM25 = BISON",
    "section": "Intrinsic evaluation",
    "text": "Intrinsic evaluation\n\nSelected 1400 representative queries and 7 million query document pairs from Bing’s search log\nPerformance-wise, USE performs the worst as it performs well only on homogeneous data, and query document pairs are heterogeneous.\nBISON outperforms all baseline models significantly."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#ms-marco",
    "href": "posts/2020-08-31-bison.html#ms-marco",
    "title": "BERT + BM25 = BISON",
    "section": "MS Marco",
    "text": "MS Marco\n\nSimilar steps followed for document full ranking task on the MS Marco dataset.\nFor each query, the top 1000 documents are returned and MR is used as performance metrics."
  },
  {
    "objectID": "posts/2020-04-21-knowledge-lm.html",
    "href": "posts/2020-04-21-knowledge-lm.html",
    "title": "How much do you know?",
    "section": "",
    "text": "Introduction\n\nThis is a new paper which explores the limits of using their new T5 titled How Much Knowledge Can You Pack Into The Parameters of a Language Model?. model in a context-free QA domain.\nAs with the T5 model itself, it is very interesting to see these one-model-to-rule-them-all architectures as they exhibit some form of generalization.\nI found this paper from Adam Roberts twitter thread which is available here\nCore Idea: This paper will test two main things:\n\nHow well does the model create a knowledge base such that it can answer questions just based on this base and no other information.\nDo model with more parameters store more information? Measuring knowledge retreiving ability is used to check this point.\n\n\n\n\nPaper Introduction\n\nReading Comprehension: Given a question and context, lookup and give the answer.\nOpen domain question answering: Random context-independent questions. It is given entire context(all the information possible in the world) and the model is expected to deduce the answer. Open book exam.\nHere, problem is similar to open book exam + no context given at all. Model should retreive info from parameters and return the values. Closed book exam.\nT5: Treat every NLP task as text-to-text problem using encoder decoder Transformer.\nFor natural questions dataset, evaluation is done as follows:\nFirst method:\n\nIgnore all “unanswerable” and “long answer” type questions.\nmodel trained to output single answer\nQuestions with answers longer than 5 tokens are ignored\nAnswers normalized before comparsion\nAnswer is correct if it matches any of the annotated answers\n\nSecond method:\n\nConsidered correct only if model predicts all the answers correctly\n\nFor fine tuning, use AdaFactor Optimizer(need to read more about this one)\n\n\n\nResults\n\nSOTA on Natural Questions(NQ) and WebQuestions(WQ) dataset. Worst performance on TriviaQA(TQA).\nPerformance increases with model size.\nGuu et all(2020) performs better than T5 on NQ and WQ. Need to read this paper as well. It\n\nRetreives Revevant documents\nAnswers questions in end-to-end fashion\n\nClosed-book model seem to perform on par with open-book models, leading to new research directions.\nFor multiple answer type questions, T5 lower than SOTA BUT much better than baseline that was published with the paper. Therefore, T5 can perform well on these types of questions as well.\n\n\n\nDrawbacks\n\nModel is far too expensive to train.\nOpen-book models provide some indication of what information was used to answer the problem. HOWEVER, T5 just has a distribution over parameters that cannot be interpreted.\nMLE does not gurantee the model will learn a fact. Therefore, difficult to ensure the model learns specific information during pre-training\nMeasure and improve performance on difficult QA tasks like DROP, which needs reasoning ability."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Shubham Gupta",
    "section": "",
    "text": "Hi! I’m Shubham, an Applied AI engineer in Singapore.\nThrough this blog, I aim to document my learning notes and experiences in software development, artificial intelligence, and distributed systems.\n\n\n\nProbabilistic Programming in Elixir with Exstan\n\nPresented at ElixirConfEU 2024.\n\n\n\n\n\nI love Elixir, and have been creating packages for probabilistic programming, fast XIRR computation using Newton’s method, etc. The total downloads are over 250K.\nI actively review AI papers in my spare time, which you can find on my Github.\n\n\n\n\nState Space Models\n\nPresented at Machine Learning, Singapore, a top ML community in Singapore.\nSSMs are a promising alternative to the Attention mechanism used in Transformers.\n\nAI-Driven Patient Engagement in Healthcare\n\nEngineered mobile app with optimized on-device LLM (3B params) and confidence-based cloud fallback using token logits to route requests to larger cloud model\nSuccessfully piloted with 100 users, enabling accessible medical guidance in low-resource regions\n\nIntroduction to GPUs and CUDA\n\nDemonstrates basics of GPU architecture and CUDA programming.\n\nStructured Generation in LLMs\n\nPresented on common methods used to generate structured output from LLMs.\n\nEnd‑to‑End Attention based Image Captioning\n\nImplemented as part of a course project at NUS, finishing in the top 10% of the leaderboard in the Kaggle contest.\n\nHierachical Bayesian CLV Model\n\nDeveloped a novel Bayesian hierarchical model for CLV (Customer Lifetime Value) prediction that integrates customer demographics, achieving 37% improvement in valuation accuracy and enabling data-driven marketing strategies.\n\n\n\n\n\nI enjoy solving technical challenges on various platforms:\n\nHackAttic\n\nCollection of real-world challenges. Ranked in top 10% of participants.\n\nCodeCrafters\n\nCollection of challenges to build real-world software.\nI’ve completed challenges like building Redis and a HTTP Server\n\n\n\n\n\nNational University of Singapore | Singapore\nMasters in Computer Science, AI Specialisation\nSep 2020 - Dec 2022\nAmrita School of Engineering | Bangalore, India\nBachelor of Technology in Computer Science\nAug 2012 - May 2016"
  },
  {
    "objectID": "about.html#talks",
    "href": "about.html#talks",
    "title": "Shubham Gupta",
    "section": "",
    "text": "Probabilistic Programming in Elixir with Exstan\n\nPresented at ElixirConfEU 2024."
  },
  {
    "objectID": "about.html#open-source",
    "href": "about.html#open-source",
    "title": "Shubham Gupta",
    "section": "",
    "text": "I love Elixir, and have been creating packages for probabilistic programming, fast XIRR computation using Newton’s method, etc. The total downloads are over 250K.\nI actively review AI papers in my spare time, which you can find on my Github."
  },
  {
    "objectID": "about.html#presentations",
    "href": "about.html#presentations",
    "title": "Shubham Gupta",
    "section": "",
    "text": "State Space Models\n\nPresented at Machine Learning, Singapore, a top ML community in Singapore.\nSSMs are a promising alternative to the Attention mechanism used in Transformers.\n\nAI-Driven Patient Engagement in Healthcare\n\nEngineered mobile app with optimized on-device LLM (3B params) and confidence-based cloud fallback using token logits to route requests to larger cloud model\nSuccessfully piloted with 100 users, enabling accessible medical guidance in low-resource regions\n\nIntroduction to GPUs and CUDA\n\nDemonstrates basics of GPU architecture and CUDA programming.\n\nStructured Generation in LLMs\n\nPresented on common methods used to generate structured output from LLMs.\n\nEnd‑to‑End Attention based Image Captioning\n\nImplemented as part of a course project at NUS, finishing in the top 10% of the leaderboard in the Kaggle contest.\n\nHierachical Bayesian CLV Model\n\nDeveloped a novel Bayesian hierarchical model for CLV (Customer Lifetime Value) prediction that integrates customer demographics, achieving 37% improvement in valuation accuracy and enabling data-driven marketing strategies."
  },
  {
    "objectID": "about.html#technical-challenges",
    "href": "about.html#technical-challenges",
    "title": "Shubham Gupta",
    "section": "",
    "text": "I enjoy solving technical challenges on various platforms:\n\nHackAttic\n\nCollection of real-world challenges. Ranked in top 10% of participants.\n\nCodeCrafters\n\nCollection of challenges to build real-world software.\nI’ve completed challenges like building Redis and a HTTP Server"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Shubham Gupta",
    "section": "",
    "text": "National University of Singapore | Singapore\nMasters in Computer Science, AI Specialisation\nSep 2020 - Dec 2022\nAmrita School of Engineering | Bangalore, India\nBachelor of Technology in Computer Science\nAug 2012 - May 2016"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tech Musings",
    "section": "",
    "text": "GPUs go brrr with Mojo - Fundamentals\n\n\n\nprogramming\n\ngpu\n\nmojo\n\n\n\nLearning GPU programming fundamentals through hands-on Mojo implementations\n\n\n\n\n\nJul 6, 2025\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nBERT + BM25 = BISON\n\n\n\nnlp\n\ntransformer\n\nreview\n\n\n\nA new framework for information retrieval from documents\n\n\n\n\n\nAug 31, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nLongFormer\n\n\n\nnlp\n\ntransformer\n\nreview\n\nlongformer\n\n\n\nTransformers for loooong documents\n\n\n\n\n\nMay 11, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nTagLM\n\n\n\nnlp\n\nlanguage_model\n\nreview\n\n\n\n\n\n\n\n\n\nApr 23, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nHow much do you know?\n\n\n\nnlp\n\nlanguage_model\n\nreview\n\n\n\nMeasure the amount of information stored in a model\n\n\n\n\n\nApr 21, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nAttention is all you need\n\n\n\nnlp\n\nattention\n\nreview\n\n\n\n\n\n\n\n\n\nApr 20, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nREALM: Retrieval-Augmented Language MOdel Pre-Training\n\n\n\nnlp\n\nbert\n\nreview\n\n\n\nA better Q&A system based on knowledge retrieval\n\n\n\n\n\nMar 14, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Golf Putting Model\n\n\n\njupyter\n\nbayesian\n\ngolf_putting\n\n\n\nAre you the next Tiger Woods?\n\n\n\n\n\nMar 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nYo!\n\n\n\nintro\n\n\n\nFirst post\n\n\n\n\n\nJan 14, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-01-14-first-post.html",
    "href": "posts/2020-01-14-first-post.html",
    "title": "Yo!",
    "section": "",
    "text": "Yo!\nSuper excited to finally get my own blog. Hope to write out those long pending articles ASAP now. Stay tuned!"
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "",
    "text": "Back on the blog after a long hiatus - this time, I’m shifting gears from just reviewing papers(which are available on my GitHub) to diving deep into hands-on implementations.\nI’ve always been interested in systems programming, but somehow never really picked it up. The rate of progress in the GenAI space has been exponential recently, with players like Google [1] reportedly processing 9.7 trillion tokens a month. Companies are now investing more time and resources in making these Large Language Models as fast and cheap as possible, by improving training and inference efficiency using “moar” compute.\nI briefly spoke about GPU computing last year, and finally decided to learn it this summer. The goal is to eventually be able to implement kernels for fast matmuls, softmax, and FlashAttention."
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html#why-mojo",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html#why-mojo",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "Why Mojo?",
    "text": "Why Mojo?\nI’ve tried learning Rust multiple times, along with a few stints of trying C, C++ and Zig, but I never really felt as comfortable in these languages as I do in Python and Elixir.\nIn early 2023, Modular announced Mojo🔥, a new systems-programming language promising:\n\nPython-like syntax\nSupport for both CPU and GPU architectures\nKernel autofusion\nBuilds on MLIR\nTraits and bounds checking\nInteropeability with PTX, Python, C\n\nModular has since announced Max, their AI inference platform, built on Mojo. The released all kernels available as part of the platform, along with their own version[2] of Sasha Rush’s GPU Puzzles [3] in Mojo. IMO, their kernels were much easier to read compared to CUDA/Triton implementations. I also enjoyed the “Democratising AI Compute”[4] series by Chris Lattner, and thus I decided to learn a bit more about how to write these kernels in Mojo."
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html#gpu-memory",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html#gpu-memory",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "GPU 101",
    "text": "GPU 101\nGPUs (Graphics Processing Units) are massively parallel processors optimized for throughput over latency. In GPU programming we:\n\nLay out data and computation as a grid of thread blocks.\nLaunch a kernel from the CPU (host) to run on the GPU (device).\nExploit thousands of lightweight threads all executing the same code (Single Instruction, Multiple Threads or SIMT).\n\nModern chips had two ways to spend their billions of transistors:\n\nCPUs invest them in large caches, branch predictors and out-of-order logic to minimize latency for one or a few threads.\nGPUs invest them in thousands of simple cores and huge register files to maximize throughput for many threads, assuming those threads can tolerate latency by waiting in parallel.\n\nThe rest of this section unpacks how that single design choice shows up in memory, execution and program flow.\n\n1. Memory hierarchy – hide latency with tons of threads\nCPUs invest transistors in large caches to minimize latency. GPUs take the opposite approach: they use thousands of threads to hide latency instead of avoiding it.\nGPU memory hierarchy (slowest/largest to fastest/smallest):\n\nGlobal (HBM): High Bandwidth Memory—the GPU’s main memory, large but high-latency, visible to all threads\n\nShared (SRAM): fast on-chip memory, ~100x faster than HBM\n\nRegisters: per-thread storage, fastest access, ~1000x faster than HBM\n\n\n\n\nSource: FlashAttention [5]. Metrics shown are for an NVIDIA A100 GPU.\n\n\nThe key insight: when threads wait for slow global memory (~400-800 cycles), the GPU immediately switches to other threads. This keeps compute units busy while data moves through the hierarchy.\n\n\n2. Execution hierarchy – launch enough warps to hide stalls\n\n\n\n\n\n\n\nGPU Execution Hierachy\n\n\nBuilding from the bottom up:\n\nThread: the basic execution unit with its own registers\nWarp: 32 threads executing the same instruction together (the basic unit of GPU scheduling)\n\nBlock: a group of threads that share shared memory and can synchronize\n\nGrid: a collection of blocks distributed across SMs\n\nGPUs schedule threads in groups of 32 (warps). When one warp stalls on memory, the scheduler switches to another warp instantly. More resident warps = better latency hiding.\n\n\n3. Program flow – CPU launches, GPU streams\n\nThe CPU launches kernels asynchronously and goes back to other work. Inside the GPU each warp executes the same instruction (SIMT). Divergent branches disable some lanes and waste those cores.\n\nHost allocates and copies data to GPU global memory\n\nHost launches the kernel with a specified grid and block size\n\nDevice executes the kernel in parallel across threads\n\nHost retrieves results from GPU memory\n\n\n\nPutting it together\nFast GPU kernels keep cores busy by: - Staging hot data in shared or registers - Launching enough threads to mask global-memory latency - Writing branch-free, data-parallel code\nWe will cover the practical implications of the above topics as we go through the puzzles."
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html#infrastructure",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html#infrastructure",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "Infrastructure",
    "text": "Infrastructure\nIf you plan on solving these puzzles, remember to pick a compatible GPU and follow the setup instructions\nI completed the puzzles on a instance with a RTX4090 Ti chip, rented via Prime Intellect at 0.22 $/hr!\nNote: The Modular team has created beautiful Manim visualizations for each puzzle, making the concepts much more intuitive. I’ll walk through these visualizations as we tackle each problem."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html",
    "href": "posts/2020-03-12-tutorial.html",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "This is inspired from Dr. Andrew Gelman’s case study, which can be found here. Specifically:\n\nThis is heavily inspired by Colin Caroll’s Blog present here. A lot of the plotting code from his blog post has been reused.\nJosh Duncan’s blog post on the same topic which can be found here.\n\nThis is not a novel solution. It is merely a replication of Dr. Gelman’s blog in PyMC3.\n\n\n\nThis is based on a popular blog post by Dr. Andrew Gelman. Here, we are given data from professional golfers on the proportion of success putts from a number of tries. Our aim is to identify:\n\nCan we model the probability of success in golf putting as a function of distance from the hole?\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\nThe source repository is present here\n\ndata = np.array([[2,1443,1346],\n[3,694,577],\n[4,455,337],\n[5,353,208],\n[6,272,149],\n[7,256,136],\n[8,240,111],\n[9,217,69],\n[10,200,67],\n[11,237,75],\n[12,202,52],\n[13,192,46],\n[14,174,54],\n[15,167,28],\n[16,201,27],\n[17,195,31],\n[18,191,33],\n[19,147,20],\n[20,152,24]])\n\ndf = pd.DataFrame(data, columns=[\n    'distance', \n    'tries', \n    'success_count'\n])\n\n\ndf\n\n\n\n\n\n\n\n\ndistance\ntries\nsuccess_count\n\n\n\n\n0\n2\n1443\n1346\n\n\n1\n3\n694\n577\n\n\n2\n4\n455\n337\n\n\n3\n5\n353\n208\n\n\n4\n6\n272\n149\n\n\n5\n7\n256\n136\n\n\n6\n8\n240\n111\n\n\n7\n9\n217\n69\n\n\n8\n10\n200\n67\n\n\n9\n11\n237\n75\n\n\n10\n12\n202\n52\n\n\n11\n13\n192\n46\n\n\n12\n14\n174\n54\n\n\n13\n15\n167\n28\n\n\n14\n16\n201\n27\n\n\n15\n17\n195\n31\n\n\n16\n18\n191\n33\n\n\n17\n19\n147\n20\n\n\n18\n20\n152\n24\n\n\n\n\n\n\n\nThe variables have the following format:\n\n\n\nVariable\nUnits\nDescription\n\n\n\n\ndistance\nfeet\nDistance from the hole for the putt attempt\n\n\ntries\ncount\nNumber of attempts at the chosen distance\n\n\nsuccess_count\ncount\nThe total successful putts\n\n\n\nLets try to visualize the dataset:\n\ndf['success_prob'] = df.success_count / df.tries\n\n\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(x='distance', y='success_prob', data=df, s=200)\nax.set(xlabel='Distance from hole(ft)', ylabel='Probability of Success')\n\n[Text(0, 0.5, 'Probability of Success'),\n Text(0.5, 0, 'Distance from hole(ft)')]\n\n\n\n\n\n\n\n\n\nWe can notice that the probability of success decreases as the distance increases."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#disclaimer",
    "href": "posts/2020-03-12-tutorial.html#disclaimer",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "This is inspired from Dr. Andrew Gelman’s case study, which can be found here. Specifically:\n\nThis is heavily inspired by Colin Caroll’s Blog present here. A lot of the plotting code from his blog post has been reused.\nJosh Duncan’s blog post on the same topic which can be found here.\n\nThis is not a novel solution. It is merely a replication of Dr. Gelman’s blog in PyMC3."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#problem",
    "href": "posts/2020-03-12-tutorial.html#problem",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "This is based on a popular blog post by Dr. Andrew Gelman. Here, we are given data from professional golfers on the proportion of success putts from a number of tries. Our aim is to identify:\n\nCan we model the probability of success in golf putting as a function of distance from the hole?"
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#eda",
    "href": "posts/2020-03-12-tutorial.html#eda",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\nThe source repository is present here\n\ndata = np.array([[2,1443,1346],\n[3,694,577],\n[4,455,337],\n[5,353,208],\n[6,272,149],\n[7,256,136],\n[8,240,111],\n[9,217,69],\n[10,200,67],\n[11,237,75],\n[12,202,52],\n[13,192,46],\n[14,174,54],\n[15,167,28],\n[16,201,27],\n[17,195,31],\n[18,191,33],\n[19,147,20],\n[20,152,24]])\n\ndf = pd.DataFrame(data, columns=[\n    'distance', \n    'tries', \n    'success_count'\n])\n\n\ndf\n\n\n\n\n\n\n\n\ndistance\ntries\nsuccess_count\n\n\n\n\n0\n2\n1443\n1346\n\n\n1\n3\n694\n577\n\n\n2\n4\n455\n337\n\n\n3\n5\n353\n208\n\n\n4\n6\n272\n149\n\n\n5\n7\n256\n136\n\n\n6\n8\n240\n111\n\n\n7\n9\n217\n69\n\n\n8\n10\n200\n67\n\n\n9\n11\n237\n75\n\n\n10\n12\n202\n52\n\n\n11\n13\n192\n46\n\n\n12\n14\n174\n54\n\n\n13\n15\n167\n28\n\n\n14\n16\n201\n27\n\n\n15\n17\n195\n31\n\n\n16\n18\n191\n33\n\n\n17\n19\n147\n20\n\n\n18\n20\n152\n24\n\n\n\n\n\n\n\nThe variables have the following format:\n\n\n\nVariable\nUnits\nDescription\n\n\n\n\ndistance\nfeet\nDistance from the hole for the putt attempt\n\n\ntries\ncount\nNumber of attempts at the chosen distance\n\n\nsuccess_count\ncount\nThe total successful putts\n\n\n\nLets try to visualize the dataset:\n\ndf['success_prob'] = df.success_count / df.tries\n\n\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(x='distance', y='success_prob', data=df, s=200)\nax.set(xlabel='Distance from hole(ft)', ylabel='Probability of Success')\n\n[Text(0, 0.5, 'Probability of Success'),\n Text(0.5, 0, 'Distance from hole(ft)')]\n\n\n\n\n\n\n\n\n\nWe can notice that the probability of success decreases as the distance increases."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#geometry-based-model",
    "href": "posts/2020-03-12-tutorial.html#geometry-based-model",
    "title": "Bayesian Golf Putting Model",
    "section": "Geometry based Model",
    "text": "Geometry based Model\n\nWe’ll try to accomodate the physics associated with the problem. Specically, we assume:\n\nAssumptions\n\nThe golfers can hit the ball in any direction with some small error. This error could be because of inaccuracy, errors in the human, etc.\nThis error refers to the angle of the shot.\nWe assume the angle is normally distributed.\n\n\nImplications\n\nThe ball goes in whenever the angle is small enough for it to hit the cup of the hole!\nLonger putt \\(\\implies\\) Larger error \\(\\implies\\) Lower success rate than shorter putt\n\nFrom Dr. Gelman’s blog, we obtain the formula as:\n\n\\(Pr(|angle| &lt; sin^{-1}(\\frac{(R-r)}{x})) = 2\\phi\\big(\\frac{sin^{-1}\\frac{R-r}{x}}{\\sigma}\\big) - 1\\)\n\n\\(\\phi \\implies\\) Cumulative Normal Distribution Function.\nHence, our model will now have two big parts:\n\\[y_j \\sim binomial(n_j, p_j)\\]\n\\[p_j = 2\\phi\\big(\\frac{sin^{-1}\\frac{R-r}{x}}{\\sigma}\\big) - 1\\]\nTypically, the diameter of a golf ball is 1.68 inches and the cup is 4.25 inches i.e\n\\[r = 1.68 \\text{inch}\\] \\[R = 4.25 \\text{inch}\\]\n\nball_radius = (1.68/2)/12\ncup_radius = (4.25/2)/12\n\n\ndef calculate_prob(angle, distance):\n    \"\"\"\n    Calculate probability that the ball with fall in the hole given the angle of the shot \n    and the distance from the hole.\n    \"\"\"\n    rad = angle * np.pi / 180.0\n    arcsin = np.arcsin((cup_radius - ball_radius)/ distance)\n    return 2 * scipy.stats.norm(0, rad).cdf(arcsin) - 1\n\n\nplt.figure(figsize=(16, 6))\nls = np.linspace(0, df.distance.max(), 200)\nax = sns.scatterplot(\n    x='distance', \n    y='success_prob', \n    data=df, \n    s=100,\n    legend='full'\n)\nfor angle in [0.5, 1, 2, 5, 20]:\n    ax.plot(\n        ls, \n        calculate_prob(angle, ls), \n        label=f\"Angle={angle}\"\n    )\nax.set(\n    xlabel='Distance from hole(ft)', \n    ylabel='Probability of Success'\n)\nax.legend()\n\n\n\n\n\n\n\n\nLet us now add this to our model!\n\nimport theano.tensor as tt\n\n\ndef calculate_phi(num):\n    \"cdf for standard normal\"\n    q = tt.erf(num / tt.sqrt(2.0)) # ERF is the Gaussian Error \n    return (1.0 + q) / 2.\n\n\nwith pm.Model() as model:\n    angle_of_shot_radians = pm.HalfNormal('angle_of_shot_radians')\n    angle_of_shot_degrees = pm.Deterministic(\n        'angle_of_shot_degrees',\n        (angle_of_shot_radians * 180.0) / np.pi\n    )\n    p_ball_goes_in = pm.Deterministic(\n        'p_ball_goes_in',\n        2 * calculate_phi(\n                tt.arcsin(\n                    (cup_radius - ball_radius)/ df.distance\n                ) / angle_of_shot_radians\n            )\n        ) - 1\n    p_success = pm.Binomial(\n        'p_success',\n        n=df.tries, \n        p=p_ball_goes_in, \n        observed=df.success_count\n    )\n\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\nwith model:\n    trace = pm.sample(4000, tune=1000, chains=4)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nERROR (theano.gof.opt): Optimization failure due to: local_grad_log_erfc_neg\nERROR (theano.gof.opt): node: Elemwise{true_div}(Elemwise{mul,no_inplace}.0, Elemwise{erfc,no_inplace}.0)\nERROR (theano.gof.opt): TRACEBACK:\nERROR (theano.gof.opt): Traceback (most recent call last):\n  File \"/home/goodhamgupta/shubham/blog/_notebooks/.env/lib/python3.6/site-packages/theano/gof/opt.py\", line 2034, in process_node\n    replacements = lopt.transform(node)\n  File \"/home/goodhamgupta/shubham/blog/_notebooks/.env/lib/python3.6/site-packages/theano/tensor/opt.py\", line 6789, in local_grad_log_erfc_neg\n    if not exp.owner.inputs[0].owner:\nAttributeError: 'NoneType' object has no attribute 'owner'\n\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [angle_of_shot_radians]\nSampling 4 chains, 0 divergences: 100%|██████████| 20000/20000 [00:10&lt;00:00, 1943.54draws/s]\nThe acceptance probability does not match the target. It is 0.8844154441842546, but should be close to 0.8. Try to increase the number of tuning steps.\n\n\n\npm.summary(trace).head(2)\n\n\n\n\n\n\n\n\nmean\nsd\nhpd_3%\nhpd_97%\nmcse_mean\nmcse_sd\ness_mean\ness_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nangle_of_shot_radians\n0.027\n0.000\n0.026\n0.027\n0.0\n0.0\n6641.0\n6641.0\n6641.0\n10874.0\n1.0\n\n\nangle_of_shot_degrees\n1.527\n0.023\n1.484\n1.570\n0.0\n0.0\n6641.0\n6641.0\n6641.0\n10874.0\n1.0\n\n\n\n\n\n\n\n\npm.plot_posterior(trace['angle_of_shot_degrees'])\n\narray([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfe4c24f60&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\nFrom the above results, we can see:\n\nPyMC3 has estimated\n\n\\(angle_of_shot_degrees\\) to be \\(1.53 \\pm 0.023\\)\n\nThe MCSE is almost 0 \\(\\implies\\) The simulation has run long enough for the chains to converge.\n\\(r\\_hat = 1.0\\) tells us that the chains have mixed well i.e hairy hedgehog pattern.\n\nLet’s visualize the fit with this new model:\n\ngeo_model_prob = calculate_prob(\n    trace['angle_of_shot_degrees'].mean(), \n    df.distance\n)\n\n\nsns.set()\nplt.figure(figsize=(16, 6))\n\nax = sns.scatterplot(x='distance', y=df.success_prob, data=df, s=200, label='Actual')\n\nsns.scatterplot(x='distance', y=df.posterior_success_prob, data=df, label='Logistic Regression',ax=ax, color='red', s=100)\nsns.scatterplot(x='distance', y=geo_model_prob, data=df, label='Geometry based ',ax=ax, color='orange', s=100)\n\nsns.lineplot(x='distance', y=df.posterior_success_prob, data=df,ax=ax, color='red')\nsns.lineplot(x='distance', y=geo_model_prob, data=df,ax=ax, color='orange')\n\nax.set(xlabel='Distance from hole(ft)', ylabel='Probability of Success')\n\n[Text(0, 0.5, 'Probability of Success'),\n Text(0.5, 0, 'Distance from hole(ft)')]\n\n\n\n\n\n\n\n\n\n\nWe can see that the geometry based model fits better than the logistic regression model.\nWhile this model is not completely accurate, it suggests that angle is a good variable to model the problem. Using this model, we can be more confident about extrapolating the data.\nFor the same 50ft putt, the probability now is:\n\n\nimport scipy\nlr_result = np.round(\n    100 * scipy.special.expit(2.223 + -0.255 * 50).mean(),\n    5\n)\ngeo_result = np.round(\n    100 * calculate_prob(\n        trace['angle_of_shot_degrees'].mean(), \n        50\n    ).mean(),\n    5\n)\n\nprint(\n    f\"Logistic Regression Model: {lr_result}% \\n\"\\\n    f\"Geometry Based Model: {geo_result}%\"\n)\n\nLogistic Regression Model: 0.00268% \nGeometry Based Model: 6.40322%"
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#new-data",
    "href": "posts/2020-03-12-tutorial.html#new-data",
    "title": "Bayesian Golf Putting Model",
    "section": "New Data!",
    "text": "New Data!\nMark Broadie obtained new data about the golfers. Let’s see how our model performs on this new dataset.\nFirst, we’ll look at the summary of the dataset.\n\n#  golf putting data from Broadie (2018)\nnew_golf_data = np.array([\n[0.28, 45198, 45183],\n[0.97, 183020, 182899],\n[1.93, 169503, 168594],\n[2.92, 113094, 108953],\n[3.93, 73855, 64740],\n[4.94, 53659, 41106],\n[5.94, 42991, 28205],\n[6.95, 37050, 21334],\n[7.95, 33275, 16615],\n[8.95, 30836, 13503],\n[9.95, 28637, 11060],\n[10.95, 26239, 9032],\n[11.95, 24636, 7687],\n[12.95, 22876, 6432],\n[14.43, 41267, 9813],\n[16.43, 35712, 7196],\n[18.44, 31573, 5290],\n[20.44, 28280, 4086],\n[21.95, 13238, 1642],\n[24.39, 46570, 4767],\n[28.40, 38422, 2980],\n[32.39, 31641, 1996],\n[36.39, 25604, 1327],\n[40.37, 20366, 834],\n[44.38, 15977, 559],\n[48.37, 11770, 311],\n[52.36, 8708, 231],\n[57.25, 8878, 204],\n[63.23, 5492, 103],\n[69.18, 3087, 35],\n[75.19, 1742, 24],\n])\n\nnew_df = pd.DataFrame(\n    new_golf_data, \n    columns=['distance', 'tries', 'success_count']\n)\n\n\nnew_geo_model_prob = calculate_prob(\n    trace['angle_of_shot_degrees'].mean(), \n    new_df.distance\n)\n\n\nnew_df['success_prob'] = new_df.success_count / new_df.tries\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(x='distance', y='success_prob', data=df, label='Old Dataset', s=200)\nsns.scatterplot(x='distance', y='success_prob', data=new_df,label='New Dataset', s=200, ax=ax)\nsns.scatterplot(x='distance', y=new_geo_model_prob, data=new_df, label='Geometry based Model ',ax=ax, color='red', s=100)\nax.set(\n    xlabel='Distance from hole(ft)', \n    ylabel='Probability of Success'\n)\nplt.setp(ax.get_legend().get_texts(), fontsize='25')\n\n\n\n\n\n\n\n\nWe can see:\n\nSuccess rate is similar in the 0-20 feet range for both datasets.\nBeyond 20 ft, success rate is lower than expected. These attempts are more difficult, even after we have accounted for increased angular precision."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#moar-features",
    "href": "posts/2020-03-12-tutorial.html#moar-features",
    "title": "Bayesian Golf Putting Model",
    "section": "Moar features!",
    "text": "Moar features!\nTo get the ball in, along with the angle, we should also need to take into account if the ball was hit hard enough.\nFrom Colin Caroll’s Blog, we have the following: &gt; Mark Broadie made the following assumptions - If a putt goes short or more than 3 feet past the hole, it will not go in. - Golfers aim for 1 foot past the hole - The distance the ball goes, \\(u\\), is distributed as: \\[ u \\sim \\mathcal{N}\\left(1 + \\text{distance}, \\sigma_{\\text{distance}} (1 + \\text{distance})\\right), \\] where we will learn \\(\\sigma_{\\text{distance}}\\).\nAfter working through the geometry and algebra, we get:\n\\[P(\\text{Good shot}) = \\bigg(2\\phi\\big(\\frac{sin^{-1}(\\frac{R-r}{x})}{\\sigma_{angle}}\\big)-1\\bigg)\\bigg(\\phi\\bigg(\\frac{2}{(x+1)\\sigma_{distance}}\\bigg) - \\phi\\bigg(\\frac{-1}{(x+1)\\sigma_{distance}}\\bigg)\\bigg)\\]\nLet’s write this down in PyMC3\n\nOVERSHOT = 1.0\nDISTANCE_TOLERANCE = 3.0\ndistances = new_df.distance.values\nwith pm.Model() as model:\n    angle_of_shot_radians = pm.HalfNormal('angle_of_shot_radians')\n    angle_of_shot_degrees = pm.Deterministic(\n        'angle_of_shot_degrees',\n        (angle_of_shot_radians * 180.0) / np.pi\n    )\n    \n    variance_of_distance = pm.HalfNormal('variance_of_distance')\n    p_good_angle = pm.Deterministic(\n        'p_good_angle',\n        2 * calculate_phi(\n                tt.arcsin(\n                    (cup_radius - ball_radius)/ distances\n                ) / angle_of_shot_radians\n            )\n        ) - 1\n    p_good_distance = pm.Deterministic(\n        'p_good_distance',\n        calculate_phi(\n            (DISTANCE_TOLERANCE - OVERSHOT) / ((distances + OVERSHOT) * variance_of_distance)) \n        - calculate_phi(\n            -OVERSHOT / ((distances + OVERSHOT) * variance_of_distance))\n\n    )\n    p_success = pm.Binomial(\n        'p_success',\n        n=new_df.tries, \n        p=p_good_angle * p_good_distance, \n        observed=new_df.success_count\n    )\n    \n\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=4)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [variance_of_distance, angle_of_shot_radians]\nSampling 4 chains, 0 divergences: 100%|██████████| 8000/8000 [00:08&lt;00:00, 969.28draws/s] \nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\npm.summary(trace).head(3)\n\n\n\n\n\n\n\n\nmean\nsd\nhpd_3%\nhpd_97%\nmcse_mean\nmcse_sd\ness_mean\ness_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nangle_of_shot_radians\n0.013\n0.000\n0.013\n0.013\n0.0\n0.0\n865.0\n865.0\n862.0\n1109.0\n1.0\n\n\nangle_of_shot_degrees\n0.761\n0.003\n0.755\n0.768\n0.0\n0.0\n865.0\n865.0\n862.0\n1109.0\n1.0\n\n\nvariance_of_distance\n0.137\n0.001\n0.136\n0.138\n0.0\n0.0\n855.0\n855.0\n855.0\n1186.0\n1.0\n\n\n\n\n\n\n\n\npm.plot_posterior(trace['variance_of_distance'])\n\narray([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdff74693c8&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\n\nwith model:\n    distance_posterior = pm.sample_posterior_predictive(trace)\n\n100%|██████████| 4000/4000 [00:04&lt;00:00, 846.25it/s]\n\n\n\ndef calculate_prob_distance(angle, distance, ls):\n    \"\"\"\n    Calculate the probability the ball will land inside the hole\n    given the variance in angle and distance.\n    \n    NOTE: Adapted from Colin Carroll's Blog.\n    \"\"\"\n    norm = scipy.stats.norm(0, 1)\n    prob_angle = 2 * norm.cdf(\n        np.arcsin((cup_radius - ball_radius) / ls) / angle) - 1\n    prob_distance_one = norm.cdf(\n        (DISTANCE_TOLERANCE - OVERSHOT) / ((ls + OVERSHOT) * distance)\n    )\n    prob_distance_two = norm.cdf(-OVERSHOT / ((ls + OVERSHOT) * distance))\n    prob_distance = prob_distance_one - prob_distance_two\n    \n    return prob_angle * prob_distance\n\n\nls = np.linspace(0, new_df.distance.max(), 200)\n\n\nnew_df['success_prob'] = new_df.success_count / new_df.tries\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(\n    x='distance', \n    y='success_prob',\n    data=new_df,\n    label='Actual', \n    s=200\n)\nsns.scatterplot(\n    x='distance', \n    y=new_geo_model_prob, \n    data=new_df, \n    label='Angle only Model',\n    ax=ax, \n    color='red', \n    s=100\n)\n\nsns.scatterplot(\n    x='distance', \n    y=calculate_prob_distance(\n        trace['angle_of_shot_radians'].mean(), \n        trace['variance_of_distance'].mean(),\n        new_df.distance\n    ), \n    data=new_df, \n    label='Distance + Angle based Model ',\n    ax=ax, \n    color='black', \n    s=100\n)\nax.set(\n    xlabel='Distance from hole(ft)', \n    ylabel='Probability of Success'\n)\n\nplt.setp(ax.get_legend().get_texts(), fontsize='25')\n\n\n\n\n\n\n\n\nFrom the graph, we can conclude that:\n\nThe model is good at distance lower than 10 ft and distances higher than 40ft.\nThere is some mismatch between 10ft to 40ft, but overall this is a good fit."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#whats-the-point",
    "href": "posts/2020-03-12-tutorial.html#whats-the-point",
    "title": "Bayesian Golf Putting Model",
    "section": "What’s the point?",
    "text": "What’s the point?\nUsing Bayesian analysis, we want to be able to quantify the unvertainity with each of our predictions. Since each prediction is a distribution, we can utilize this to see where the putts will fall if they do not fall in the hole.\n\ndef simulate_from_distance(trace, distance_to_hole, trials=10_000):\n    n_samples = trace['angle_of_shot_radians'].shape[0]\n\n    idxs = np.random.randint(0, n_samples, trials)\n    variance_of_shot = trace['angle_of_shot_radians'][idxs]\n    variance_of_distance = trace['variance_of_distance'][idxs]\n\n    theta = np.random.normal(0, variance_of_shot)\n    distance = np.random.normal(distance_to_hole + OVERSHOT, (distance_to_hole + OVERSHOT) * variance_of_distance)\n\n    final_position = np.array([distance * np.cos(theta), distance * np.sin(theta)])\n\n    made_it = np.abs(theta) &lt; np.arcsin((cup_radius - ball_radius) / distance_to_hole)\n    made_it = made_it * (final_position[0] &gt; distance_to_hole) * (final_position[0] &lt; distance_to_hole + DISTANCE_TOLERANCE)\n    \n    _, ax = plt.subplots()\n\n    ax.plot(0, 0, 'k.', lw=1, mfc='black', ms=150 / distance_to_hole)\n    ax.plot(*final_position[:, ~made_it], '.', alpha=0.1, mfc='r', ms=250 / distance_to_hole, mew=0.5)\n    ax.plot(distance_to_hole, 0, 'ko', lw=1, mfc='black', ms=350 / distance_to_hole)\n\n    ax.set_facecolor(\"#e6ffdb\")\n    ax.set_title(f\"Final position of {trials:,d} putts from {distance_to_hole}ft.\\n({100 * made_it.mean():.1f}% made)\")\n    return ax\n\nsimulate_from_distance(trace, distance_to_hole=10);"
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html",
    "href": "posts/2020-04-23-tag-lm.html",
    "title": "TagLM",
    "section": "",
    "text": "This is the TagLM paper mentioned in Lecture 13 in the CS224N course title Semi-supervised sequence tagging with bidirectional language models\nThis paper demonstrates how we can use context embeddings from BiLSTM models and use it for sequence labelling tasks"
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html#overview",
    "href": "posts/2020-04-23-tag-lm.html#overview",
    "title": "TagLM",
    "section": "Overview",
    "text": "Overview\n\nExtract word and LM embeddings for every token\nPrepare embedding by concatinating both embeddings\nse them in supervised sequence tagging model"
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html#baseline",
    "href": "posts/2020-04-23-tag-lm.html#baseline",
    "title": "TagLM",
    "section": "Baseline",
    "text": "Baseline\n\nBaseline model is hierachical neural tagging model\nObtain word and character embeddings. Concatenate them to form final embedding.\n\\[ x_k = [c_k;w_k] \\]\nChar embedding: Can be obtained via CNN or RNN\nWord embedding: Use pretrained embeddings\nUse multiple bidirectional RNN to learn context embedding\nFor every token, concatenate forward and backward hidden states at each layer\n2nd layer will use the above output and predict next hidden state\nUse GRU or LSTM depending on task\nUse output of final layer to predict score for each possible tag using dense layer\nBetter to predict tags for full sequence than for a single token\nTHEREFORE, add another layer with parameters for each label bigram.\nCompute sentence conditional random field loss(CRF) using forward-backward algorithm.\nUse Viterbi algorithm to find most likely sequence\n\n\n\nOverview of architecture\n\n\nLM embedding will be created by concatenating forward and backward embeddings. No parameter sharing between these two embeddings."
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html#analysis",
    "href": "posts/2020-04-23-tag-lm.html#analysis",
    "title": "TagLM",
    "section": "Analysis",
    "text": "Analysis\n\nSecond RNN captures interactions between task specific context\nBackward LM addition has significant performance gains\nModel size makes a difference. Using bigger CNN model lead to  0.3 percent improvement\nAlso tried training the model JUST ON THE CoNLL data. Reduced model size\nIncluding embeddings from this model decreased performance compared to baseline system.\nReplacing task specific RNN with using LM embeddings with a dense layer and CRF reduced performance\nImprovement shown by transferring knowledge from other tasks almost disappers when the initial model is trained on a large dataset."
  }
]