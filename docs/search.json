[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Shubham Gupta",
    "section": "",
    "text": "Hi! I’m Shubham, an Applied AI engineer in Singapore.\nThrough this blog, I aim to document my learning notes and experiences in software development, artificial intelligence, and distributed systems.\n\n\n\nProbabilistic Programming in Elixir with Exstan\n\nPresented at ElixirConfEU 2024.\n\n\n\n\n\nI love Elixir, and have been creating packages for probabilistic programming, fast XIRR computation using Newton’s method, etc. The total downloads are over 250K.\nI actively review AI papers in my spare time, which you can find on my Github.\n\n\n\n\nState Space Models\n\nPresented at Machine Learning, Singapore, a top ML community in Singapore.\nSSMs are a promising alternative to the Attention mechanism used in Transformers.\n\nAI-Driven Patient Engagement in Healthcare\n\nEngineered mobile app with optimized on-device LLM (3B params) and confidence-based cloud fallback using token logits to route requests to larger cloud model\nSuccessfully piloted with 100 users, enabling accessible medical guidance in low-resource regions\n\nIntroduction to GPUs and CUDA\n\nDemonstrates basics of GPU architecture and CUDA programming.\n\nStructured Generation in LLMs\n\nPresented on common methods used to generate structured output from LLMs.\n\nEnd‑to‑End Attention based Image Captioning\n\nImplemented as part of a course project at NUS, finishing in the top 10% of the leaderboard in the Kaggle contest.\n\nHierachical Bayesian CLV Model\n\nDeveloped a novel Bayesian hierarchical model for CLV (Customer Lifetime Value) prediction that integrates customer demographics, achieving 37% improvement in valuation accuracy and enabling data-driven marketing strategies.\n\n\n\n\n\nI enjoy solving technical challenges on various platforms:\n\nHackAttic\n\nCollection of real-world challenges. Ranked in top 10% of participants.\n\nCodeCrafters\n\nCollection of challenges to build real-world software.\nI’ve completed challenges like building Redis and a HTTP Server\n\n\n\n\n\nNational University of Singapore | Singapore\nMasters in Computer Science, AI Specialisation\nSep 2020 - Dec 2022\nAmrita School of Engineering | Bangalore, India\nBachelor of Technology in Computer Science\nAug 2012 - May 2016"
  },
  {
    "objectID": "about.html#talks",
    "href": "about.html#talks",
    "title": "Shubham Gupta",
    "section": "",
    "text": "Probabilistic Programming in Elixir with Exstan\n\nPresented at ElixirConfEU 2024."
  },
  {
    "objectID": "about.html#open-source",
    "href": "about.html#open-source",
    "title": "Shubham Gupta",
    "section": "",
    "text": "I love Elixir, and have been creating packages for probabilistic programming, fast XIRR computation using Newton’s method, etc. The total downloads are over 250K.\nI actively review AI papers in my spare time, which you can find on my Github."
  },
  {
    "objectID": "about.html#presentations",
    "href": "about.html#presentations",
    "title": "Shubham Gupta",
    "section": "",
    "text": "State Space Models\n\nPresented at Machine Learning, Singapore, a top ML community in Singapore.\nSSMs are a promising alternative to the Attention mechanism used in Transformers.\n\nAI-Driven Patient Engagement in Healthcare\n\nEngineered mobile app with optimized on-device LLM (3B params) and confidence-based cloud fallback using token logits to route requests to larger cloud model\nSuccessfully piloted with 100 users, enabling accessible medical guidance in low-resource regions\n\nIntroduction to GPUs and CUDA\n\nDemonstrates basics of GPU architecture and CUDA programming.\n\nStructured Generation in LLMs\n\nPresented on common methods used to generate structured output from LLMs.\n\nEnd‑to‑End Attention based Image Captioning\n\nImplemented as part of a course project at NUS, finishing in the top 10% of the leaderboard in the Kaggle contest.\n\nHierachical Bayesian CLV Model\n\nDeveloped a novel Bayesian hierarchical model for CLV (Customer Lifetime Value) prediction that integrates customer demographics, achieving 37% improvement in valuation accuracy and enabling data-driven marketing strategies."
  },
  {
    "objectID": "about.html#technical-challenges",
    "href": "about.html#technical-challenges",
    "title": "Shubham Gupta",
    "section": "",
    "text": "I enjoy solving technical challenges on various platforms:\n\nHackAttic\n\nCollection of real-world challenges. Ranked in top 10% of participants.\n\nCodeCrafters\n\nCollection of challenges to build real-world software.\nI’ve completed challenges like building Redis and a HTTP Server"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Shubham Gupta",
    "section": "",
    "text": "National University of Singapore | Singapore\nMasters in Computer Science, AI Specialisation\nSep 2020 - Dec 2022\nAmrita School of Engineering | Bangalore, India\nBachelor of Technology in Computer Science\nAug 2012 - May 2016"
  },
  {
    "objectID": "posts/2020-04-21-knowledge-lm.html",
    "href": "posts/2020-04-21-knowledge-lm.html",
    "title": "How much do you know?",
    "section": "",
    "text": "Introduction\n\nThis is a new paper which explores the limits of using their new T5 titled How Much Knowledge Can You Pack Into The Parameters of a Language Model?. model in a context-free QA domain.\nAs with the T5 model itself, it is very interesting to see these one-model-to-rule-them-all architectures as they exhibit some form of generalization.\nI found this paper from Adam Roberts twitter thread which is available here\nCore Idea: This paper will test two main things:\n\nHow well does the model create a knowledge base such that it can answer questions just based on this base and no other information.\nDo model with more parameters store more information? Measuring knowledge retreiving ability is used to check this point.\n\n\n\n\nPaper Introduction\n\nReading Comprehension: Given a question and context, lookup and give the answer.\nOpen domain question answering: Random context-independent questions. It is given entire context(all the information possible in the world) and the model is expected to deduce the answer. Open book exam.\nHere, problem is similar to open book exam + no context given at all. Model should retreive info from parameters and return the values. Closed book exam.\nT5: Treat every NLP task as text-to-text problem using encoder decoder Transformer.\nFor natural questions dataset, evaluation is done as follows:\nFirst method:\n\nIgnore all “unanswerable” and “long answer” type questions.\nmodel trained to output single answer\nQuestions with answers longer than 5 tokens are ignored\nAnswers normalized before comparsion\nAnswer is correct if it matches any of the annotated answers\n\nSecond method:\n\nConsidered correct only if model predicts all the answers correctly\n\nFor fine tuning, use AdaFactor Optimizer(need to read more about this one)\n\n\n\nResults\n\nSOTA on Natural Questions(NQ) and WebQuestions(WQ) dataset. Worst performance on TriviaQA(TQA).\nPerformance increases with model size.\nGuu et all(2020) performs better than T5 on NQ and WQ. Need to read this paper as well. It\n\nRetreives Revevant documents\nAnswers questions in end-to-end fashion\n\nClosed-book model seem to perform on par with open-book models, leading to new research directions.\nFor multiple answer type questions, T5 lower than SOTA BUT much better than baseline that was published with the paper. Therefore, T5 can perform well on these types of questions as well.\n\n\n\nDrawbacks\n\nModel is far too expensive to train.\nOpen-book models provide some indication of what information was used to answer the problem. HOWEVER, T5 just has a distribution over parameters that cannot be interpreted.\nMLE does not gurantee the model will learn a fact. Therefore, difficult to ensure the model learns specific information during pre-training\nMeasure and improve performance on difficult QA tasks like DROP, which needs reasoning ability."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html",
    "href": "posts/2020-03-12-tutorial.html",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "This is inspired from Dr. Andrew Gelman’s case study, which can be found here. Specifically:\n\nThis is heavily inspired by Colin Caroll’s Blog present here. A lot of the plotting code from his blog post has been reused.\nJosh Duncan’s blog post on the same topic which can be found here.\n\nThis is not a novel solution. It is merely a replication of Dr. Gelman’s blog in PyMC3.\n\n\n\nThis is based on a popular blog post by Dr. Andrew Gelman. Here, we are given data from professional golfers on the proportion of success putts from a number of tries. Our aim is to identify:\n\nCan we model the probability of success in golf putting as a function of distance from the hole?\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\nThe source repository is present here\n\ndata = np.array([[2,1443,1346],\n[3,694,577],\n[4,455,337],\n[5,353,208],\n[6,272,149],\n[7,256,136],\n[8,240,111],\n[9,217,69],\n[10,200,67],\n[11,237,75],\n[12,202,52],\n[13,192,46],\n[14,174,54],\n[15,167,28],\n[16,201,27],\n[17,195,31],\n[18,191,33],\n[19,147,20],\n[20,152,24]])\n\ndf = pd.DataFrame(data, columns=[\n    'distance', \n    'tries', \n    'success_count'\n])\n\n\ndf\n\n\n\n\n\n\n\n\n\ndistance\ntries\nsuccess_count\n\n\n\n\n0\n2\n1443\n1346\n\n\n1\n3\n694\n577\n\n\n2\n4\n455\n337\n\n\n3\n5\n353\n208\n\n\n4\n6\n272\n149\n\n\n5\n7\n256\n136\n\n\n6\n8\n240\n111\n\n\n7\n9\n217\n69\n\n\n8\n10\n200\n67\n\n\n9\n11\n237\n75\n\n\n10\n12\n202\n52\n\n\n11\n13\n192\n46\n\n\n12\n14\n174\n54\n\n\n13\n15\n167\n28\n\n\n14\n16\n201\n27\n\n\n15\n17\n195\n31\n\n\n16\n18\n191\n33\n\n\n17\n19\n147\n20\n\n\n18\n20\n152\n24\n\n\n\n\n\n\n\n\nThe variables have the following format:\n\n\n\nVariable\nUnits\nDescription\n\n\n\n\ndistance\nfeet\nDistance from the hole for the putt attempt\n\n\ntries\ncount\nNumber of attempts at the chosen distance\n\n\nsuccess_count\ncount\nThe total successful putts\n\n\n\nLets try to visualize the dataset:\n\ndf['success_prob'] = df.success_count / df.tries\n\n\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(x='distance', y='success_prob', data=df, s=200)\nax.set(xlabel='Distance from hole(ft)', ylabel='Probability of Success')\n\n[Text(0, 0.5, 'Probability of Success'),\n Text(0.5, 0, 'Distance from hole(ft)')]\n\n\n\n\n\n\n\n\n\nWe can notice that the probability of success decreases as the distance increases."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#disclaimer",
    "href": "posts/2020-03-12-tutorial.html#disclaimer",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "This is inspired from Dr. Andrew Gelman’s case study, which can be found here. Specifically:\n\nThis is heavily inspired by Colin Caroll’s Blog present here. A lot of the plotting code from his blog post has been reused.\nJosh Duncan’s blog post on the same topic which can be found here.\n\nThis is not a novel solution. It is merely a replication of Dr. Gelman’s blog in PyMC3."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#problem",
    "href": "posts/2020-03-12-tutorial.html#problem",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "This is based on a popular blog post by Dr. Andrew Gelman. Here, we are given data from professional golfers on the proportion of success putts from a number of tries. Our aim is to identify:\n\nCan we model the probability of success in golf putting as a function of distance from the hole?"
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#eda",
    "href": "posts/2020-03-12-tutorial.html#eda",
    "title": "Bayesian Golf Putting Model",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\nThe source repository is present here\n\ndata = np.array([[2,1443,1346],\n[3,694,577],\n[4,455,337],\n[5,353,208],\n[6,272,149],\n[7,256,136],\n[8,240,111],\n[9,217,69],\n[10,200,67],\n[11,237,75],\n[12,202,52],\n[13,192,46],\n[14,174,54],\n[15,167,28],\n[16,201,27],\n[17,195,31],\n[18,191,33],\n[19,147,20],\n[20,152,24]])\n\ndf = pd.DataFrame(data, columns=[\n    'distance', \n    'tries', \n    'success_count'\n])\n\n\ndf\n\n\n\n\n\n\n\n\n\ndistance\ntries\nsuccess_count\n\n\n\n\n0\n2\n1443\n1346\n\n\n1\n3\n694\n577\n\n\n2\n4\n455\n337\n\n\n3\n5\n353\n208\n\n\n4\n6\n272\n149\n\n\n5\n7\n256\n136\n\n\n6\n8\n240\n111\n\n\n7\n9\n217\n69\n\n\n8\n10\n200\n67\n\n\n9\n11\n237\n75\n\n\n10\n12\n202\n52\n\n\n11\n13\n192\n46\n\n\n12\n14\n174\n54\n\n\n13\n15\n167\n28\n\n\n14\n16\n201\n27\n\n\n15\n17\n195\n31\n\n\n16\n18\n191\n33\n\n\n17\n19\n147\n20\n\n\n18\n20\n152\n24\n\n\n\n\n\n\n\n\nThe variables have the following format:\n\n\n\nVariable\nUnits\nDescription\n\n\n\n\ndistance\nfeet\nDistance from the hole for the putt attempt\n\n\ntries\ncount\nNumber of attempts at the chosen distance\n\n\nsuccess_count\ncount\nThe total successful putts\n\n\n\nLets try to visualize the dataset:\n\ndf['success_prob'] = df.success_count / df.tries\n\n\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(x='distance', y='success_prob', data=df, s=200)\nax.set(xlabel='Distance from hole(ft)', ylabel='Probability of Success')\n\n[Text(0, 0.5, 'Probability of Success'),\n Text(0.5, 0, 'Distance from hole(ft)')]\n\n\n\n\n\n\n\n\n\nWe can notice that the probability of success decreases as the distance increases."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#geometry-based-model",
    "href": "posts/2020-03-12-tutorial.html#geometry-based-model",
    "title": "Bayesian Golf Putting Model",
    "section": "Geometry based Model",
    "text": "Geometry based Model\n\nWe’ll try to accomodate the physics associated with the problem. Specically, we assume:\n\nAssumptions\n\nThe golfers can hit the ball in any direction with some small error. This error could be because of inaccuracy, errors in the human, etc.\nThis error refers to the angle of the shot.\nWe assume the angle is normally distributed.\n\n\nImplications\n\nThe ball goes in whenever the angle is small enough for it to hit the cup of the hole!\nLonger putt \\(\\implies\\) Larger error \\(\\implies\\) Lower success rate than shorter putt\n\nFrom Dr. Gelman’s blog, we obtain the formula as:\n\n\\(Pr(|angle| &lt; sin^{-1}(\\frac{(R-r)}{x})) = 2\\phi\\big(\\frac{sin^{-1}\\frac{R-r}{x}}{\\sigma}\\big) - 1\\)\n\n\\(\\phi \\implies\\) Cumulative Normal Distribution Function.\nHence, our model will now have two big parts:\n\\[y_j \\sim binomial(n_j, p_j)\\]\n\\[p_j = 2\\phi\\big(\\frac{sin^{-1}\\frac{R-r}{x}}{\\sigma}\\big) - 1\\]\nTypically, the diameter of a golf ball is 1.68 inches and the cup is 4.25 inches i.e\n\\[r = 1.68 \\text{inch}\\] \\[R = 4.25 \\text{inch}\\]\n\nball_radius = (1.68/2)/12\ncup_radius = (4.25/2)/12\n\n\ndef calculate_prob(angle, distance):\n    \"\"\"\n    Calculate probability that the ball with fall in the hole given the angle of the shot \n    and the distance from the hole.\n    \"\"\"\n    rad = angle * np.pi / 180.0\n    arcsin = np.arcsin((cup_radius - ball_radius)/ distance)\n    return 2 * scipy.stats.norm(0, rad).cdf(arcsin) - 1\n\n\nplt.figure(figsize=(16, 6))\nls = np.linspace(0, df.distance.max(), 200)\nax = sns.scatterplot(\n    x='distance', \n    y='success_prob', \n    data=df, \n    s=100,\n    legend='full'\n)\nfor angle in [0.5, 1, 2, 5, 20]:\n    ax.plot(\n        ls, \n        calculate_prob(angle, ls), \n        label=f\"Angle={angle}\"\n    )\nax.set(\n    xlabel='Distance from hole(ft)', \n    ylabel='Probability of Success'\n)\nax.legend()\n\n\n\n\n\n\n\n\nLet us now add this to our model!\n\nimport theano.tensor as tt\n\n\ndef calculate_phi(num):\n    \"cdf for standard normal\"\n    q = tt.erf(num / tt.sqrt(2.0)) # ERF is the Gaussian Error \n    return (1.0 + q) / 2.\n\n\nwith pm.Model() as model:\n    angle_of_shot_radians = pm.HalfNormal('angle_of_shot_radians')\n    angle_of_shot_degrees = pm.Deterministic(\n        'angle_of_shot_degrees',\n        (angle_of_shot_radians * 180.0) / np.pi\n    )\n    p_ball_goes_in = pm.Deterministic(\n        'p_ball_goes_in',\n        2 * calculate_phi(\n                tt.arcsin(\n                    (cup_radius - ball_radius)/ df.distance\n                ) / angle_of_shot_radians\n            )\n        ) - 1\n    p_success = pm.Binomial(\n        'p_success',\n        n=df.tries, \n        p=p_ball_goes_in, \n        observed=df.success_count\n    )\n\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\nwith model:\n    trace = pm.sample(4000, tune=1000, chains=4)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nERROR (theano.gof.opt): Optimization failure due to: local_grad_log_erfc_neg\nERROR (theano.gof.opt): node: Elemwise{true_div}(Elemwise{mul,no_inplace}.0, Elemwise{erfc,no_inplace}.0)\nERROR (theano.gof.opt): TRACEBACK:\nERROR (theano.gof.opt): Traceback (most recent call last):\n  File \"/home/goodhamgupta/shubham/blog/_notebooks/.env/lib/python3.6/site-packages/theano/gof/opt.py\", line 2034, in process_node\n    replacements = lopt.transform(node)\n  File \"/home/goodhamgupta/shubham/blog/_notebooks/.env/lib/python3.6/site-packages/theano/tensor/opt.py\", line 6789, in local_grad_log_erfc_neg\n    if not exp.owner.inputs[0].owner:\nAttributeError: 'NoneType' object has no attribute 'owner'\n\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [angle_of_shot_radians]\nSampling 4 chains, 0 divergences: 100%|██████████| 20000/20000 [00:10&lt;00:00, 1943.54draws/s]\nThe acceptance probability does not match the target. It is 0.8844154441842546, but should be close to 0.8. Try to increase the number of tuning steps.\n\n\n\npm.summary(trace).head(2)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhpd_3%\nhpd_97%\nmcse_mean\nmcse_sd\ness_mean\ness_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nangle_of_shot_radians\n0.027\n0.000\n0.026\n0.027\n0.0\n0.0\n6641.0\n6641.0\n6641.0\n10874.0\n1.0\n\n\nangle_of_shot_degrees\n1.527\n0.023\n1.484\n1.570\n0.0\n0.0\n6641.0\n6641.0\n6641.0\n10874.0\n1.0\n\n\n\n\n\n\n\n\n\npm.plot_posterior(trace['angle_of_shot_degrees'])\n\narray([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdfe4c24f60&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\nFrom the above results, we can see:\n\nPyMC3 has estimated\n\n\\(angle_of_shot_degrees\\) to be \\(1.53 \\pm 0.023\\)\n\nThe MCSE is almost 0 \\(\\implies\\) The simulation has run long enough for the chains to converge.\n\\(r\\_hat = 1.0\\) tells us that the chains have mixed well i.e hairy hedgehog pattern.\n\nLet’s visualize the fit with this new model:\n\ngeo_model_prob = calculate_prob(\n    trace['angle_of_shot_degrees'].mean(), \n    df.distance\n)\n\n\nsns.set()\nplt.figure(figsize=(16, 6))\n\nax = sns.scatterplot(x='distance', y=df.success_prob, data=df, s=200, label='Actual')\n\nsns.scatterplot(x='distance', y=df.posterior_success_prob, data=df, label='Logistic Regression',ax=ax, color='red', s=100)\nsns.scatterplot(x='distance', y=geo_model_prob, data=df, label='Geometry based ',ax=ax, color='orange', s=100)\n\nsns.lineplot(x='distance', y=df.posterior_success_prob, data=df,ax=ax, color='red')\nsns.lineplot(x='distance', y=geo_model_prob, data=df,ax=ax, color='orange')\n\nax.set(xlabel='Distance from hole(ft)', ylabel='Probability of Success')\n\n[Text(0, 0.5, 'Probability of Success'),\n Text(0.5, 0, 'Distance from hole(ft)')]\n\n\n\n\n\n\n\n\n\n\nWe can see that the geometry based model fits better than the logistic regression model.\nWhile this model is not completely accurate, it suggests that angle is a good variable to model the problem. Using this model, we can be more confident about extrapolating the data.\nFor the same 50ft putt, the probability now is:\n\n\nimport scipy\nlr_result = np.round(\n    100 * scipy.special.expit(2.223 + -0.255 * 50).mean(),\n    5\n)\ngeo_result = np.round(\n    100 * calculate_prob(\n        trace['angle_of_shot_degrees'].mean(), \n        50\n    ).mean(),\n    5\n)\n\nprint(\n    f\"Logistic Regression Model: {lr_result}% \\n\"\\\n    f\"Geometry Based Model: {geo_result}%\"\n)\n\nLogistic Regression Model: 0.00268% \nGeometry Based Model: 6.40322%"
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#new-data",
    "href": "posts/2020-03-12-tutorial.html#new-data",
    "title": "Bayesian Golf Putting Model",
    "section": "New Data!",
    "text": "New Data!\nMark Broadie obtained new data about the golfers. Let’s see how our model performs on this new dataset.\nFirst, we’ll look at the summary of the dataset.\n\n#  golf putting data from Broadie (2018)\nnew_golf_data = np.array([\n[0.28, 45198, 45183],\n[0.97, 183020, 182899],\n[1.93, 169503, 168594],\n[2.92, 113094, 108953],\n[3.93, 73855, 64740],\n[4.94, 53659, 41106],\n[5.94, 42991, 28205],\n[6.95, 37050, 21334],\n[7.95, 33275, 16615],\n[8.95, 30836, 13503],\n[9.95, 28637, 11060],\n[10.95, 26239, 9032],\n[11.95, 24636, 7687],\n[12.95, 22876, 6432],\n[14.43, 41267, 9813],\n[16.43, 35712, 7196],\n[18.44, 31573, 5290],\n[20.44, 28280, 4086],\n[21.95, 13238, 1642],\n[24.39, 46570, 4767],\n[28.40, 38422, 2980],\n[32.39, 31641, 1996],\n[36.39, 25604, 1327],\n[40.37, 20366, 834],\n[44.38, 15977, 559],\n[48.37, 11770, 311],\n[52.36, 8708, 231],\n[57.25, 8878, 204],\n[63.23, 5492, 103],\n[69.18, 3087, 35],\n[75.19, 1742, 24],\n])\n\nnew_df = pd.DataFrame(\n    new_golf_data, \n    columns=['distance', 'tries', 'success_count']\n)\n\n\nnew_geo_model_prob = calculate_prob(\n    trace['angle_of_shot_degrees'].mean(), \n    new_df.distance\n)\n\n\nnew_df['success_prob'] = new_df.success_count / new_df.tries\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(x='distance', y='success_prob', data=df, label='Old Dataset', s=200)\nsns.scatterplot(x='distance', y='success_prob', data=new_df,label='New Dataset', s=200, ax=ax)\nsns.scatterplot(x='distance', y=new_geo_model_prob, data=new_df, label='Geometry based Model ',ax=ax, color='red', s=100)\nax.set(\n    xlabel='Distance from hole(ft)', \n    ylabel='Probability of Success'\n)\nplt.setp(ax.get_legend().get_texts(), fontsize='25')\n\n\n\n\n\n\n\n\nWe can see:\n\nSuccess rate is similar in the 0-20 feet range for both datasets.\nBeyond 20 ft, success rate is lower than expected. These attempts are more difficult, even after we have accounted for increased angular precision."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#moar-features",
    "href": "posts/2020-03-12-tutorial.html#moar-features",
    "title": "Bayesian Golf Putting Model",
    "section": "Moar features!",
    "text": "Moar features!\nTo get the ball in, along with the angle, we should also need to take into account if the ball was hit hard enough.\nFrom Colin Caroll’s Blog, we have the following: &gt; Mark Broadie made the following assumptions - If a putt goes short or more than 3 feet past the hole, it will not go in. - Golfers aim for 1 foot past the hole - The distance the ball goes, \\(u\\), is distributed as: \\[ u \\sim \\mathcal{N}\\left(1 + \\text{distance}, \\sigma_{\\text{distance}} (1 + \\text{distance})\\right), \\] where we will learn \\(\\sigma_{\\text{distance}}\\).\nAfter working through the geometry and algebra, we get:\n\\[P(\\text{Good shot}) = \\bigg(2\\phi\\big(\\frac{sin^{-1}(\\frac{R-r}{x})}{\\sigma_{angle}}\\big)-1\\bigg)\\bigg(\\phi\\bigg(\\frac{2}{(x+1)\\sigma_{distance}}\\bigg) - \\phi\\bigg(\\frac{-1}{(x+1)\\sigma_{distance}}\\bigg)\\bigg)\\]\nLet’s write this down in PyMC3\n\nOVERSHOT = 1.0\nDISTANCE_TOLERANCE = 3.0\ndistances = new_df.distance.values\nwith pm.Model() as model:\n    angle_of_shot_radians = pm.HalfNormal('angle_of_shot_radians')\n    angle_of_shot_degrees = pm.Deterministic(\n        'angle_of_shot_degrees',\n        (angle_of_shot_radians * 180.0) / np.pi\n    )\n    \n    variance_of_distance = pm.HalfNormal('variance_of_distance')\n    p_good_angle = pm.Deterministic(\n        'p_good_angle',\n        2 * calculate_phi(\n                tt.arcsin(\n                    (cup_radius - ball_radius)/ distances\n                ) / angle_of_shot_radians\n            )\n        ) - 1\n    p_good_distance = pm.Deterministic(\n        'p_good_distance',\n        calculate_phi(\n            (DISTANCE_TOLERANCE - OVERSHOT) / ((distances + OVERSHOT) * variance_of_distance)) \n        - calculate_phi(\n            -OVERSHOT / ((distances + OVERSHOT) * variance_of_distance))\n\n    )\n    p_success = pm.Binomial(\n        'p_success',\n        n=new_df.tries, \n        p=p_good_angle * p_good_distance, \n        observed=new_df.success_count\n    )\n    \n\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\nwith model:\n    trace = pm.sample(1000, tune=1000, chains=4)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [variance_of_distance, angle_of_shot_radians]\nSampling 4 chains, 0 divergences: 100%|██████████| 8000/8000 [00:08&lt;00:00, 969.28draws/s] \nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\npm.summary(trace).head(3)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhpd_3%\nhpd_97%\nmcse_mean\nmcse_sd\ness_mean\ness_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nangle_of_shot_radians\n0.013\n0.000\n0.013\n0.013\n0.0\n0.0\n865.0\n865.0\n862.0\n1109.0\n1.0\n\n\nangle_of_shot_degrees\n0.761\n0.003\n0.755\n0.768\n0.0\n0.0\n865.0\n865.0\n862.0\n1109.0\n1.0\n\n\nvariance_of_distance\n0.137\n0.001\n0.136\n0.138\n0.0\n0.0\n855.0\n855.0\n855.0\n1186.0\n1.0\n\n\n\n\n\n\n\n\n\npm.plot_posterior(trace['variance_of_distance'])\n\narray([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdff74693c8&gt;],\n      dtype=object)\n\n\n\n\n\n\n\n\n\n\nwith model:\n    distance_posterior = pm.sample_posterior_predictive(trace)\n\n100%|██████████| 4000/4000 [00:04&lt;00:00, 846.25it/s]\n\n\n\ndef calculate_prob_distance(angle, distance, ls):\n    \"\"\"\n    Calculate the probability the ball will land inside the hole\n    given the variance in angle and distance.\n    \n    NOTE: Adapted from Colin Carroll's Blog.\n    \"\"\"\n    norm = scipy.stats.norm(0, 1)\n    prob_angle = 2 * norm.cdf(\n        np.arcsin((cup_radius - ball_radius) / ls) / angle) - 1\n    prob_distance_one = norm.cdf(\n        (DISTANCE_TOLERANCE - OVERSHOT) / ((ls + OVERSHOT) * distance)\n    )\n    prob_distance_two = norm.cdf(-OVERSHOT / ((ls + OVERSHOT) * distance))\n    prob_distance = prob_distance_one - prob_distance_two\n    \n    return prob_angle * prob_distance\n\n\nls = np.linspace(0, new_df.distance.max(), 200)\n\n\nnew_df['success_prob'] = new_df.success_count / new_df.tries\nsns.set()\nplt.figure(figsize=(16, 6))\nax = sns.scatterplot(\n    x='distance', \n    y='success_prob',\n    data=new_df,\n    label='Actual', \n    s=200\n)\nsns.scatterplot(\n    x='distance', \n    y=new_geo_model_prob, \n    data=new_df, \n    label='Angle only Model',\n    ax=ax, \n    color='red', \n    s=100\n)\n\nsns.scatterplot(\n    x='distance', \n    y=calculate_prob_distance(\n        trace['angle_of_shot_radians'].mean(), \n        trace['variance_of_distance'].mean(),\n        new_df.distance\n    ), \n    data=new_df, \n    label='Distance + Angle based Model ',\n    ax=ax, \n    color='black', \n    s=100\n)\nax.set(\n    xlabel='Distance from hole(ft)', \n    ylabel='Probability of Success'\n)\n\nplt.setp(ax.get_legend().get_texts(), fontsize='25')\n\n\n\n\n\n\n\n\nFrom the graph, we can conclude that:\n\nThe model is good at distance lower than 10 ft and distances higher than 40ft.\nThere is some mismatch between 10ft to 40ft, but overall this is a good fit."
  },
  {
    "objectID": "posts/2020-03-12-tutorial.html#whats-the-point",
    "href": "posts/2020-03-12-tutorial.html#whats-the-point",
    "title": "Bayesian Golf Putting Model",
    "section": "What’s the point?",
    "text": "What’s the point?\nUsing Bayesian analysis, we want to be able to quantify the unvertainity with each of our predictions. Since each prediction is a distribution, we can utilize this to see where the putts will fall if they do not fall in the hole.\n\ndef simulate_from_distance(trace, distance_to_hole, trials=10_000):\n    n_samples = trace['angle_of_shot_radians'].shape[0]\n\n    idxs = np.random.randint(0, n_samples, trials)\n    variance_of_shot = trace['angle_of_shot_radians'][idxs]\n    variance_of_distance = trace['variance_of_distance'][idxs]\n\n    theta = np.random.normal(0, variance_of_shot)\n    distance = np.random.normal(distance_to_hole + OVERSHOT, (distance_to_hole + OVERSHOT) * variance_of_distance)\n\n    final_position = np.array([distance * np.cos(theta), distance * np.sin(theta)])\n\n    made_it = np.abs(theta) &lt; np.arcsin((cup_radius - ball_radius) / distance_to_hole)\n    made_it = made_it * (final_position[0] &gt; distance_to_hole) * (final_position[0] &lt; distance_to_hole + DISTANCE_TOLERANCE)\n    \n    _, ax = plt.subplots()\n\n    ax.plot(0, 0, 'k.', lw=1, mfc='black', ms=150 / distance_to_hole)\n    ax.plot(*final_position[:, ~made_it], '.', alpha=0.1, mfc='r', ms=250 / distance_to_hole, mew=0.5)\n    ax.plot(distance_to_hole, 0, 'ko', lw=1, mfc='black', ms=350 / distance_to_hole)\n\n    ax.set_facecolor(\"#e6ffdb\")\n    ax.set_title(f\"Final position of {trials:,d} putts from {distance_to_hole}ft.\\n({100 * made_it.mean():.1f}% made)\")\n    return ax\n\nsimulate_from_distance(trace, distance_to_hole=10);"
  },
  {
    "objectID": "posts/2020-01-14-first-post.html",
    "href": "posts/2020-01-14-first-post.html",
    "title": "Yo!",
    "section": "",
    "text": "Yo!\nSuper excited to finally get my own blog. Hope to write out those long pending articles ASAP now. Stay tuned!"
  },
  {
    "objectID": "posts/2020-05-11-longformer.html",
    "href": "posts/2020-05-11-longformer.html",
    "title": "LongFormer",
    "section": "",
    "text": "The NLP world had its ImageNet moment with the introduction of the Transformer in the paper Attention is All you Need.\nThe ability to be able to process multiple words/tokens in parallel and train models without labeled data(using self-attention) led to the creation of multiple models which gave us SOTA results on many interesting tasks such as Question Answering, Summarization, etc.\nHowever, the biggest drawback is the Transformer architecture is the limitation it has on the number of tokens it can process at a once, due to exponentially increasing memory and compute requirements(typically about 512 tokens), causing the performance to deteriorate over large documents.\nLongformer by the team at Allen AI aims to address this problem and demonstrate it’s application to do transfer learning for large documents.\nOther approaches to are described in recent work such as Transformer XL, Blockwise, Reformer, etc. Their characteristics are mentioned below:\n\n\n\n\nComparison"
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#sliding-window-attention",
    "href": "posts/2020-05-11-longformer.html#sliding-window-attention",
    "title": "LongFormer",
    "section": "Sliding Window Attention",
    "text": "Sliding Window Attention\n\nTLDR : Similar to kernels for CNN which apply a matrix operation to a set of pixels and move onto the next set, apply attention to tokens in current window only.\nIn this, we change the attention objective to only focus on the tokens that occur in a context window \\(w\\).\nEach token will be able to attend to \\(\\frac{1}{2}w\\) number of tokens to it’s left and right.\nQuestion: But doesn’t this limit the number of tokens being taken into account to only the tokens in the window?\n\nYes, it does. This is why we stack multiple layers of self-attention. As shown in the image below, the green neuron learns from the first 3 tokens(Lionel, Messi, is). However, the brown neuron learns from the green, yellow, and red neuron, who together learn from the first 5 tokens. This way, we can apply attention to long sequences(Lionel, Messi, is, the, true).\n\nAs with the CNN, we will have \\(l\\) layers to this sliding window attention(multi-head attention) implemented to learn low level and high-level features. A balance should be found between the number of layers \\(l\\)(efficiency) and the window size \\(w\\)(model representation capacity).\n\n\n\n\nSliding Window Attention\n\n\n\nPros: Reduces computation from \\(O(n^2)\\) to \\(O(n*w)\\) i.e the computation complexity will only scale linearly now.\nCons: To learn dependencies for a large sequence, we would either have to increase the window size \\(w\\) or increase the number of layers \\(l\\), both of which will cause an increase in the amount of memory and processing power required to train and test the model."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#dilated-sliding-window",
    "href": "posts/2020-05-11-longformer.html#dilated-sliding-window",
    "title": "LongFormer",
    "section": "Dilated Sliding Window",
    "text": "Dilated Sliding Window\n\nTLDR: Use dilation instead of window attention i.e for some particular window size, take alternate elements while performing self-attention.\nTo solve the problem for long sequences, the authors propose that instead of considering all tokens in window \\(w\\), consider alternate(or any number \\(d\\))tokens instead. The range of tokens will now be \\(l * d * w\\), which will be large for even a small value of \\(d\\).\nPros: This small change will allow us to cover a wider range of tokens without significant changes to the architecture.\nCons: Skipping tokens might lead to loss of information in the lower layers which will get propagated to the higher layers. This will lead to unstable training and poor model performance."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#global-attention",
    "href": "posts/2020-05-11-longformer.html#global-attention",
    "title": "LongFormer",
    "section": "Global Attention",
    "text": "Global Attention\n\nTLDR: Use full attention for certain tokens depending on the task. This is an engineering choice.\nIn BERT style models, optimal representation for input sequence varies by task.\n\nFor MLM, local context is used to predict the masked word\nFor classification, [CLS] token is used.\nFor QnA, the question is concatenated with the document to help model learn through self-attention.\n\nThe windowed and dilated attention is not flexible enough to learn task-specific representations.\nHence, for some tokens enable global tokens i.e at these tokens, all tokens in the sequence can attend to it. For classification, enable global attention on the [CLS] token.\nPros:\n\nAdding global attention improves performance for specific tasks. Since these tokens are limited in number, the complexity still stays at \\(O(n)\\).\nIt also increases the representational power of the model.\n\n\n\nLinear Projections\n\nTLDR: Use two sets of Q,K and V matrices, one for sliding window attention, one for global attention.\nAttention is defined as:\n\\[\n\\begin{aligned}\nAttention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n\\end{aligned}\n\\]\nWe will use two different sets of Q,K and V matrices for sliding window and global attention.\n\\(Q_g\\), \\(K_g\\), \\(V_g\\) are initialized with \\(Q_s\\), \\(K_s\\), \\(V_s\\)\n\n\n\n\nBanded Matrix\n\n\n\nBanded Matrix(Source)\n\n\n\n\nCompressed Banded Matrix\n\n\n\nCompressed Banded Matrix(Source)\n\n\n\nCUDA Kernels\n\nOne of the important and interesting contributions of this paper is the implementation of matrix multiplication via CUDA kernels.\nIn the dilated sliding window, the matrix formed is called a band matrix i.e there are diagonal bands of indices that have values and the other values are 0.\nImplementing matrix operations for band matrices using native for loops and via frameworks is not easy and optimized.\nThe authors have provided custom CUDA kernels implemented using TVM for this banded matrix operations.\nAs demonstrated in the image below, the custom CUDA kernels have a significant impact on the time and memory consumption of the model. The kernels and implementation for the longformer are available here. \n\nLongFormer Performance"
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#attention-pattern",
    "href": "posts/2020-05-11-longformer.html#attention-pattern",
    "title": "LongFormer",
    "section": "Attention Pattern",
    "text": "Attention Pattern\n\nIn multi-head attention, each head computes a different score.\nTo get a good representation of all tokens, the authors propose that normal sliding window attention can be used for the lower layers, and dilated sliding window attention can be used the higher layers(top 1-2 layers).\nThe reasoning for this approach is that in the lower layers, the local context is more important, and in the upper layers, the global context is more important. Hence, it is acceptable to skip over a few tokens in the upper layers."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#task-and-datasets",
    "href": "posts/2020-05-11-longformer.html#task-and-datasets",
    "title": "LongFormer",
    "section": "Task and Datasets",
    "text": "Task and Datasets\n\nThe authors focus on character level modeling because the sequences are naturally longer than those of word-level language modeling.\nDatasets that were used are text8 and enwik8."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#training-and-evaluation",
    "href": "posts/2020-05-11-longformer.html#training-and-evaluation",
    "title": "LongFormer",
    "section": "Training and Evaluation",
    "text": "Training and Evaluation\n\nThe model was trained in multiple phases.\n\nThe window and sequence length was increased in each phase. This is to allow local context from tokens to be learned efficiently.\nOverall five training phases used, starting from the token length of 2048 to 23040 (45x more than vanilla BERT).\nTwo models were created for evaluation:\n\nSmall model: 12 layers, 512 hidden size\nLarge model: 30 layers, 512 hidden sizes (2.5x larger)\n\nDuring the model evaluation, the model can run on a sequence length of 32256(63x more than vanilla BERT)."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#results",
    "href": "posts/2020-05-11-longformer.html#results",
    "title": "LongFormer",
    "section": "Results",
    "text": "Results\n\n\n\nResults\n\n\n\nLongformer achieves SOTA using the small models with BPC of 1.10 and 1.00 for text8 and enwik8.\nThe large model was only tested on enwik8 due to the computational cost of training.\nIt’s also important to note that, while the large model did not achieve SOTA, it performs much better than it’s counterparts who have almost 2x more parameters."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#copy-initialization-trick",
    "href": "posts/2020-05-11-longformer.html#copy-initialization-trick",
    "title": "LongFormer",
    "section": "Copy initialization trick",
    "text": "Copy initialization trick\n\nSince the MLM objective pretraining objective is expensive, the authors continue to train from the checkpoints of the RoBERTA model.\nThe attention mechanism is replaced with the new attention module.\nFor the position embeddings:\n\nRoBERTA has position embeddings for 512 tokens.\nLongFormer can support position embeddings for 4096 tokens(larger for larger GPU)\nTo use the weight checkpoints from RoBERTA, instead of random initialization, copy the 512 position embeddings multiple times as analysis of the BERT attention heads showed a strong learned bias to attend to the local context."
  },
  {
    "objectID": "posts/2020-05-11-longformer.html#pretraining",
    "href": "posts/2020-05-11-longformer.html#pretraining",
    "title": "LongFormer",
    "section": "Pretraining",
    "text": "Pretraining\n\nApart from the datasets(Books corpus + English Wikipedia) used in RoBERTA, \\(\\frac{1}{3}^{rd}\\) Realnews dataset was added with tokens larger than 1200.\nBoth models(small and large) trained with varying gradient updates.\n\n\n\n\nCopy init\n\n\n\nMLM BPC for RoBERTA with various model config"
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "",
    "text": "Back on the blog after a long hiatus - this time, I’m shifting gears from just reviewing papers(which are available on my GitHub) to diving deep into hands-on implementations.\nI’ve always been interested in systems programming, but somehow never really picked it up. The rate of progress in the GenAI space has been exponential recently, with players like Google [1] reportedly processing 9.7 trillion tokens a month. Companies are now investing more time and resources in making these Large Language Models as fast and cheap as possible, by improving training and inference efficiency using “moar” compute.\nI briefly spoke about GPU computing last year, and finally decided to learn it this summer. The goal is to eventually be able to implement kernels for fast matmuls, softmax, and FlashAttention."
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html#why-mojo",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html#why-mojo",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "Why Mojo?",
    "text": "Why Mojo?\nI’ve tried learning Rust multiple times, along with a few stints of trying C, C++ and Zig, but I never really felt as comfortable in these languages as I do in Python and Elixir.\nIn early 2023, Modular announced Mojo🔥, a new systems-programming language promising:\n\nPython-like syntax\nSupport for both CPU and GPU architectures\nKernel autofusion\nBuilds on MLIR\nTraits and bounds checking\nInteropeability with PTX, Python, C\n\nModular has since announced Max, their AI inference platform, built on Mojo. The released all kernels available as part of the platform, along with their own version[2] of Sasha Rush’s GPU Puzzles [3] in Mojo. IMO, their kernels were much easier to read compared to CUDA/Triton implementations. I also enjoyed the “Democratising AI Compute”[4] series by Chris Lattner, and thus I decided to learn a bit more about how to write these kernels in Mojo."
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html#gpu-memory",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html#gpu-memory",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "GPU 101",
    "text": "GPU 101\nGPUs (Graphics Processing Units) are massively parallel processors optimized for throughput over latency. In GPU programming we:\n\nLay out data and computation as a grid of thread blocks.\nLaunch a kernel from the CPU (host) to run on the GPU (device).\nExploit thousands of lightweight threads all executing the same code (Single Instruction, Multiple Threads or SIMT).\n\nModern chips had two ways to spend their billions of transistors:\n\nCPUs invest them in large caches, branch predictors and out-of-order logic to minimize latency for one or a few threads.\nGPUs invest them in thousands of simple cores and huge register files to maximize throughput for many threads, assuming those threads can tolerate latency by waiting in parallel.\n\nThe rest of this section unpacks how that single design choice shows up in memory, execution and program flow.\n\n1. Memory hierarchy – hide latency with tons of threads\nCPUs invest transistors in large caches to minimize latency. GPUs take the opposite approach: they use thousands of threads to hide latency instead of avoiding it.\nGPU memory hierarchy (slowest/largest to fastest/smallest):\n\nGlobal (HBM): High Bandwidth Memory—the GPU’s main memory, large but high-latency, visible to all threads\n\nShared (SRAM): fast on-chip memory, ~100x faster than HBM\n\nRegisters: per-thread storage, fastest access, ~1000x faster than HBM\n\n\n\n\nSource: FlashAttention [5]. Metrics shown are for an NVIDIA A100 GPU.\n\n\nThe key insight: when threads wait for slow global memory (~400-800 cycles), the GPU immediately switches to other threads. This keeps compute units busy while data moves through the hierarchy.\n\n\n2. Execution hierarchy – launch enough warps to hide stalls\n\n\n\nGPU Execution Hierachy\n\n\nBuilding from the bottom up:\n\nThread: the basic execution unit with its own registers\nWarp: 32 threads executing the same instruction together (the basic unit of GPU scheduling)\n\nBlock: a group of threads that share shared memory and can synchronize\n\nGrid: a collection of blocks distributed across SMs\n\nGPUs schedule threads in groups of 32 (warps). When one warp stalls on memory, the scheduler switches to another warp instantly. More resident warps = better latency hiding.\n\n\n3. Program flow – CPU launches, GPU streams\n\nThe CPU launches kernels asynchronously and goes back to other work. Inside the GPU each warp executes the same instruction (SIMT). Divergent branches disable some lanes and waste those cores.\n\nHost allocates and copies data to GPU global memory\n\nHost launches the kernel with a specified grid and block size\n\nDevice executes the kernel in parallel across threads\n\nHost retrieves results from GPU memory\n\n\n\nPutting it together\nFast GPU kernels keep cores busy by:\n\nStaging hot data in shared or registers\nLaunching enough threads to mask global-memory latency\nWriting branch-free, data-parallel code\n\nWe will cover the practical implications of the above topics as we go through the puzzles."
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html#infrastructure",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html#infrastructure",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "Infrastructure",
    "text": "Infrastructure\nIf you plan on solving these puzzles, remember to pick a compatible GPU and follow the setup instructions\nI completed the puzzles on a instance with a RTX4090 Ti chip, rented via Prime Intellect at 0.22 $/hr!\nNote: The Modular team has created beautiful Manim visualizations for each puzzle, making the concepts much more intuitive. I’ll walk through these visualizations as we tackle each problem."
  },
  {
    "objectID": "posts/2025-07-06-gpu-puzzles-p1.html#conclusion",
    "href": "posts/2025-07-06-gpu-puzzles-p1.html#conclusion",
    "title": "GPUs go brrr with Mojo - Fundamentals",
    "section": "Conclusion",
    "text": "Conclusion\nWe covered simple algorithms such as map, zip using Mojo, and did some initial work with their LayoutTensor abstraction, which similar to the CuTe library.\nIf you spot mistakes or have better/faster Mojo code, open a PR or ping me on Twitter/X. Happy hacking!\n\n\n\nSource: FlashAttention [5]. Metrics shown are for an NVIDIA A100 GPU.\nGPU Execution Hierachy\nMojo Parameter Syntax"
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html",
    "href": "posts/2020-04-23-tag-lm.html",
    "title": "TagLM",
    "section": "",
    "text": "This is the TagLM paper mentioned in Lecture 13 in the CS224N course title Semi-supervised sequence tagging with bidirectional language models\nThis paper demonstrates how we can use context embeddings from BiLSTM models and use it for sequence labelling tasks"
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html#overview",
    "href": "posts/2020-04-23-tag-lm.html#overview",
    "title": "TagLM",
    "section": "Overview",
    "text": "Overview\n\nExtract word and LM embeddings for every token\nPrepare embedding by concatinating both embeddings\nse them in supervised sequence tagging model"
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html#baseline",
    "href": "posts/2020-04-23-tag-lm.html#baseline",
    "title": "TagLM",
    "section": "Baseline",
    "text": "Baseline\n\nBaseline model is hierachical neural tagging model\nObtain word and character embeddings. Concatenate them to form final embedding.\n\\[ x_k = [c_k;w_k] \\]\nChar embedding: Can be obtained via CNN or RNN\nWord embedding: Use pretrained embeddings\nUse multiple bidirectional RNN to learn context embedding\nFor every token, concatenate forward and backward hidden states at each layer\n2nd layer will use the above output and predict next hidden state\nUse GRU or LSTM depending on task\nUse output of final layer to predict score for each possible tag using dense layer\nBetter to predict tags for full sequence than for a single token\nTHEREFORE, add another layer with parameters for each label bigram.\nCompute sentence conditional random field loss(CRF) using forward-backward algorithm.\nUse Viterbi algorithm to find most likely sequence\n\n\n\nOverview of architecture\n\n\nLM embedding will be created by concatenating forward and backward embeddings. No parameter sharing between these two embeddings."
  },
  {
    "objectID": "posts/2020-04-23-tag-lm.html#analysis",
    "href": "posts/2020-04-23-tag-lm.html#analysis",
    "title": "TagLM",
    "section": "Analysis",
    "text": "Analysis\n\nSecond RNN captures interactions between task specific context\nBackward LM addition has significant performance gains\nModel size makes a difference. Using bigger CNN model lead to  0.3 percent improvement\nAlso tried training the model JUST ON THE CoNLL data. Reduced model size\nIncluding embeddings from this model decreased performance compared to baseline system.\nReplacing task specific RNN with using LM embeddings with a dense layer and CRF reduced performance\nImprovement shown by transferring knowledge from other tasks almost disappers when the initial model is trained on a large dataset."
  },
  {
    "objectID": "posts/2020-08-31-bison.html",
    "href": "posts/2020-08-31-bison.html",
    "title": "BERT + BM25 = BISON",
    "section": "",
    "text": "This paper aims to create a framework to map query and doc into semantic vectors via self-attention models.\nWe cant use prior knowledge about important tokens for models based on self-attention.\n\nWords are split into different tokens using a tokenization mechanism such as WordPiece. We cannot translate word-level knowledge into different tokens.\n\nHowever, from classical information retrieval, we know that prior knowledge about the word is important. For example, ERNIE used a Knowledge Graph to achieve SOTA on several GLUE tasks.\nFurthermore, documents have different fields with varying degrees of importance such as text, header, filetypes, etc. We cannot combine these fields directly because their importance varies for a task.\nKey takeaways:\n\nCombine BM25 to learn attention scores with Query(Q) and Key(K) matrices, which are used in self-attention.\nWord weight sharing to reduce knowledge discrepancy between tokens and words.\nCombine multiple fields by placing different fields in different segments using a BM25F, a variation of BM25."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#overview-of-bison",
    "href": "posts/2020-08-31-bison.html#overview-of-bison",
    "title": "BERT + BM25 = BISON",
    "section": "Overview of BISON",
    "text": "Overview of BISON\n\n\n\nBISON Architecture\n\n\n\nThe framework has 4 important parts:\n\nWord level BM25: In this, we prepend the CLS token to the query and use combined fields representation for the documents.\nToken level representation: As is the norm, we will use the token, position and segment embedding\nBISON Encoder: This will encode the query q and the document d into semantic spacy by siamese structure making it possible to serve the model online. The architecture consists of 3 stacked BISON layers.\nScoring: The documents are scored using the cosine similarity metric."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#bison-encoder-weighted-self-attention",
    "href": "posts/2020-08-31-bison.html#bison-encoder-weighted-self-attention",
    "title": "BERT + BM25 = BISON",
    "section": "BISON Encoder: Weighted Self Attention",
    "text": "BISON Encoder: Weighted Self Attention\n\nAs we know from the original “Attention” paper, attention is computed using the query, key, and value matrices.\nTo the above, we will add the importance of tokens via BM25. We will introduce w_i and multiply with above attention to get new attention score i.e Weighted Self Attention \\[ A_{ij}^w = w_j\\frac{q_i.k_j^T}{\\sqrt{d}} \\]\n\n\n\n\nWeighted Self Attention\n\n\n\nMathematically, it is represented as:\n\n\\[WeightedSelfAttention(Q,K,W,V) = softmax(W (.) \\frac{QK^T}{\\sqrt{d}}V\\]\n\nWSA is the main block unit. Multiple such units are tacked to get the multi-head structure.\nRescaling by \\(W^o\\), we get Complex Weighted Self Attention(CWSA).\nA fully connected layer is added. In both CWSA and fully connected layer, layer norm and residual connections are used\n\n\\[CWSA = Concat(WeightedSelfAttention1,... WeightedSelfAttention, n)W^o\\]\n\\[CWSA_{out}=LayerNorm(CWSA + X)\\]\n\\[BISONEncoder = LayerNorm(CWSA_{out} + FeedForward(CWSA_{out}))\\]"
  },
  {
    "objectID": "posts/2020-08-31-bison.html#bm25-weight-generation",
    "href": "posts/2020-08-31-bison.html#bm25-weight-generation",
    "title": "BERT + BM25 = BISON",
    "section": "BM25 Weight generation",
    "text": "BM25 Weight generation\n\nUse BM25 for weight scores in query and BM25F for weight scores in multi-field documents\nBM25F, a variation of BM25, is for documents with different fields, each having different importance in terms of relevance saturation and length normalization. Find additional details in the file here.\n\n\nInherent Query BM25\n\nFor a given query, BM25 is calculated within the query.\n\n\\(l_q\\): query length\n\\(avl_q\\): query average length along collection\n\n\n\\[\nw_i^{BM25} = idf_i \\frac{tf_i}{tf_i + k_1(1-b+b \\frac{l_q}{avl_q})}\n\\]\n\n\nInherent Document BM25F\n\nBM25F is implemented by assigning different degrees of importance to the different zones in a document such as title, header, footer, filetype, text, etc. For a \\(word_j\\) in a document field \\(c\\), it’s frequency \\(f_j^c\\) is defined as:\n\n\\[\natf_j^c = \\frac{fw_c . tf_j^c}{1.0 + fln_c . (\\frac{fl_c}{avl_c}-1.0)}\n\\]\n\nThe corresponding BM25F score is computed as\n\n\\[\nw_j^{BM25F} = idf_j\\frac{atf_j}{k_1 + atf_j}\n\\]"
  },
  {
    "objectID": "posts/2020-08-31-bison.html#whole-word-weight-sharing",
    "href": "posts/2020-08-31-bison.html#whole-word-weight-sharing",
    "title": "BERT + BM25 = BISON",
    "section": "Whole word weight sharing",
    "text": "Whole word weight sharing\n\nBERT uses wordpiece to produce tokens from raw text. However, because of this, we cannot directly apply the prior knowledge we obtained from B-52.\nSolution: Assign the same word weight to all tokens for a given word. This way, a token might have a different weight depending on the context of the given word."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#combined-fields-representation",
    "href": "posts/2020-08-31-bison.html#combined-fields-representation",
    "title": "BERT + BM25 = BISON",
    "section": "Combined Fields Representation",
    "text": "Combined Fields Representation\n\nDocuments typically consist of different fields, each of which provides complementary information. Thus, these fields need to be taken into consideration. Typical fields considered are:\n\nPrimitive Fields(Title, URL, header, etc.)\nOther fields(anchor, click signal via parsing search log, etc)\n\nFor the experiment, only the following fields were picked for performance reasons(the body has too much text to encode in a single space):\n\nTitle\nAnchor\nURL\nClicked query\n\nFor each field, we learn their representation individually and combine them. Further, we also restrict the number of tokens for each of the above fields to a total of 128 tokens.\n\n20 tokens each for Title, Anchor and URL.\nOnly consider the top 5 clicked queries for a maximum of 68 tokens.\n\nFor given fields, the document representation \\(\\phi(D)\\) is given by:\n\n\\[\n\\phi(D) = A_{f_i}(\\phi_{F_1}(F_1)+ \\phi_{F_2}(F_2)+...+\\phi_{F_n}(F_n))\n\\]\n\nHere,\n\n\\(F_i\\) is the field\n\\(\\phi(F_1)\\) denotes the representation learned for each field \\(F_i\\)\n\\(A_{f_i}\\) is a function to aggregate all representations\n\nThe remaining tokens(512-128=384) are used to encode the query."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#optimization",
    "href": "posts/2020-08-31-bison.html#optimization",
    "title": "BERT + BM25 = BISON",
    "section": "Optimization",
    "text": "Optimization\n\nThe [CLS] token from the last layer is used as a representation for the query and the document\nMatching score \\(s\\) is computed as: \\[\ns = cos(BISON(query)_{last cls}, BISON(document)_{last cls})\n\\]\nCross entropy loss is used to determine if the retrieved document is relevant or not. \\[\nLoss = -ylog(\\delta(w.s+b))-(1-y)log(1 - \\delta(w.s+b))\n\\]"
  },
  {
    "objectID": "posts/2020-08-31-bison.html#intrinsic-evaluation",
    "href": "posts/2020-08-31-bison.html#intrinsic-evaluation",
    "title": "BERT + BM25 = BISON",
    "section": "Intrinsic evaluation",
    "text": "Intrinsic evaluation\n\nSelected 1400 representative queries and 7 million query document pairs from Bing’s search log\nPerformance-wise, USE performs the worst as it performs well only on homogeneous data, and query document pairs are heterogeneous.\nBISON outperforms all baseline models significantly."
  },
  {
    "objectID": "posts/2020-08-31-bison.html#ms-marco",
    "href": "posts/2020-08-31-bison.html#ms-marco",
    "title": "BERT + BM25 = BISON",
    "section": "MS Marco",
    "text": "MS Marco\n\nSimilar steps followed for document full ranking task on the MS Marco dataset.\nFor each query, the top 1000 documents are returned and MR is used as performance metrics."
  },
  {
    "objectID": "posts/2020-04-20-attention.html",
    "href": "posts/2020-04-20-attention.html",
    "title": "Attention is all you need",
    "section": "",
    "text": "This paper review is following the blog from Jay Alammar’s blog on the Illustrated Transformer. The blog can be found here."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#encoder-and-decoder-stacks",
    "href": "posts/2020-04-20-attention.html#encoder-and-decoder-stacks",
    "title": "Attention is all you need",
    "section": "Encoder and decoder stacks",
    "text": "Encoder and decoder stacks\n\nEncoder: 6 identical layers. 2 sub layers per layer\nFirst: multi-head self attention mechanism\nSecond: Fully connected feed forward network\nApply residual connection for each of the two laters\nApply layer normalization\nDecoder: 6 identical layers. 2 sub layers as above + 1 more which performs multi-head attention over output of encoder stack\nResidual blocks: Present around all 3 sub layers\nLayer normalization: Normalizes input across features instead of normalizing input features across batch dimension(i.e in batch normalization). There is a great overview of normalization layers available by Akash Bindal here.\nModify self-attention sub layer to prevent positions from attending to subsequent positions. Ensures that i output depends only on words before i."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#attention",
    "href": "posts/2020-04-20-attention.html#attention",
    "title": "Attention is all you need",
    "section": "Attention",
    "text": "Attention\n\n3 vectors: Query(Q), Key(K) and Value(V)\nOutput = Weighted sum of values. Weights assigned as a function of query with key.\nScaled dot-product attention and multi-head attention\n\n\n\nTypes of Attention\n\n\nAttention is calculated as:\n\\[\n        Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n   \\]\nDot product attention is faster and more space-efficient than additive attention."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#multi-head-attention",
    "href": "posts/2020-04-20-attention.html#multi-head-attention",
    "title": "Attention is all you need",
    "section": "Multi head attention",
    "text": "Multi head attention\n\nUsing multile q, k and v vectors. Get the final output, concatenate them and get another final projection \\(d_{v}\\).\n$$ MultiHead(Q,K,V) = Concat(head_1,…,head_h)W^O \\\n\\text{where } head_i = Attention(QW_{i}^{Q}, KW_{i}^{K},VW_{i}^{V})\n$$\nDimensions of the key and value matrices will be: \\(d_{k} = d_{v} = d_{model}/h = 64\\)"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#applications-of-attention",
    "href": "posts/2020-04-20-attention.html#applications-of-attention",
    "title": "Attention is all you need",
    "section": "Applications of attention",
    "text": "Applications of attention\n\nEncoder-decoder attention: Q from previours decoder, K and V from output of decoder. Attend to all positions in the input sequence.\nEncoder: Self attentnion laters. Q,K and V from output of previous layer in the encoder. Some talk about leftward flow, didn’t really understand this bit. Will come back to this in sometime."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#position-wise-feed-forward-networks",
    "href": "posts/2020-04-20-attention.html#position-wise-feed-forward-networks",
    "title": "Attention is all you need",
    "section": "Position-wise Feed-Forward Networks",
    "text": "Position-wise Feed-Forward Networks\n\nEach layer contains feed-forward network.\n\\[\n        FFN(x) = max(o, xW_1,+ b_1)W_2 + b_2\n\\]"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#embeddings-and-softmax",
    "href": "posts/2020-04-20-attention.html#embeddings-and-softmax",
    "title": "Attention is all you need",
    "section": "Embeddings and Softmax",
    "text": "Embeddings and Softmax\n\nConvert input and output string to vectors of dim \\(d_{model}\\)\nShare weight matrix between two embedding layers and the pre-softmaax linear transformation"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#positional-encoding",
    "href": "posts/2020-04-20-attention.html#positional-encoding",
    "title": "Attention is all you need",
    "section": "Positional Encoding",
    "text": "Positional Encoding\n\nEncode positions of the tokens for the input and output.\nSame vector size i.e \\(d_{model}\\)\n$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\\n    PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n$$\nMight allow approximation of longer sequence lenghts than seen in the training set"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#why-self-attention",
    "href": "posts/2020-04-20-attention.html#why-self-attention",
    "title": "Attention is all you need",
    "section": "Why self attention?",
    "text": "Why self attention?\n\nTotal computational complexity per layer\nParallel Computation\nPath length between long-range dependencies in the network."
  },
  {
    "objectID": "posts/2020-04-20-attention.html#optimizer",
    "href": "posts/2020-04-20-attention.html#optimizer",
    "title": "Attention is all you need",
    "section": "Optimizer",
    "text": "Optimizer\n\nUse Adam. Vary learning rate according to formula: \\(lrate = d_{model}^{-0.5} . min(step_num^{-0.5}, step_num . warmupsteps^{-1.5})\\)\nIncrease LR for warmup steps, then decrease propotionally to inverse square root of step number. Warmup steps = 4000"
  },
  {
    "objectID": "posts/2020-04-20-attention.html#regularization",
    "href": "posts/2020-04-20-attention.html#regularization",
    "title": "Attention is all you need",
    "section": "Regularization",
    "text": "Regularization\n\nResidual Dropout\nLabel Smoothing: Instead of using 0 and 1 as class labels, allow for some uncertainity in the prediction, and use values like 0.1 and 0.9 for the classes"
  },
  {
    "objectID": "posts/2020-03-14-realm.html",
    "href": "posts/2020-03-14-realm.html",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "",
    "text": "REALM is a paper mentioned in the T5 paper titled: How Much Knowledge Can You Pack Into The Parameters of a Language Model?\nTLDR: This paper retrieves documents that have the information present while solving Question-Answer type problems.\n\nNOTE: This post is more like my running notes while reading the paper than a comprehensive blog. I will update this blog once I learn a little more about the transformer architecture.\n\nIntroduced a latent knowledge retriever, which can attend and retrieve documents over large corpus and can be trained in unsupervised manner using masked language modelling technique and backprop through retreiver which considers lots of docs.\n\n\n\nTraining process for REALM\n\n\nKey point: Train retriever using a performance-based signal from unsupervised text.\nRetrieval based LM =&gt; Moar computational resources =&gt; Moar money\n\nSolution: Computation performed for each doc is cached and can be used again. Best doc selected using Maximum Inner Product Search(MIPS). Read the paper here.\n\nREALM retriever can be used on downstream tasks via transfer learning.\nREALM is SOTA on NQ-Open, WQ and CuratedTrec."
  },
  {
    "objectID": "posts/2020-03-14-realm.html#retreive-then-predict-generative-process",
    "href": "posts/2020-03-14-realm.html#retreive-then-predict-generative-process",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Retreive-then-predict generative process",
    "text": "Retreive-then-predict generative process\n\nTraining: Masked-LM. Fine-tuning: Open QA task\nComputing chance of the document given a question decomposed into two steps:\n\nFunction to be computed: \\[p(y\\|x)\\]\nGiven \\[x\\],retrive documents \\[z\\] from corpus \\[Z\\]. Modelled as: \\[p(z\\|x)\\]\nCondition of both \\[z\\] and \\[x\\] to generate output \\[y\\] i.e \\[p(y\\|z, x)\\]\nOverall likelihood \\[y\\] is generated by treating \\[z\\] as latent variable and marginalizing over all documents \\[z\\]\n\\[\np(y\\|x) = \\sum_{z \\epsilon Z} p(y\\|z, x) * p(z\\|x)\n\\]"
  },
  {
    "objectID": "posts/2020-03-14-realm.html#neural-knowledge-retriever",
    "href": "posts/2020-03-14-realm.html#neural-knowledge-retriever",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Neural Knowledge Retriever",
    "text": "Neural Knowledge Retriever\n\nDense inner product model.\n\\[\n\\begin{aligned}\n    p(z\\|x) = \\frac{exp(f(x,z))}{\\sum_{z'}{exp(f(x,z'))}} \\\\\n    f(x,z) = Embed_{input}(x)^TEmbed_{doc}(z)\n\\end{aligned}\n\\]\n\\[Embed_{input}\\] and \\[Embed_{doc}\\] are embedding functions\n\\[f(x,z)\\] is called relevance score. It is inner product of vector embeddings.\nRelevant Distribution is softmax over all relevance scores\nEmbedding implement using BERT-style transformers. Join using &lt;SEP&gt;, prefix using &lt;CLS&gt; and append &lt;SEP&gt; as the end token. \\[\\begin{aligned}\n        \\\\ join_{BERT}(x) = [CLS]x[SEP]\n        \\\\ join_{BERT}(x_1, x_2) = [CLS]x_1[SEP]x_2[SEP]\n    \\end{aligned}\\]\nPass above into transformer, which gives over vector for each token. Perform linear projection to reduce dimensionality of vector \\[\\begin{aligned}\n    \\\\ Embed_{input}(x) = W_{input}BERT_{CLS}(join_{BERT}(x))\n    \\\\ Embed_{doc}(z) = W_{doc}BERT_{CLS}(join_{BERT}(z_{title}, z_{body}))\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/2020-03-14-realm.html#knowledge-augmented-encoder",
    "href": "posts/2020-03-14-realm.html#knowledge-augmented-encoder",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Knowledge-Augmented Encoder",
    "text": "Knowledge-Augmented Encoder\n\nGiven input \\[x\\] and relevant doc \\[z\\], this defines \\[p(y\\|z,x)\\]\nJoin \\[x\\] and \\[z\\] into single sequence and feed into transformer\nHere, training is different for pre-training vs fine-tuning\n\nFor pre-training, predict [MASK] token. Use same Masked LM(MLM) loss as in Transformer(Devlin et al.)\nFor Open-QA, we need to produce string \\[y\\].\nAssumption: \\[y\\] occurs as sequence of tokens in some document in the corpus."
  },
  {
    "objectID": "posts/2020-03-14-realm.html#training",
    "href": "posts/2020-03-14-realm.html#training",
    "title": "REALM: Retrieval-Augmented Language MOdel Pre-Training",
    "section": "Training",
    "text": "Training\n\nCompute gradients in \\[\\theta\\] and \\[\\phi\\] and optimize using SGD.\nChallenge: Computing \\[p(y\\|x)\\]\nApprox by summing over top \\[k\\] documents with highest prob under \\[p(z\\|x)\\]\nQuestion: How to find top \\[k\\] docs? Answer: Use MIPS\nNeed to precompute \\[Embed_{doc}(x)\\] for all docs. Problem? It changes with each step of SGD.\nSolution: Async refresh \\(Embed_{doc}\\) every 500 steps\nUse MIPS to select top \\(k\\) docs. For these docs, recompute \\(p(z\\|x)\\) using new \\(\\theta\\).\n\n\nImplementing async MIPS refreshes\n\nTwo jobs running in parallel:\n\nPrimary trainer: Perform gradient updates on parameters\nSecondary index builder: Embeds and indexes the docs\n\n\n\nAsync MIPS implementation\n\n\nAsync refresh used only for pre-training\nFor fine tuning, build index once from pre-trained \\(\\theta\\) and use it.\n\n\n\n\nWhat does retriever learn?\n\nRetriever promotes docs that improve accuracy\nThis can be analyzed by analyzing gradient wrt the parameters\n\n\n\nInjecting inductive biases into pre-trianing\n\nSalient span masking: Some questions require only local context. Select named entities and dates and mask one of them. Performs better.\nNull document: Add null document to top \\[k\\] documents to allow answers even when no context is required\nProhibiting trivial retrievals: If knowledge corpus \\[Z\\] is the same as pre-training corpus \\[X\\], it can predict \\[y\\] by looking at \\[x\\] in \\[z\\]. Exclude trivial candidate\nInitialization: Warm up \\[Embed_{input}\\] and \\[Embed_{doc}\\] using Inverse Cloze Task(ICT) i.e model trained to retrieve the doc where the sentence came from."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tech Musings",
    "section": "",
    "text": "GPUs go brrr with Mojo: Algorithms\n\n\n\n\n\nMoar GPU puzzles with slide-n-sum pooling, tile-flipping convs & warp-speed scans\n\n\n\n\n\nJul 20, 2025\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nGPUs go brrr with Mojo - Fundamentals\n\n\n\n\n\nLearning GPU programming fundamentals through hands-on Mojo implementations\n\n\n\n\n\nJul 6, 2025\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nBERT + BM25 = BISON\n\n\n\n\n\nA new framework for information retrieval from documents\n\n\n\n\n\nAug 31, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nLongFormer\n\n\n\n\n\nTransformers for loooong documents\n\n\n\n\n\nMay 11, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nTagLM\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nHow much do you know?\n\n\n\n\n\nMeasure the amount of information stored in a model\n\n\n\n\n\nApr 21, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nAttention is all you need\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nREALM: Retrieval-Augmented Language MOdel Pre-Training\n\n\n\n\n\nA better Q&A system based on knowledge retrieval\n\n\n\n\n\nMar 14, 2020\n\n\nShubham Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Golf Putting Model\n\n\n\n\n\nAre you the next Tiger Woods?\n\n\n\n\n\nMar 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nYo!\n\n\n\n\n\nFirst post\n\n\n\n\n\nJan 14, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "",
    "text": "Picking up right where the the last post left off, this follow-up dives into the bread-and-butter building blocks of deep-learning kernels. We’ll implement and benchmark core algorithms-sliding-window pools, tile-wise convolutions, warp-level scans, and more."
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#raw-memory",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#raw-memory",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Raw Memory",
    "text": "Raw Memory\n\n\nSolution\n\n\n\np10.mojo\n\nfn dot_product(\n    output: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    b: UnsafePointer[Scalar[dtype]],\n    size: Int,\n):\n    shared = stack_allocation[\n        TPB, Scalar[dtype], address_space = AddressSpace.SHARED\n    ]()\n\n    global_idx = block_dim.x * block_idx.x + thread_idx.x\n    local_idx = thread_idx.x\n    if global_idx &lt; size:\n        shared[local_idx] = a[global_idx] * b[global_idx]\n\n    barrier()\n\n    stride = TPB // 2\n    while(stride &gt; 0):\n        if local_idx &lt; stride:\n            shared[local_idx] += shared[local_idx + stride]\n        \n        barrier()\n        stride = stride // 2\n    \n    # only allow thread 0 to write result\n    if local_idx == 0:\n        output[0] = shared[0]\n\n\nNote: Instead of doing the parallel reduction, we could also implement the solution using a loop:\n-    stride = TPB // 2\n-    while(stride &gt; 0):\n-        if local_idx &lt; stride:\n-            shared[local_idx] += shared[local_idx + stride]\n-        \n-        barrier()\n-        stride = stride // 2\n-    \n-    # only allow thread 0 to write result\n-    if local_idx == 0:\n-        output[0] = shared[0]\n+    if global_idx &lt; size:\n+        for idx in range(size):\n+            output[0] = output[0] + shared[idx]\nWhile this approach also gives the correct answer for this puzzle, it has multiple problems:\n\nRace conditions: Multiple threads would simultaneously try to update output[0] without synchronization, causing lost updates.\nThread divergence: When threads in a warp take different execution paths (some running the loop, others not), the GPU must serialize execution, destroying parallelism.\nRedundant computation: Every qualifying thread would compute the exact same sum over the entire array, wasting compute resources.\nMemory bottleneck: Repeated atomic operations to the same memory location (output[0]) create severe contention."
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#layouttensor",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#layouttensor",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "LayoutTensor",
    "text": "LayoutTensor\nalias TPB = 8 alias SIZE = 8 alias BLOCKS_PER_GRID = (1, 1) alias THREADS_PER_BLOCK = (SIZE, 1) alias dtype = DType.float32 alias layout = Layout.row_major(SIZE) alias out_layout = Layout.row_major(1)\n\n\nSolution\n\n\n\np10.mojo\n\nfn dot_product[\n    in_layout: Layout, out_layout: Layout\n](\n    output: LayoutTensor[mut=True, dtype, out_layout],\n    a: LayoutTensor[mut=True, dtype, in_layout],\n    b: LayoutTensor[mut=True, dtype, in_layout],\n    size: Int,\n):\n    # Use LayoutTensorBuilder instead of stack_allocation\n    shared = tb[dtype]().row_major[TPB]().shared().alloc()\n    global_idx = block_dim.x * block_idx.x + thread_idx.x\n    local_idx = thread_idx.x\n\n    if global_idx &lt; size:\n        shared[local_idx] = a[global_idx] * b[global_idx]\n\n    barrier()\n\n    stride = TPB // 2\n    while(stride &gt; 0):\n        if local_idx &lt; stride:\n            shared[local_idx] += shared[local_idx + stride]\n        \n        barrier()\n        stride = stride // 2\n    \n    # only allow thread 0 to write result\n    if local_idx == 0:\n        output[0] = shared[0]"
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#single-block-with-shared-memory",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#single-block-with-shared-memory",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Single Block with Shared Memory",
    "text": "Single Block with Shared Memory\nFor this version, we assume that we only have a single block, and both the input data and the kernel fit within a block.\n\nThe implementation is:\n\nIntialise shared memory for both the input and the kernel\nLoad data in the shared memory, and use barrier() to sync all threads before performing computations.\nIn a loop, multiple the value of input and kernel, and add to a local variable.\nAssign the local variable to the right output index.\n\n\n\nSolution\n\n\n\np11.mojo\n\nalias TPB = 8\nalias SIZE = 6\nalias CONV = 3\nalias BLOCKS_PER_GRID = (1, 1)\nalias THREADS_PER_BLOCK = (TPB, 1)\nalias dtype = DType.float32\nalias in_layout = Layout.row_major(SIZE)\nalias out_layout = Layout.row_major(SIZE)\nalias conv_layout = Layout.row_major(CONV)\n\n\nfn conv_1d_simple[\n    in_layout: Layout, out_layout: Layout, conv_layout: Layout\n](\n    output: LayoutTensor[mut=False, dtype, out_layout],\n    a: LayoutTensor[mut=False, dtype, in_layout],\n    b: LayoutTensor[mut=False, dtype, conv_layout],\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i = thread_idx.x\n    # This is oversized! I've explained it later :)\n    shared_a = tb[dtype]().row_major[TPB]().shared().alloc()\n    shared_b = tb[dtype]().row_major[TPB]().shared().alloc()\n\n    # This can also be optimised, as shown later.\n    if global_i &lt; SIZE:\n        shared_a[local_i] = a[global_i]\n        shared_b[local_i] = b[global_i]\n    \n\n    barrier()\n\n    if global_i &lt; SIZE:\n\n        # Ensure the local var has the same type as the output\n        # to avoid type casting errors.\n        var local_sum: output.element_type = 0\n\n        # Perform loop unrolling.\n        @parameter\n        for j in range(CONV):\n            if local_i + j &lt; SIZE:\n                local_sum += shared_a[local_i + j] * shared_b[j]\n            barrier()\n        \n        output[global_i] = local_sum\n\n\nI deliberately allocate shared_a and shared_b with the block width (TPB) instead of the input length (SIZE) and filter length (CONV). The extra space isn’t needed for correctness-the kernel only touches the first SIZE/CONV elements-but it nicely demonstrates LayoutTensor’s masking: out-of-range indices are silently ignored. This trick keeps the buffer shape uniform across puzzles without cluttering the code with edge-case branches. The flip side is a bit of wasted shared memory, which can pinch if your kernel is already pushing the SRAM limit.\nThe optimal allocation of shared memory would be:\n-    shared_a = tb[dtype]().row_major[TPB]().shared().alloc()\n-    shared_b = tb[dtype]().row_major[TPB]().shared().alloc()\n+    # Allocate exactly SIZE elements → smaller shared-mem footprint\n+    shared_a = tb[dtype]().row_major[SIZE]().shared().alloc()\n+    # Allocate exactly CONV elements → smaller shared-mem footprint\n+    shared_b = tb[dtype]().row_major[CONV]().shared().alloc()\n...\n\n-    if global_i &lt; SIZE:\n-        shared_a[local_i] = a[global_i]\n-        shared_b[local_i] = b[global_i]\n+    if global_i &lt; SIZE:\n+        shared_a[local_i] = a[global_i]\n+    if global_i &lt; CONV:\n+        shared_b[local_i] = b[global_i]\n\nLoop Unrolling\n@parameter is Mojo’s implementation of loop unrolling. This has the same functionality as pragma unroll(N) in CUDA.\nWhen unroll is in effect, the optimizer determines and applies the best unrolling factor for each loop; in some cases, the loop control might be modified to avoid unnecessary branching. The compiler remains the final arbiter of whether the loop is unrolled[4].\n@parameter isn’t limited to loops/branches-you can slap it on an inner function and Mojo will build a parametric closure, defined as[5]:\n\nA parametric closure is a nested function decorated with @parameter. Any values it captures from the surrounding scope are treated as compile-time constants. The compiler materialises one specialised version of the closure for every distinct set of captured values\n\nExample:\n\n\nparametric_closure.mojo\n\nfn make_shift(off: Int):\n    @parameter            # ← specialised per ‘off’\n    fn shift(x: Int) -&gt; Int:\n        return x + off\n    return shift\n\nlet s1 = make_shift(1)    # emits shift-$off=1\nlet s4 = make_shift(4)    # emits shift-$off=4\n\nNo runtime captures, no heap boxing-the constant off is literally spliced into the generated IR, so calls to s1/s4 inline like normal code and can be further unrolled or constant-folded.\nWhy is this safe? Mojo’s origin system[6] assigns each compile-time constant its own immutable origin. The closure therefore can’t outlive or mutate the thing it captured; once the surrounding scope ends those origins die too, guaranteeing that the specialised code never touches expired storage.\nIn summary, you get closure ergonomics plus “zero-cost abstraction”[7] performance-ideal for GPU kernels where every cycle and register matters."
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#block-boundary",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#block-boundary",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Block Boundary",
    "text": "Block Boundary\nWe now aim to perform convolution over an input that is larger than a single block. Due to the nature of convolution operation, this introduces interesting boundary conditions. Specifically, the output of block N now depends on block N - 1, when N &gt; 1.\nThe blue cells are the data owned by the current thread-block. The orange cells are the first few elements of the next block that the convolution window will inevitably peek at.\n\nProblem statement\nRun a 1-D convolution with a CONV₂-tap kernel over an input that is longer than one block (TPB threads). We want every thread to:\n• pull data from shared memory only (once it’s loaded, stay in-block)\n• avoid divergent branches and random global reads\n• keep the load pattern fully coalesced\nNaïve global loads meet none of those goals-once a window crosses the block edge the tail threads must issue conditional, straggling reads (i.e. each thread grabs a lone, scattered element from global memory instead of part of one tidy, coalesced burst).\nThe halo idea\nGive each block an in-block “fence extension”:\nshared_a = …[TPB + (CONV₂ − 1)]   # main slice + halo\nThe extra (CONV₂ − 1) slots-the halo-mirror the first (CONV₂ − 1) elements of the next block (or zeros if we’re already at EOF). That single change guarantees that every sliding window lives in one contiguous span of shared memory.\nThe elements that are involved in multiple tiles and loaded by multiple blocks are commonly referred to as halo cells or skirt cells since they “hang” from the side of the part that is used solely by a single block[8].\nLoading recipe (matches the numbered arrows in the figure):\n\nBulk copy – all TPB threads dump their element:\nshared_a[t] = a[blockStart + t]\nHalo fill – threads t &lt; (CONV₂ − 1) copy the tail:\nshared_a[TPB + t] = (a[blockStart + TPB + t] if in-range else 0)\nKernel stash – threads t &lt; CONV₂ cache the weights:\nshared_b[t] = b[t]\nbarrier() – everyone syncs\n\nAfter step 4 every thread sees:\n      main slice              halo\n[ … local_i … TPB − 1 | TPB … TPB+CONV₂−2 ]\nCode to perform the actual computation is the same as in Puzzle 10.\nOne barrier, no branches and 100 % shared-memory hits ensure our kernel is fast and efficient!\n\n\nSolution\n\n\n\np11_block_boundary.mojo\n\nalias SIZE_2 = 15\nalias CONV_2 = 4\nalias BLOCKS_PER_GRID_2 = (2, 1)\nalias THREADS_PER_BLOCK_2 = (TPB, 1)\nalias in_2_layout = Layout.row_major(SIZE_2)\nalias out_2_layout = Layout.row_major(SIZE_2)\nalias conv_2_layout = Layout.row_major(CONV_2)\n\nfn conv_1d_block_boundary[\n    in_layout: Layout, out_layout: Layout, conv_layout: Layout, dtype: DType\n](\n    output: LayoutTensor[mut=False, dtype, out_layout],\n    a: LayoutTensor[mut=False, dtype, in_layout],\n    b: LayoutTensor[mut=False, dtype, conv_layout],\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i  = thread_idx.x\n\n    # input slice + halo\n    shared_a = tb[dtype]().row_major[TPB + CONV_2 - 1]().shared().alloc()\n\n    # load kernel\n    shared_b = tb[dtype]().row_major[CONV_2]().shared().alloc()\n\n    if global_i &lt; SIZE_2:\n        # coalesced load of main slice\n        shared_a[local_i] = a[global_i]                  \n\n    # only first CONV_2 threads participate\n    if local_i &lt; CONV_2:\n        # load kernel into shared memory\n        shared_b[local_i] = b[local_i]                   \n\n    # threads responsible for halo load\n    if local_i &lt; CONV_2 - 1:\n        # element that lives in next block\n        var next_idx = global_i + TPB                    \n        # pad with zeros\n        shared_a[local_i + TPB] = a[next_idx] if next_idx &lt; SIZE_2 else 0.0\n\n    barrier()\n\n    # skip threads mapping past the end\n    if global_i &lt; SIZE_2:\n        var local_sum: output.element_type = 0.0\n\n        @parameter                                       \n        for j in range(CONV_2):                          \n            # dot product of window & kernel\n            local_sum += shared_a[local_i + j] * shared_b[j]\n        output[global_i] = local_sum\n\npixi run p11 --block-boundary\n# out: HostBuffer([14.0, 20.0, 26.0, 32.0, 38.0, 44.0, 50.0, 56.0, 62.0, 68.0, 74.0, 80.0, 41.0, 14.0, 0.0])\n# expected: HostBuffer([14.0, 20.0, 26.0, 32.0, 38.0, 44.0, 50.0, 56.0, 62.0, 68.0, 74.0, 80.0, 41.0, 14.0, 0.0])"
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#hillissteele-algorithm",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#hillissteele-algorithm",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Hillis–Steele Algorithm",
    "text": "Hillis–Steele Algorithm\nA straightforward parallel scan is the Hillis–Steele approach: at each distance d = 1, 2, 4, … every element adds in the value from d positions back. This is the same as the method shown in Puzzle 10\n# inclusive scan, power-of-two length\ndef hillis_steele_scan(a, ⊕):\n    n = len(a)\n    temp = a.copy()\n    d = 1\n    while d &lt; n:\n        for i in range(n):\n            temp[i] = a[i] if i &lt; d else a[i - d] ⊕ a[i]\n        a, temp = temp, a\n        d *= 2\n    return a\nIn Mojo, this looks as follows:\n\n\nSolution\n\n\n\np12_simple.mojo\n\nfn prefix_sum_simple[\n    layout: Layout\n](\n    output: LayoutTensor[mut=False, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    size: Int,\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i = thread_idx.x\n    for idx in range(Int(log2(Scalar[dtype](TPB)))):\n        if local_i &gt;= offset and local_i &lt; SIZE:\n            shared[local_i] += shared[local_i - offset]\n\n        barrier()\n        offset *= 2\n\n    if global_i &lt; SIZE:\n        output[global_i] = shared[local_i]\n\npixi run p12 --simple\n# out: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\n# expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\nEach of the log₂(n) rounds does up to n parallel additions (one per active element), so total work is \\(\\sum_k n = nlog(n)\\). Because rounds are serialized by barriers, the longest dependency chain is one add per round i.e \\(O(log n)\\)."
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#blellochs-twopass-algorithm",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#blellochs-twopass-algorithm",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Blelloch’s Two‐Pass Algorithm",
    "text": "Blelloch’s Two‐Pass Algorithm\nBlelloch’s two-pass scan does Θ(n) work by splitting the job into an up-sweep (build a reduction tree) and a down-sweep (propagate prefixes) [10].\nWhy prefer it over the classic Hillis–Steele (Algorithm 1)?\n\nHardware reality.\nHillis–Steele assumes one processor per element and updates the array in-place every round.\nA real GPU doesn’t grant that luxury: a “1024-thread” block actually runs in 32-thread warps that time-slice on the same SM. When warp 0 pauses and warp 1 resumes, in-place writes from one warp can overwrite data the other still needs.\nSynchronisation cost.\nAvoiding the overwrite requires a barrier after every addition - log₂(n) rounds × n threads ⇒ Θ(n log n) operations plus all those barriers.\nBlelloch’s fix.\n• Up-sweep and down-sweep touch disjoint tree levels, so threads never trample each other within a phase.\n• Only two global barriers are needed (one between the phases, one at the end).\n• Now you get Θ(n) work and correctness, even for arrays much bigger than a warp.\n\nThe result is a scan that is both faster and safer on modern GPUs.\n\nUp-sweep (reduce)\n\nBuild a binary reduction tree over log₂(n) rounds:\n\nRound 1 (step=1): sum each adjacent pair, storing results at indices 1, 3, 5, …\nRound 2 (step=2): merge those partial sums into blocks of 4, writing into indices 3, 7, 11, …\nContinue doubling the span each round until step = n/2\n\nAfter the final round, a[n-1] holds the overall total\n\n Up-Sweep: combining elements in a binary-tree fashion-build partial sums until the final element holds the total.\nDown-sweep (propagate)\nAfter the up-sweep leaves a[n-1] containing the overall sum, we walk the tree top-down to scatter prefix sums into every slot:\n\nInitialize the down-sweep with a window size of step = n/2.\n\nLoop as long as step &gt;= 1:\n\nPartition the array into blocks of size 2*step. For each block starting at index i:\n• Temporarily store the left-child total from a[i + step - 1].\n• Overwrite that left slot with the right-child subtotal from a[i + 2*step - 1].\n• Add the saved left-child total to the right slot, giving the correct prefix for that subtree.\n\nIssue a barrier() so all threads sync before shrinking the window.\n\nHalve the window: step = step / 2.\n\n\nWith each pass, the partial sums trickle down one level of the binary tree; after log₂(n) iterations every element holds its exclusive prefix sum.\n\n\nDown Sweep: siblings swap and accumulate, driving the scan from root back to leaves.\n\nTime Complexity: Θ(log₂ n) parallel steps, Work: Θ(n) total operations.\n\n\nSolution (Blelloch up-sweep + down-sweep)\n\n\n\np12_blelloch.mojo\n\nfn prefix_sum_blelloch[\n    layout: Layout\n](\n    output:   LayoutTensor[mut=True, dtype, layout],\n    a:     LayoutTensor[mut=False, dtype, layout],\n    size:  Int,\n):\n    global_idx = block_idx.x*block_dim.x + thread_idx.x\n    local_idx = thread_idx.x\n    shared = tb[dtype]().row_major[SIZE]().shared().alloc()\n\n    if global_idx &lt; size:\n        shared[local_idx] = a[global_idx]\n    barrier()\n\n    # Up-sweep\n    var stride = 1\n    while stride &lt; size:\n        step = stride * 2\n        if (local_idx % step == step - 1) and (local_idx &lt; size):\n            shared[local_idx] += shared[local_idx - stride]\n        barrier()\n        stride = step\n\n    # Down-sweep\n    if local_idx == size - 1:\n        shared[local_idx] = 0\n    barrier()\n\n    var half = stride &gt;&gt; 1\n    while half &gt; 0:\n        step = half * 2\n        if (local_idx % step == step - 1) and (local_idx &lt; size):\n            t = shared[local_idx - half]\n            shared[local_idx - half] = shared[local_idx]\n            shared[local_idx] += t\n        barrier()\n        half = half &gt;&gt; 1\n\n    if global_idx &lt; size:\n        output[global_idx] = shared[local_idx] + a[global_idx]\n\npixi run p12 --blelloch\n# out: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\n# expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\n\nThis is not the most efficient implementation, but I hope this provides some intuition about the algorithm!"
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#block-boundary-1",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#block-boundary-1",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Block Boundary",
    "text": "Block Boundary\nThe key difference in this version is that now we have an input array that is larger than the size of a single block.\nWe split the global scan into two bite-sized passes:\nPhase 1 – Local Scan\n\nEach block copies its slice into shared memory.\n\nPerform an in-block naive scan/Blelloch scan exactly as in the single-block case.\n\nThe last thread of the block stashes the block’s total after the scan into an auxiliary slot at the tail of output:\n#  |&lt;---  SIZE_2  ---&gt;|&lt;-- #blocks --&gt;|\n#  [   prefix sums   ][ block totals ]\n\nPhase 2 – Propagate block totals\n\nEvery thread grabs the aggregate from the previous block (totals[block_id-1]) and adds it to its own prefix.\nNow every element holds the inclusive scan over the whole array.\n\n\nWe launch the above phases as two separate kernels.\nA host-side synchronisation sits between the launches. That call flushes the work queue and waits until Phase 1 has fully committed its writes to global memory, ensuring the per-block totals are complete and visible before Phase 2 starts consuming them. Skip the sync and the driver is free to overlap or reorder the kernels, letting Phase 2 read garbage.\n\n\nSolution (Block Boundary Version)\n\n\n\np12_block_boundary.mojo\n\nfn prefix_sum_local_phase[\n    out_layout: Layout, in_layout: Layout\n](\n    output: LayoutTensor[mut=False, dtype, out_layout],\n    a: LayoutTensor[mut=False, dtype, in_layout],\n    size: Int,\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i = thread_idx.x\n    shared = tb[dtype]().row_major[EXTENDED_SIZE]().shared().alloc()\n\n    if global_i &lt; SIZE_2:\n        shared[local_i] = a[global_i]\n    \n    barrier()\n    offset = 1\n\n    for idx in range(Int(log2(Scalar[dtype](TPB)))):\n        if local_i &gt;= offset and local_i &lt; SIZE_2:\n            shared[local_i] += shared[local_i - offset]\n\n        barrier()\n        offset *= 2\n\n    if global_i &lt; SIZE_2:\n        output[global_i] = shared[local_i]\n    \n    if local_i == TPB - 1:\n        output[size + block_idx.x] += shared[local_i]\n\n\n# Kernel 2: Add block sums to their respective blocks\nfn prefix_sum_block_sum_phase[\n    layout: Layout\n](output: LayoutTensor[mut=False, dtype, layout], size: Int):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    # FILL ME IN (roughly 3 lines)\n    if block_idx.x &gt; 0 and global_i &lt; size:\n        prev_block_sum = output[SIZE_2 + block_idx.x - 1]\n        output[global_i] += prev_block_sum\n\npixi run p12\n# out: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0, 36.0, 45.0, 55.0, 66.0, 78.0, 91.0, 105.0, 28.0, 77.0]) # last 2 elements are the block sums\n# expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0, 36.0, 45.0, 55.0, 66.0, 78.0, 91.0, 105.0])"
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#global-memory-version",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#global-memory-version",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Global Memory Version",
    "text": "Global Memory Version\nBased on the 2D indexing section, each thread computes one C[row, col] by loading A[row, k] and B[k, col] from global memory, multiplying and accumulating over k. We unroll the k‐loop to cut loop overhead and boost throughput.\n\n\nSolution\n\n\n\np14_naive.mojo\n\nfn naive_matmul[\n    layout: Layout, size: Int\n](\n    output: LayoutTensor[mut=False, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    b: LayoutTensor[mut=False, dtype, layout],\n):\n    row = block_dim.y * block_idx.y + thread_idx.y\n    col = block_dim.x * block_idx.x + thread_idx.x\n\n    if row &lt; SIZE and col &lt; SIZE:\n        # Need this to ensure the mojo compiler knows\n        # the type of `running_sum`, otherwise it will\n        # complain\n        var running_sum: output.element_type = 0\n\n        @parameter\n        for k in range(SIZE):\n            running_sum += a[row, k] * b[k, col]\n        output[row, col] = running_sum\n\npixi run p14 --naive\n# out: HostBuffer([4.0, 6.0, 12.0, 22.0])\n# expected: HostBuffer([4.0, 6.0, 12.0, 22.0])"
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#shared-memory-version",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#shared-memory-version",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Shared Memory Version",
    "text": "Shared Memory Version\nThe previous version suffers from repeated global memory reads. We can optimize this using shared memory:\n\nLoad matrix tiles once\nSynchronize threads\nCompute using the cached data.\n\n\n\n\nMatmul with shared memory\n\n\n\n\nSolution\n\n\n\np14_shared.mojo\n\nfn single_block_matmul[\n    layout: Layout, size: Int\n](\n    output: LayoutTensor[mut=False, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    b: LayoutTensor[mut=False, dtype, layout],\n):\n    row = block_dim.y * block_idx.y + thread_idx.y\n    col = block_dim.x * block_idx.x + thread_idx.x\n    local_row = thread_idx.y\n    local_col = thread_idx.x\n    shared_a = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n    shared_b = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n    if row &lt; size and col &lt; size and local_row &lt; size and local_col &lt; size:\n        shared_a[local_row, local_col] = a[row, col]\n        shared_b[local_row, local_col] = b[row, col]\n\n    barrier()\n    if row &lt; size and col &lt; size and local_row &lt; size and local_col &lt; size:\n        var running_sum: output.element_type = 0.0\n\n        @parameter\n        for k in range(size):\n            running_sum += a[local_row, k] * b[k, local_col]\n\n        output[row, col] = running_sum\n\npixi run p14 --naive\n# out: HostBuffer([4.0, 6.0, 12.0, 22.0])\n# expected: HostBuffer([4.0, 6.0, 12.0, 22.0])\nThe Roofline Model offers a first-order answer to a GPU performance question: is my kernel limited by arithmetic throughput or by memory bandwidth?\nIt does so by plotting operational intensity (FLOPs per byte) against two ceilings—the hardware’s peak FLOP/s and peak DRAM bandwidth—so you can see at a glance which resource is the bottleneck."
  },
  {
    "objectID": "posts/2025-07-03-gpu-puzzles-p2.html#roofline-model",
    "href": "posts/2025-07-03-gpu-puzzles-p2.html#roofline-model",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Roofline Model",
    "text": "Roofline Model\n\nNote: The Modular GPU Puzzles guide already walks through the full roofline derivation, but we’ll repeat it here so that you can follow along without leaving this post.\n\nThe first step is abstracting the hardware-software complexity into a tractable model.\n\nHardware Model\nClassic roofline assumes ideal hardware with perfect overlap:\n\n\n\nSource: NHR at FAU[12]\n\n\nThe cartoon GPU has only two levers:\n\nCompute engine — peak rate \\(P_{peak}\\) (FLOP/s, integer ops/s, etc.)\nMemory datapath — peak bandwidth \\(b_s\\) (bytes/s)\n\n\n\nSoftware Model\n\n\n\nSoftware abstraction: complex GPU kernel simplified to steady-state loop with N flops and V bytes per iteration. Credits: NHR at FAU[12]\n\n\nWe collapse the kernel’s steady-state loop to:\n\n\\(N\\) floating-point operations per iteration\n\\(V\\) bytes moved per iteration\n\nThe operational intensity is defined as:\n\\[I = \\frac{N}{V} \\text{ flop/byte}\\]\nThis ratio is all that survives of the algorithm - prologue/epilogue work, control flow, and synchronizations are swept aside.\nHardware Assumptions:\n\n\n\n\n\n\n\n\n\n\n#\nAssumption\nWorks because…\nReality\nBreaks when…\n\n\n\n\nH1\nPeak DRAM bandwidth reachable\nIdeal streaming\nRequires 100% streaming, &gt;1MB tiles\nStrided or tiny tiles\n\n\nH2\nPeak FLOP/s reachable\nFull FMA rate\nAll ALUs busy every cycle\nDivergence, low occupancy\n\n\nH3\nOne bandwidth number is enough\nDRAM dominates\nL1/L2/SMEM add separate roofs\nLower-level choke points\n\n\n\nSoftware Assumptions:\n\n\n\n\n\n\n\n\n\n\n#\nAssumption\nWorks because…\nReality\nBreaks when…\n\n\n\n\nS1\nLoads fully hide latency\n1000s inflight warps\nRequires deep pipelining\nShort kernels, frequent syncs\n\n\nS2\nSingle operational intensity\nSteady-state loop\nReal kernels mix phases\nGather/scatter, epilogue code\n\n\nS3\nLaunch/transfer overhead small\nLong kernel runs\nAmortised over many iterations\nMicro-benchmarks, chaining\n\n\n\n\n\nThe Roofline Equation\nWith these assumptions, hardware and software collapse to one parameter—the operational intensity \\(I\\)—and attainable performance becomes\n\\[\n\\begin{aligned}\nP(I) &= \\min\\!\\bigl(P_{\\text{peak}},\\, I\\,b_s\\bigr) \\\\\nI_{\\text{crit}} &= \\frac{P_{\\text{peak}}}{b_s}\n\\end{aligned}\n\\]\nAt the critical intensity \\(I_{crit}\\), the bandwidth and compute roofs intersect, splitting kernels into two classes:\n\nMemory-bound (\\(I &lt; I_{crit}\\)) → Performance rises linearly with \\(I\\)\nCompute-bound (\\(I \\geq I_{crit}\\)) → Performance plateaus at \\(P_{peak}\\)\n\n\n\n\nRoofline model: sloped red line shows memory bandwidth limit, flat blue line is compute peak, kernel’s operational intensity marked as a dot.\n\n\n\n\nWhere the Roofline Model Fails\nEven in small puzzle kernels, these assumptions falter. In real workloads, they break down completely.\nWhat actually works:\n\nMeasure real limits with tools like Nsight or rocprof\nRedraw the roofline using measured ceilings—L2 roof, Tensor-core roof, not just DRAM and peak FLOPs\nAdjust your kernel: boost \\(I\\) (tiling, shared memory, tensor ops) or raise the ceilings (improve occupancy, reduce stalls)\n\n\nUnfortunately no Nsight eye-candy as of yet - my ncu setup hit a permissions wall (https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-ters). I’ll fix it and share a profiler deep-dive soon. Stay tuned!\n\nThe textbook roofline is a guide, not reality. Measure, adapt, and push your kernel as close to the real limits as you can.\n\n\n\nPooling\nDot Product\nZeno Paradox\n2D Block Layout\n2D Thread Layout\nMatrix Indexing\nSource: Toast Lab\nPrefix‐Sum Illustration\nAxis Sum\nRow Sum\nMatmul with shared memory\nSource: NHR at FAU[12]\nSoftware abstraction: complex GPU kernel simplified to steady-state loop with N flops and V bytes per iteration. Credits: NHR at FAU[12]\nRoofline model: sloped red line shows memory bandwidth limit, flat blue line is compute peak, kernel’s operational intensity marked as a dot."
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "",
    "text": "Picking up right where the the last post left off, this follow-up dives into the bread-and-butter building blocks of deep-learning kernels. We’ll implement and benchmark core algorithms-sliding-window pools, tile-wise convolutions, warp-level scans, and more."
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#raw-memory",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#raw-memory",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Raw Memory",
    "text": "Raw Memory\n\n\nSolution\n\n\n\np10.mojo\n\nfn dot_product(\n    output: UnsafePointer[Scalar[dtype]],\n    a: UnsafePointer[Scalar[dtype]],\n    b: UnsafePointer[Scalar[dtype]],\n    size: Int,\n):\n    global_idx = block_dim.x * block_idx.x + thread_idx.x\n    local_idx = thread_idx.x\n    if global_idx &lt; size:\n        shared[local_idx] = a[global_idx] * b[global_idx]\n\n    barrier()\n\n    stride = TPB // 2\n    while(stride &gt; 0):\n        if local_idx &lt; stride:\n            shared[local_idx] += shared[local_idx + stride]\n        \n        barrier()\n        stride = stride // 2\n    \n    # only allow thread 0 to write result\n    if local_idx == 0:\n        output[0] = shared[0]\n\n\nNote: Instead of doing the parallel reduction, we could also implement the solution using a loop:\n-    stride = TPB // 2\n-    while(stride &gt; 0):\n-        if local_idx &lt; stride:\n-            shared[local_idx] += shared[local_idx + stride]\n-        \n-        barrier()\n-        stride = stride // 2\n-    \n-    # only allow thread 0 to write result\n-    if local_idx == 0:\n-        output[0] = shared[0]\n+    if global_idx &lt; size:\n+        for idx in range(size):\n+            output[0] = output[0] + shared[idx]\nWhile this approach also gives the correct answer for this puzzle, it has multiple problems:\n\nRace conditions: Multiple threads would simultaneously try to update output[0] without synchronization, causing lost updates.\nThread divergence: When threads in a warp take different execution paths (some running the loop, others not), the GPU must serialize execution, destroying parallelism.\nRedundant computation: Every qualifying thread would compute the exact same sum over the entire array, wasting compute resources.\nMemory bottleneck: Repeated atomic operations to the same memory location (output[0]) create severe contention."
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#layouttensor",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#layouttensor",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "LayoutTensor",
    "text": "LayoutTensor\n\n\nSolution\n\nalias TPB = 8 alias SIZE = 8 alias BLOCKS_PER_GRID = (1, 1) alias THREADS_PER_BLOCK = (SIZE, 1) alias dtype = DType.float32 alias layout = Layout.row_major(SIZE) alias out_layout = Layout.row_major(1)\n\n\np10.mojo\n\nfn dot_product[\n    in_layout: Layout, out_layout: Layout\n](\n    output: LayoutTensor[mut=True, dtype, out_layout],\n    a: LayoutTensor[mut=True, dtype, in_layout],\n    b: LayoutTensor[mut=True, dtype, in_layout],\n    size: Int,\n):\n    # Use LayoutTensorBuilder instead of stack_allocation\n    shared = tb[dtype]().row_major[TPB]().shared().alloc()\n    global_idx = block_dim.x * block_idx.x + thread_idx.x\n    local_idx = thread_idx.x\n\n    if global_idx &lt; size:\n        shared[local_idx] = a[global_idx] * b[global_idx]\n\n    barrier()\n\n    stride = TPB // 2\n    while(stride &gt; 0):\n        if local_idx &lt; stride:\n            shared[local_idx] += shared[local_idx + stride]\n        \n        barrier()\n        stride = stride // 2\n    \n    # only allow thread 0 to write result\n    if local_idx == 0:\n        output[0] = shared[0]"
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#single-block-with-shared-memory",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#single-block-with-shared-memory",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Single Block with Shared Memory",
    "text": "Single Block with Shared Memory\nFor this version, we assume that we only have a single block, and both the input data and the kernel fit within a block.\n\nThe implementation is:\n\nIntialise shared memory for both the input and the kernel\nLoad data in the shared memory, and use barrier() to sync all threads before performing computations.\nIn a loop, multiple the value of input and kernel, and add to a local variable.\nAssign the local variable to the right output index.\n\n\n\nSolution\n\n\n\np11.mojo\n\nalias TPB = 8\nalias SIZE = 6\nalias CONV = 3\nalias BLOCKS_PER_GRID = (1, 1)\nalias THREADS_PER_BLOCK = (TPB, 1)\nalias dtype = DType.float32\nalias in_layout = Layout.row_major(SIZE)\nalias out_layout = Layout.row_major(SIZE)\nalias conv_layout = Layout.row_major(CONV)\n\n\nfn conv_1d_simple[\n    in_layout: Layout, out_layout: Layout, conv_layout: Layout\n](\n    output: LayoutTensor[mut=False, dtype, out_layout],\n    a: LayoutTensor[mut=False, dtype, in_layout],\n    b: LayoutTensor[mut=False, dtype, conv_layout],\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i = thread_idx.x\n    # This is oversized! I've explained it later :)\n    shared_a = tb[dtype]().row_major[TPB]().shared().alloc()\n    shared_b = tb[dtype]().row_major[TPB]().shared().alloc()\n\n    # This can also be optimised, as shown later.\n    if global_i &lt; SIZE:\n        shared_a[local_i] = a[global_i]\n        shared_b[local_i] = b[global_i]\n    \n\n    barrier()\n\n    if global_i &lt; SIZE:\n\n        # Ensure the local var has the same type as the output\n        # to avoid type casting errors.\n        var local_sum: output.element_type = 0\n\n        # Perform loop unrolling.\n        @parameter\n        for j in range(CONV):\n            if local_i + j &lt; SIZE:\n                local_sum += shared_a[local_i + j] * shared_b[j]\n        \n        output[global_i] = local_sum\n\n\nI deliberately allocate shared_a and shared_b with the block width (TPB) instead of the input length (SIZE) and filter length (CONV). The extra space isn’t needed for correctness-the kernel only touches the first SIZE/CONV elements-but it nicely demonstrates LayoutTensor’s masking: out-of-range indices are silently ignored. This trick keeps the buffer shape uniform across puzzles without cluttering the code with edge-case branches. The flip side is a bit of wasted shared memory, which can pinch if your kernel is already pushing the SRAM limit.\nThe optimal allocation of shared memory would be:\n-    shared_a = tb[dtype]().row_major[TPB]().shared().alloc()\n-    shared_b = tb[dtype]().row_major[TPB]().shared().alloc()\n+    # Allocate exactly SIZE elements -&gt; smaller shared-mem footprint\n+    shared_a = tb[dtype]().row_major[SIZE]().shared().alloc()\n+    # Allocate exactly CONV elements -&gt; smaller shared-mem footprint\n+    shared_b = tb[dtype]().row_major[CONV]().shared().alloc()\n...\n\n-    if global_i &lt; SIZE:\n-        shared_a[local_i] = a[global_i]\n-        shared_b[local_i] = b[global_i]\n+    if global_i &lt; SIZE:\n+        shared_a[local_i] = a[global_i]\n+    if global_i &lt; CONV:\n+        shared_b[local_i] = b[global_i]\n\nLoop Unrolling\n@parameter is Mojo’s implementation of loop unrolling. This has the same functionality as pragma unroll(N) in CUDA.\nWhen unroll is in effect, the optimizer determines and applies the best unrolling factor for each loop; in some cases, the loop control might be modified to avoid unnecessary branching. The compiler remains the final arbiter of whether the loop is unrolled[4].\n@parameter isn’t limited to loops/branches-you can slap it on an inner function and Mojo will build a parametric closure, defined as[5]:\n\nA parametric closure is a nested function decorated with @parameter. Any values it captures from the surrounding scope are treated as compile-time constants. The compiler materialises one specialised version of the closure for every distinct set of captured values\n\nExample:\n\n\nparametric_closure.mojo\n\nfn make_shift(off: Int):\n    @parameter            # ← specialised per ‘off'\n    fn shift(x: Int) -&gt; Int:\n        return x + off\n    return shift\n\nlet s1 = make_shift(1)    # emits shift-$off=1\nlet s4 = make_shift(4)    # emits shift-$off=4\n\nNo runtime captures, no heap boxing-the constant off is literally spliced into the generated IR, so calls to s1/s4 inline like normal code and can be further unrolled or constant-folded.\nWhy is this safe? Mojo’s origin system[6] assigns each compile-time constant its own immutable origin. The closure therefore can’t outlive or mutate the thing it captured; once the surrounding scope ends those origins die too, guaranteeing that the specialised code never touches expired storage.\nIn summary, you get closure ergonomics plus “zero-cost abstraction”[7] performance-ideal for GPU kernels where every cycle and register matters."
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#block-boundary",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#block-boundary",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Block Boundary",
    "text": "Block Boundary\nWe now aim to perform convolution over an input that is larger than a single block. Due to the nature of convolution operation, this introduces interesting boundary conditions. Specifically, the output of block N now depends on block N - 1, when N &gt; 1.\nThe blue cells are the data owned by the current thread-block. The orange cells are the first few elements of the next block that the convolution window will inevitably peek at.\n\n\nProblem statement\nRun a 1-D convolution with a CONV₂-tap kernel over an input that is longer than one block (TPB threads). We want every thread to:\n\nPull data from shared memory only (once it’s loaded, stay in-block)\n\nAvoid divergent branches and random global reads\n\nKeep the load pattern fully coalesced\n\nNaïve global loads meet none of those goals-once a window crosses the block edge the tail threads must issue conditional, straggling reads (i.e. each thread grabs a lone, scattered element from global memory instead of part of one tidy, coalesced burst).\n\n\nThe halo idea\nGive each block an in-block “fence extension”:\nshared_a = …[TPB + (CONV₂ − 1)]   # main slice + halo\nThe extra (CONV₂ − 1) slots-the halo-mirror the first (CONV₂ − 1) elements of the next block (or zeros if we’re already at EOF). That single change guarantees that every sliding window lives in one contiguous span of shared memory.\nThe elements that are involved in multiple tiles and loaded by multiple blocks are commonly referred to as halo cells or skirt cells since they “hang” from the side of the part that is used solely by a single block[8].\nLoading recipe (matches the numbered arrows in the figure):\n\nBulk copy - all TPB threads dump their element:\nshared_a[t] = a[blockStart + t]\nHalo fill - threads t &lt; (CONV₂ − 1) copy the tail:\nshared_a[TPB + t] = (a[blockStart + TPB + t] if in-range else 0)\nKernel stash - threads t &lt; CONV₂ cache the weights:\nshared_b[t] = b[t]\nbarrier() - everyone syncs\n\nAfter step 4 every thread sees:\n      main slice              halo\n[ … local_i … TPB − 1 | TPB … TPB+CONV₂−2 ]\nCode to perform the actual computation is the same as in Puzzle 10.\nOne barrier, no branches and 100 % shared-memory hits ensure our kernel is fast and efficient!\n\n\nSolution\n\n\n\np11_block_boundary.mojo\n\nalias SIZE_2 = 15\nalias CONV_2 = 4\nalias BLOCKS_PER_GRID_2 = (2, 1)\nalias THREADS_PER_BLOCK_2 = (TPB, 1)\nalias in_2_layout = Layout.row_major(SIZE_2)\nalias out_2_layout = Layout.row_major(SIZE_2)\nalias conv_2_layout = Layout.row_major(CONV_2)\n\nfn conv_1d_block_boundary[\n    in_layout: Layout, out_layout: Layout, conv_layout: Layout, dtype: DType\n](\n    output: LayoutTensor[mut=False, dtype, out_layout],\n    a: LayoutTensor[mut=False, dtype, in_layout],\n    b: LayoutTensor[mut=False, dtype, conv_layout],\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i  = thread_idx.x\n\n    # input slice + halo\n    shared_a = tb[dtype]().row_major[TPB + CONV_2 - 1]().shared().alloc()\n\n    # load kernel\n    shared_b = tb[dtype]().row_major[CONV_2]().shared().alloc()\n\n    if global_i &lt; SIZE_2:\n        # coalesced load of main slice\n        shared_a[local_i] = a[global_i]                  \n\n    # only first CONV_2 threads participate\n    if local_i &lt; CONV_2:\n        # load kernel into shared memory\n        shared_b[local_i] = b[local_i]                   \n\n    # threads responsible for halo load\n    if local_i &lt; CONV_2 - 1:\n        # element that lives in next block\n        var next_idx = global_i + TPB                    \n        # pad with zeros\n        shared_a[local_i + TPB] = a[next_idx] if next_idx &lt; SIZE_2 else 0.0\n\n    barrier()\n\n    # skip threads mapping past the end\n    if global_i &lt; SIZE_2:\n        var local_sum: output.element_type = 0.0\n\n        @parameter                                       \n        for j in range(CONV_2):                          \n            # dot product of window & kernel\n            local_sum += shared_a[local_i + j] * shared_b[j]\n        output[global_i] = local_sum\n\npixi run p11 --block-boundary\n# out: HostBuffer([14.0, 20.0, 26.0, 32.0, 38.0, 44.0, 50.0, 56.0, 62.0, 68.0, 74.0, 80.0, 41.0, 14.0, 0.0])\n# expected: HostBuffer([14.0, 20.0, 26.0, 32.0, 38.0, 44.0, 50.0, 56.0, 62.0, 68.0, 74.0, 80.0, 41.0, 14.0, 0.0])"
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#hillis-steele-algorithm",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#hillis-steele-algorithm",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Hillis-Steele Algorithm",
    "text": "Hillis-Steele Algorithm\nA straightforward parallel scan is the Hillis-Steele approach: at each distance d = 1, 2, 4, … every element adds in the value from d positions back. This is the same as the method shown in Puzzle 10\n# inclusive scan, power-of-two length\ndef hillis_steele_scan(a, ⊕):\n    n = len(a)\n    temp = a.copy()\n    d = 1\n    while d &lt; n:\n        for i in range(n):\n            temp[i] = a[i] if i &lt; d else a[i - d] ⊕ a[i]\n        a, temp = temp, a\n        d *= 2\n    return a\nIn Mojo, this looks as follows:\n\n\nSolution\n\n\n\np12_simple.mojo\n\nfn prefix_sum_simple[\n    layout: Layout\n](\n    output: LayoutTensor[mut=False, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    size: Int,\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i = thread_idx.x\n    for idx in range(Int(log2(Scalar[dtype](TPB)))):\n        if local_i &gt;= offset and local_i &lt; SIZE:\n            shared[local_i] += shared[local_i - offset]\n\n        barrier()\n        offset *= 2\n\n    if global_i &lt; SIZE:\n        output[global_i] = shared[local_i]\n\npixi run p12 --simple\n# out: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\n# expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\nEach of the log₂(n) rounds does up to n parallel additions (one per active element), so total work is \\(\\sum_k n = nlog(n)\\). Because rounds are serialized by barriers, the longest dependency chain is one add per round i.e \\(O(log n)\\)."
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#blellochs-twopass-algorithm",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#blellochs-twopass-algorithm",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Blelloch’s Two‐Pass Algorithm",
    "text": "Blelloch’s Two‐Pass Algorithm\nBlelloch’s two-pass scan does Θ(n) work by splitting the job into an up-sweep (build a reduction tree) and a down-sweep (propagate prefixes) [11].\nWhy prefer it over the classic Hillis-Steele (Algorithm 1)?\n\nHardware constraints: Hillis-Steele assumes one processor per element and updates the array in-place every round. A real GPU doesn’t grant that luxury: a “1024-thread” block actually runs in 32-thread warps that time-slice on the same SM. When warp 0 pauses and warp 1 resumes, in-place writes from one warp can overwrite data the other still needs.\nSynchronisation cost: Avoiding the overwrite requires a barrier after every addition - log₂(n) rounds × n threads ⇒ Θ(n log n) operations plus all those barriers.\n\nBlelloch’s fix to these problems is to break the up-sweep and down-sweep into separate phases:\n\nUp-sweep and down-sweep touch disjoint tree levels, so threads never trample each other within a phase.\nOnly two global barriers are needed (one between the phases, one at the end).\nNow you get Θ(n) work and correctness, even for arrays much bigger than a warp.\n\nThe result is a scan that is both faster and safer on modern GPUs.\n\nUp-sweep (reduce)\n\nBuild a binary reduction tree over log₂(n) rounds:\n\nRound 1 (step=1): sum each adjacent pair, storing results at indices 1, 3, 5, …\nRound 2 (step=2): merge those partial sums into blocks of 4, writing into indices 3, 7, 11, …\nContinue doubling the span each round until step = n/2\n\nAfter the final round, a[n-1] holds the overall total\n\n Up-Sweep: combining elements in a binary-tree fashion-build partial sums until the final element holds the total.\n\n\nDown-sweep (propagate)\nAfter the up-sweep leaves a[n-1] containing the overall sum, we walk the tree top-down to scatter prefix sums into every slot:\n\nInitialize the down-sweep with a window size of step = n/2.\n\nLoop as long as step &gt;= 1:\n\nPartition the array into blocks of size 2*step. For each block starting at index i:\n\nTemporarily store the left-child total from a[i + step - 1].\n\nOverwrite that left slot with the right-child subtotal from a[i + 2*step - 1].\n\nAdd the saved left-child total to the right slot, giving the correct prefix for that subtree.\n\n\nIssue a barrier() so all threads sync before shrinking the window.\n\nHalve the window: step = step / 2.\n\n\nWith each pass, the partial sums trickle down one level of the binary tree; after log₂(n) iterations every element holds its exclusive prefix sum.\n\n\nDown Sweep: siblings swap and accumulate, driving the scan from root back to leaves.\nTotal Operations: \\(\\Theta(n)\\), parallel depth: \\(\\Theta(\\log_2 n)\\).\n\n\nSolution (Blelloch up-sweep + down-sweep)\n\n\n\np12_blelloch.mojo\n\nfn prefix_sum_blelloch[\n    layout: Layout\n](\n    output:   LayoutTensor[mut=True, dtype, layout],\n    a:     LayoutTensor[mut=False, dtype, layout],\n    size:  Int,\n):\n    global_idx = block_idx.x*block_dim.x + thread_idx.x\n    local_idx = thread_idx.x\n    shared = tb[dtype]().row_major[SIZE]().shared().alloc()\n\n    if global_idx &lt; size:\n        shared[local_idx] = a[global_idx]\n    barrier()\n\n    # Up-sweep\n    var stride = 1\n    while stride &lt; size:\n        step = stride * 2\n        if (local_idx % step == step - 1) and (local_idx &lt; size):\n            shared[local_idx] += shared[local_idx - stride]\n        barrier()\n        stride = step\n\n    # Down-sweep\n    if local_idx == size - 1:\n        shared[local_idx] = 0\n    barrier()\n\n    var half = stride &gt;&gt; 1\n    while half &gt; 0:\n        step = half * 2\n        if (local_idx % step == step - 1) and (local_idx &lt; size):\n            t = shared[local_idx - half]\n            shared[local_idx - half] = shared[local_idx]\n            shared[local_idx] += t\n        barrier()\n        half = half &gt;&gt; 1\n\n    if global_idx &lt; size:\n        output[global_idx] = shared[local_idx] + a[global_idx]\n\npixi run p12 --blelloch\n# out: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\n# expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0])\n\nThis is not the most efficient implementation, but I hope this provides some intuition about the algorithm!"
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#block-boundary-1",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#block-boundary-1",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Block Boundary",
    "text": "Block Boundary\nThe key difference in this version is that now we have an input array that is larger than the size of a single block.\nWe split the global scan into two bite-sized passes:\n\nPhase 1 - Local Scan\n\nEach block copies its slice into shared memory.\n\nPerform an in-block naive scan/Blelloch scan exactly as in the single-block case.\n\nThe last thread of the block stashes the block’s total after the scan into an auxiliary slot at the tail of output:\n#  |&lt;---  SIZE_2  ---&gt;|&lt;-- #blocks --&gt;|\n#  [   prefix sums   ][ block totals ]\n\n\n\nPhase 2 - Propagate block totals\n\nEvery thread grabs the aggregate from the previous block (totals[block_id-1]) and adds it to its own prefix.\nNow every element holds the inclusive scan over the whole array.\n\n\nWe launch the above phases as two separate kernels.\nA host-side synchronisation sits between the launches. That call flushes the work queue and waits until Phase 1 has fully committed its writes to global memory, ensuring the per-block totals are complete and visible before Phase 2 starts consuming them. Skip the sync and the driver is free to overlap or reorder the kernels, letting Phase 2 read garbage.\n\n\nSolution (Block Boundary Version)\n\n\n\np12_block_boundary.mojo\n\nfn prefix_sum_local_phase[\n    out_layout: Layout, in_layout: Layout\n](\n    output: LayoutTensor[mut=False, dtype, out_layout],\n    a: LayoutTensor[mut=False, dtype, in_layout],\n    size: Int,\n):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    local_i = thread_idx.x\n    shared = tb[dtype]().row_major[EXTENDED_SIZE]().shared().alloc()\n\n    if global_i &lt; SIZE_2:\n        shared[local_i] = a[global_i]\n    \n    barrier()\n    offset = 1\n\n    for idx in range(Int(log2(Scalar[dtype](TPB)))):\n        if local_i &gt;= offset and local_i &lt; SIZE_2:\n            shared[local_i] += shared[local_i - offset]\n\n        barrier()\n        offset *= 2\n\n    if global_i &lt; SIZE_2:\n        output[global_i] = shared[local_i]\n    \n    if local_i == TPB - 1:\n        output[size + block_idx.x] += shared[local_i]\n\n\n# Kernel 2: Add block sums to their respective blocks\nfn prefix_sum_block_sum_phase[\n    layout: Layout\n](output: LayoutTensor[mut=False, dtype, layout], size: Int):\n    global_i = block_dim.x * block_idx.x + thread_idx.x\n    # FILL ME IN (roughly 3 lines)\n    if block_idx.x &gt; 0 and global_i &lt; size:\n        prev_block_sum = output[SIZE_2 + block_idx.x - 1]\n        output[global_i] += prev_block_sum\n\npixi run p12\n# out: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0, 36.0, 45.0, 55.0, 66.0, 78.0, 91.0, 105.0, 28.0, 77.0]) # last 2 elements are the block sums\n# expected: HostBuffer([0.0, 1.0, 3.0, 6.0, 10.0, 15.0, 21.0, 28.0, 36.0, 45.0, 55.0, 66.0, 78.0, 91.0, 105.0])"
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#global-memory-version",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#global-memory-version",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Global Memory Version",
    "text": "Global Memory Version\nBased on the 2D indexing section, each thread computes one C[row, col] by loading A[row, k] and B[k, col] from global memory, multiplying and accumulating over k. We unroll the k‐loop to cut loop overhead and boost throughput.\n\n\nSolution\n\n\n\np14_naive.mojo\n\nfn naive_matmul[\n    layout: Layout, size: Int\n](\n    output: LayoutTensor[mut=False, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    b: LayoutTensor[mut=False, dtype, layout],\n):\n    row = block_dim.y * block_idx.y + thread_idx.y\n    col = block_dim.x * block_idx.x + thread_idx.x\n\n    if row &lt; SIZE and col &lt; SIZE:\n        # Need this to ensure the mojo compiler knows\n        # the type of `running_sum`, otherwise it will\n        # complain\n        var running_sum: output.element_type = 0\n\n        @parameter\n        for k in range(SIZE):\n            running_sum += a[row, k] * b[k, col]\n        output[row, col] = running_sum\n\npixi run p14 --naive\n# out: HostBuffer([4.0, 6.0, 12.0, 22.0])\n# expected: HostBuffer([4.0, 6.0, 12.0, 22.0])"
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#shared-memory-version",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#shared-memory-version",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Shared Memory Version",
    "text": "Shared Memory Version\nThe previous version suffers from repeated global memory reads. We can optimize this using shared memory:\n\nLoad matrix tiles once\nSynchronize threads\nCompute using the cached data.\n\n\n\n\nMatmul with shared memory\n\n\n\n\nSolution\n\n\n\np14_shared.mojo\n\nfn single_block_matmul[\n    layout: Layout, size: Int\n](\n    output: LayoutTensor[mut=False, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    b: LayoutTensor[mut=False, dtype, layout],\n):\n    row = block_dim.y * block_idx.y + thread_idx.y\n    col = block_dim.x * block_idx.x + thread_idx.x\n    local_row = thread_idx.y\n    local_col = thread_idx.x\n    shared_a = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n    shared_b = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n    if row &lt; size and col &lt; size and local_row &lt; size and local_col &lt; size:\n        shared_a[local_row, local_col] = a[row, col]\n        shared_b[local_row, local_col] = b[row, col]\n\n    barrier()\n    if row &lt; size and col &lt; size and local_row &lt; size and local_col &lt; size:\n        var running_sum: output.element_type = 0.0\n\n        @parameter\n        for k in range(size):\n            running_sum += a[local_row, k] * b[k, local_col]\n\n        output[row, col] = running_sum\n\npixi run p14 --naive\n# out: HostBuffer([4.0, 6.0, 12.0, 22.0])\n# expected: HostBuffer([4.0, 6.0, 12.0, 22.0])\nThe Roofline Model offers a first-order answer to a GPU performance question: is my kernel limited by arithmetic throughput or by memory bandwidth?\nIt does so by plotting operational intensity (FLOPs per byte) against two ceilings - the hardware’s peak FLOP/s and peak DRAM bandwidth—so you can see at a glance which resource is the bottleneck."
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#roofline-model",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#roofline-model",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Roofline Model",
    "text": "Roofline Model\n\nNote: The Modular GPU Puzzles guide already walks through the full roofline derivation, but we’ll repeat it here so that you can follow along without leaving this post.\n\nThe first step is abstracting the hardware-software complexity into a tractable model.\n\nHardware Model\nClassic roofline assumes ideal hardware with perfect overlap:\n\n\n\nSource: NHR at FAU[13]\n\n\nThe cartoon GPU has only two levers:\n\nCompute engine — peak rate \\(P_{peak}\\) (FLOP/s, integer ops/s, etc.)\nMemory datapath — peak bandwidth \\(b_s\\) (bytes/s)\n\n\n\nSoftware Model\n\n\n\nSoftware abstraction: complex GPU kernel simplified to steady-state loop with N flops and V bytes per iteration. Credits: NHR at FAU[13]\n\n\nWe collapse the kernel’s steady-state loop to:\n\n\\(N\\) floating-point operations per iteration\n\\(V\\) bytes moved per iteration\n\nThe operational intensity is defined as:\n\\[I = \\frac{N}{V} \\text{ flop/byte}\\]\nThis ratio is all that survives of the algorithm - prologue/epilogue work, control flow, and synchronizations are swept aside.\nHardware Assumptions:\n\n\n\n\n\n\n\n\n\n\n#\nAssumption\nWorks because…\nReality\nBreaks when…\n\n\n\n\nH1\nPeak DRAM bandwidth is reachable\nIdeal streaming\nRequires 100% streaming, &gt;1MB tiles\nStrided or tiny tiles\n\n\nH2\nPeak FLOP/s reachable\nFull FMA rate\nAll ALUs busy every cycle\nDivergence, low occupancy\n\n\nH3\nOne bandwidth number is enough\nDRAM dominates\nL1/L2/SMEM add separate roofs\nLower-level choke points\n\n\n\nSoftware Assumptions:\n\n\n\n\n\n\n\n\n\n\n#\nAssumption\nWorks because…\nReality\nBreaks when…\n\n\n\n\nS1\nLoads fully hide latency\n1000s inflight warps\nRequires deep pipelining\nShort kernels, frequent syncs\n\n\nS2\nSingle operational intensity\nSteady-state loop\nReal kernels mix phases\nGather/scatter, epilogue code\n\n\nS3\nLaunch/transfer overhead small\nLong kernel runs\nAmortised over many iterations\nMicro-benchmarks, chaining\n\n\n\n\n\nNaive Roofline Model\nWith these assumptions, hardware and software collapse to one parameter—the operational intensity \\(I\\)—and attainable performance becomes\n\\[\n\\begin{aligned}\nP(I) &= \\min\\!\\bigl(P_{\\text{peak}},\\, I\\,b_s\\bigr) \\\\\nI_{\\text{crit}} &= \\frac{P_{\\text{peak}}}{b_s}\n\\end{aligned}\n\\]\nAt the critical intensity \\(I_{crit}\\), the bandwidth and compute roofs intersect, splitting kernels into two classes:\n\nMemory-bound (\\(I &lt; I_{crit}\\)) -&gt; Performance rises linearly with \\(I\\)\nCompute-bound (\\(I \\geq I_{crit}\\)) -&gt; Performance plateaus at \\(P_{peak}\\)\n\n\n\n\nRoofline model: sloped red line shows memory bandwidth limit, flat blue line is compute peak, kernel’s operational intensity marked as a dot.\n\n\n\n\nWhere the Roofline Model Fails\nEven in small puzzle kernels, these assumptions falter. In real workloads, they break down completely.\nWhat actually works:\n\nMeasure real limits with tools like Nsight or rocprof\nRedraw the roofline using measured ceilings—L2 roof, Tensor-core roof, not just DRAM and peak FLOPs\nAdjust your kernel: boost \\(I\\) (tiling, shared memory, tensor ops) or raise the ceilings (improve occupancy, reduce stalls)\n\n\nUnfortunately no Nsight eye-candy as of yet - my ncu setup hit a permissions wall. I’ll fix it and share a profiler deep-dive soon. Stay tuned!\n\nThe textbook roofline is a guide, not reality. Measure, adapt, and push your kernel as close to the real limits as you can."
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#roofline-estimation",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#roofline-estimation",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Roofline Estimation",
    "text": "Roofline Estimation\nLet’s apply the roofline model to a 3×3 matrix multiplication, which is still small enough to hand-calculate.\nThe RTX 4000 Ada provides[14]:\n\nPeak compute: 26.7 TFLOPS (single-precision)\n\nPeak DRAM bandwidth: 360 GB/s\n\nCritical intensity: \\(I_{crit} = \\frac{26.7 \\times 10^{12}}{360 \\times 10^9} = 74.2\\) FLOP/byte\n\n\nNaive MatMul Analysis\nFor \\(C = A \\times B\\) where all matrices are 3×3:\n\nCompute work\n\nEach output element is a dot product of length 3\n3 fused multiply-adds -&gt; 3 FLOPs per output element\n9 elements -&gt; 27 FLOPs total\n\n\n\nDRAM traffic\n\nLoad matrix A: 9 floats × 4 bytes = 36 bytes\nLoad matrix B: 9 floats × 4 bytes = 36 bytes\n\nStore matrix C: 9 floats × 4 bytes = 36 bytes\nTotal: 108 bytes\n\n\n\nOperational intensity:\n\\[I_{naive} = \\frac{27 \\text{ FLOPs}}{108 \\text{ bytes}} = 0.25 \\text{ FLOP/byte}\\]\nSince \\(I_{naive} = 0.25 \\ll I_{crit} = 74.2\\), this kernel is memory-bound.\n\n\nPredicted performance\n\\[\n\\begin{aligned}\nP_{naive} \\;\\; & = \\min(26.7~\\text{TFLOPS},\\; 0.25 \\times 360~\\text{GB/s}) \\\\\n               & = \\min(26.7~\\text{TFLOPS},\\; 90~\\text{GFLOPS}) \\\\\n               & = \\boxed{90~\\text{GFLOPS}}\n\\end{aligned}\n\\]\n\n\n\nShared Memory Optimization\nBy staging 3×3 tiles of A and B in shared memory, each element feeds all three required dot products instead of being fetched repeatedly from DRAM.\n\nImproved traffic pattern\n\nDRAM loads for A and B drop by ~3×\nStores remain unchanged (36 bytes)\nApproximate traffic: \\((36+36)/3 + 36 = 60\\) bytes\n\n\n\nNew operational intensity\n\\[I_{shared} = \\frac{27 \\text{ FLOPs}}{60 \\text{ bytes}} = 0.45 \\text{ FLOP/byte}\\]\n\n\nPredicted performance\n\\[\n\\begin{aligned}\nP_{shared} \\;\\; & = \\min(26.7~\\text{TFLOPS},\\; 0.45 \\times 360~\\text{GB/s}) \\\\\n                & = \\min(26.7~\\text{TFLOPS},\\; 162~\\text{GFLOPS}) \\\\\n                & = \\boxed{162~\\text{GFLOPS}}\n\\end{aligned}\n\\]\nThis gives us a 1.8× speedup from shared memory optimization, but we’re still memory-bound.\n\n\n\nRTX 4000 Ada Roofline for Matmul\n\n\n\n\nPlot Code\n\n\n\nroofline_plot.py\n\n# /// script\n# dependencies = [\n#   \"matplotlib\",\n#   \"numpy\",\n# ]\n# ///\n\n# Run using uv run roofline_plot.py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npeak_compute = 26.7 * 1000  # Convert to GFLOPS\npeak_bandwidth = 360  # GB/s\ncritical_intensity = peak_compute / peak_bandwidth\n\nkernels = [\n    {\"name\": \"Naive 3×3\", \"intensity\": 0.25, \"performance\": 90, \"color\": \"#f39c12\"},\n    {\"name\": \"Shared 3×3\", \"intensity\": 0.45, \"performance\": 162, \"color\": \"#27ae60\"}\n]\n\ndef generate_roofline_data():\n    # Memory-bound region\n    memory_intensities = np.arange(0.01, critical_intensity, 0.05)\n    memory_performance = memory_intensities * peak_bandwidth\n\n    # Compute-bound region\n    compute_intensities = np.arange(critical_intensity, 1000, 5)\n    compute_performance = np.full_like(compute_intensities, peak_compute)\n\n    return memory_intensities, memory_performance, compute_intensities, compute_performance\n\nfig, ax = plt.subplots(figsize=(10, 6.5))\n\nax.set_xscale('log', base=10)\nax.set_yscale('log', base=10)\n\nmem_i, mem_p, comp_i, comp_p = generate_roofline_data()\n\nax.loglog(mem_i, mem_p, color=\"#e74c3c\", linewidth=3, label=\"Memory-bound\")\nax.loglog(comp_i, comp_p, color=\"#3498db\", linewidth=3, label=\"Compute-bound\")\n\nax.axhline(y=peak_compute, color=\"#3498db\", linestyle=\"--\", alpha=0.7, linewidth=1)\nax.axvline(x=critical_intensity, color=\"#999\", linestyle=\"--\", alpha=0.7, linewidth=1)\n\nfor kernel in kernels:\n    ax.loglog(kernel[\"intensity\"], kernel[\"performance\"],\n             'o', color=kernel[\"color\"], markersize=8,\n             markeredgecolor=\"#333\", markeredgewidth=2)\n\n    # Add kernel labels with better positioning to avoid overlap\n    if kernel[\"name\"] == \"Naive 3×3\":\n        offset_x = kernel[\"intensity\"] * 0.7  # Move left\n        offset_y = kernel[\"performance\"] * 0.65  # Move down\n    else:  # Shared 3×3\n        offset_x = kernel[\"intensity\"] * 1.4  # Move right\n        offset_y = kernel[\"performance\"] * 1.3  # Move up\n\n    ax.annotate(kernel[\"name\"],\n               (kernel[\"intensity\"], kernel[\"performance\"]),\n               xytext=(offset_x, offset_y),\n               fontsize=11, fontweight=\"bold\", ha=\"center\",\n               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n\nax.text(1.5, peak_bandwidth * 1.5, \"Memory-bound\",\n        color=\"#e74c3c\", fontsize=14, fontweight=\"bold\", ha=\"center\")\nax.text(200, peak_compute * 0.8, \"Compute-bound\",\n        color=\"#3498db\", fontsize=14, fontweight=\"bold\")\n\nax.text(critical_intensity * 1.3, peak_compute * 1.1,\n        f\"I_crit = {critical_intensity:.1f}\",\n        color=\"#f39c12\", fontsize=12, fontweight=\"bold\")\n\nax.set_xlim(0.1, 1000)\nax.set_ylim(10, 30000)\nax.set_xlabel(\"Operational Intensity (FLOP/byte) - Log_10 Scale\", fontsize=12)\nax.set_ylabel(\"Performance (GFLOP/s) - Log_10 Scale\", fontsize=12)\nax.grid(True, alpha=0.3)\n\nlegend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n                             markerfacecolor=k[\"color\"], markersize=8,\n                             label=k[\"name\"], markeredgecolor=\"#333\")\n                  for k in kernels]\nax.legend(handles=legend_elements, loc=\"lower right\")\n\nplt.tight_layout()\nplt.savefig('./mojo_gpu_puzzles/p14_roofline_naive_and_shared.png', bbox_inches='tight')\n\nprint(\"Performance Analysis:\")\nprint(\"-\" * 40)\nfor kernel in kernels:\n    efficiency = (kernel[\"performance\"] / peak_compute) * 100\n    print(f\"{kernel['name']}:\")\n    print(f\"  Intensity: {kernel['intensity']} FLOP/byte\")\n    print(f\"  Performance: {kernel['performance']} GFLOP/s\")\n    print(f\"  Efficiency: {efficiency:.1f}% of peak\")\n\n\n\n\n\nKey Insights\n\nIntensity grows with matrix size - For naive \\(N \\times N\\) GEMM: \\(I = \\frac{N^3}{4N^2} = \\frac{N}{4}\\) FLOP/byte\nSmall kernels are bandwidth-bound - Even perfect caching can’t reach the 74 FLOP/byte crossover until \\(N \\approx 300\\)\nShared memory helps, but only up to the ridge - Further speedups require compute-side tuning (tensor cores, ILP, etc.)\n\nNext, we’ll look at one specific optimisation for Matmul: Tile-based GEMM!"
  },
  {
    "objectID": "posts/2025-07-20-gpu-puzzles-p2.html#tiled-matrix-multiplication-gemm",
    "href": "posts/2025-07-20-gpu-puzzles-p2.html#tiled-matrix-multiplication-gemm",
    "title": "GPUs go brrr with Mojo: Algorithms",
    "section": "Tiled Matrix-Multiplication (GEMM)",
    "text": "Tiled Matrix-Multiplication (GEMM)\nOur shared-memory kernel already cut global-DRAM traffic by loading each A[i,k] / B[k,j] element once per thread row/column instead of once per output multiply.\nFor large matrices, however, even that version still:\n\nBrings the entire row of A and column of B into shared SRAM, quickly exhausting the 48–112 KiB available per SM.\nLeaves many threads idle while others finish their portion of the dot-product.\nMisses an opportunity to keep a hot, register-resident accumulator and hide global-latency behind computation.\n\nEnter tiling / blocking—the canonical GPU GEMM strategy.\n\nTile?\nThink of an N×N matmul as a chessboard. Instead of letting every thread wander across the whole board, we slice it into T×T sub-squares (tiles).\nA thread-block is assigned one output tile, and:\n\nCooperatively loads the matching T×T A-tile and B-tile from global DRAM to shared memory (two coalesced 2-D memcpy’s).\nPerforms T fused-multiply-add sweeps of that data, each thread keeping its running sum in a register.\nBarriers, slides the tile window by T along the inner-k dimension, and repeats until the dot-product is complete.\nFinally writes the T×T block of C back to DRAM.\n\n\n\n\nMatmul Operation for a 4x4 Matrix, computing the first 4 output elements. Credits: Simon Oz\n\n\nEach element of A/B is now read once per tile—independent of N—and re-used T times, boosting arithmetic intensity from O(1) to O(T) FLOP/B.\n\n\nMemory mapping for tiled GEMM\nThe memory hierachy(discussed in the previous post), is utilised as follows:\n\nRegisters: per-thread accumulators that hold partial C values across all tile iterations\nShared SRAM: the current A_tile and B_tile, cooperatively loaded once and reused T times\n\nGlobal HBM: original A, B matrices and final C; each element touched once per tile load/store\n\n\n\nRaw Memory\n\n\nManual Indexing Tiled Matmul\n\n\n\np14_matmul_tiled_manual.mojo\n\n\nalias SIZE_TILED = 8 # Size of the matrix we are multiplying, NOT the size of a tile\nalias BLOCKS_PER_GRID_TILED = (3, 3)  # each block convers 3x3 elements\nalias THREADS_PER_BLOCK_TILED = (TPB, TPB)\nalias layout_tiled = Layout.row_major(SIZE_TILED, SIZE_TILED)\n\n\nfn matmul_tiled[\n    layout: Layout, size: Int\n](\n    output: LayoutTensor[mut=False, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    b: LayoutTensor[mut=False, dtype, layout],\n):\n    local_row = thread_idx.y\n    local_col = thread_idx.x\n    global_row = block_idx.y * TPB + local_row\n    global_col = block_idx.x * TPB + local_col\n\n    shared_a = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n    shared_b = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n\n    var local_sum: output.element_type = 0.0\n\n    @parameter\n    # (size + TPB - 1) // TPB == ceil(size / TPB) -&gt; number of tile-steps we need\n    for tile in range((size + TPB - 1) // TPB):\n        # Load elements of A into shared mem\n        if global_row &lt; size and (tile * TPB + local_col) &lt; size:\n            shared_a[local_row, local_col] = a[\n                global_row, tile * TPB + local_col\n            ]\n\n        # Load elements of B into shared mem\n        if global_col &lt; size and (tile * TPB + local_row) &lt; size:\n            shared_b[local_row, local_col] = b[\n                tile * TPB + local_row, global_col\n            ]\n\n        barrier()\n\n        # Perform matmul\n        if global_row &lt; size and global_col &lt; size:\n\n            @parameter\n            for k in range(min(TPB, size - tile * TPB)):\n                local_sum += shared_a[local_row, k] * shared_b[k, local_col]\n\n            barrier()\n\n        if global_row &lt; size and global_col &lt; size:\n            output[global_row, global_col] = local_sum\n\npixi run p14 --tiled\n# out: HostBuffer([2240.0, 2296.0, 2352.0, 2408.0, 2464.0, 2520.0, 2576.0, 2632.0, 5824.0, 6008.0, 6192.0, 6376.0, 6560.0, 6744.0, 6928.0, 7112.0, 9408.0, 9720.0, 10032.0, 10344.0, 10656.0, 10968.0, 11280.0, 11592.0, 12992.0, 13432.0, 13872.0, 14312.0, 14752.0, 15192.0, 15632.0, 16072.0, 16576.0, 17144.0, 17712.0, 18280.0, 18848.0, 19416.0, 19984.0, 20552.0, 20160.0, 20856.0, 21552.0, 22248.0, 22944.0, 23640.0, 24336.0, 25032.0, 23744.0, 24568.0, 25392.0, 26216.0, 27040.0, 27864.0, 28688.0, 29512.0, 27328.0, 28280.0, 29232.0, 30184.0, 31136.0, 32088.0, 33040.0, 33992.0])\n# expected: HostBuffer([2240.0, 2296.0, 2352.0, 2408.0, 2464.0, 2520.0, 2576.0, 2632.0, 5824.0, 6008.0, 6192.0, 6376.0, 6560.0, 6744.0, 6928.0, 7112.0, 9408.0, 9720.0, 10032.0, 10344.0, 10656.0, 10968.0, 11280.0, 11592.0, 12992.0, 13432.0, 13872.0, 14312.0, 14752.0, 15192.0, 15632.0, 16072.0, 16576.0, 17144.0, 17712.0, 18280.0, 18848.0, 19416.0, 19984.0, 20552.0, 20160.0, 20856.0, 21552.0, 22248.0, 22944.0, 23640.0, 24336.0, 25032.0, 23744.0, 24568.0, 25392.0, 26216.0, 27040.0, 27864.0, 28688.0, 29512.0, 27328.0, 28280.0, 29232.0, 30184.0, 31136.0, 32088.0, 33040.0, 33992.0])\n\nThe new formulas in the tiling implementation deserve explanation. Let’s break them down into key concepts:\n\nHow many Tiles are needed?\nThis is the expression: range((size + TPB - 1) // TPB)\nThe key idea here is: Step through k by TPB each time; if there’s a leftover chunk, do one last tile for it.\n\n\n\nMapping of elements to tiles when size=17 and TPB=8\n\n\nThe above example needs 3 tiles. Ceiling division captures this with a simple formula: \\[\n\\lceil\\frac{size}{TPB}\\rceil = \\lfloor\\frac{size + TPB - 1}{TPB}\\rfloor\n\\]\n\n\nWhich element does a thread fetch in this tile?\nEach thread brings in one A value and one B value. Thread indices inside the block:\nHere, tile is the “which-chunk-of-k” loop counter.\nInside a block each thread is responsible for one output element C[global_row, global_col].\nFor that element you need every pair (A[row, k], B[k, col]) as k runs.\nFor tile t:\nA: a[global_row, t*TPB + local_col]\nB: b[t*TPB + local_row, global_col]\n\n\n\nTiled 9×9 matmul: Each thread loads 3x3 A and 3x3 B elements per tile, computes a partial sum, then syncs. Tile size = 3x3, 9 threads per block.\n\n\nThe two loads align on the same k slice (t*TPB … t*TPB+TPB-1), ensuring every multiply in this tile has operands in shared memory.\n\n\nWhy do we swap local_row and local_col for B?\nGPUs coalesce global memory when adjacent threads read adjacent addresses. With the swap:\n\nFor A: neighboring threads in x-direction (local_col) read consecutive k’s ⇒ coalesced\nFor B: neighboring threads in y-direction (local_row) read consecutive k’s ⇒ also coalesced\n\nWithout the swap, one matrix would be fetched “strided” collapsing into 32 separate memory transactions per warp - a 32× slowdown on bandwidth-bound kernels.\nQuick primer: Shared memory isn’t one monolithic block. It’s chopped into 32 independent “banks”[15] [16].\n\n\n\nShared memory banking: conflict-free access (left) vs bank conflicts (right). When multiple threads access different addresses in the same bank, hardware serializes the requests. Source: CUDA Programming Blogspot\n\n\nEach is a tiny SRAM with its own read/write port that can service one request (or one 32-bit access per cycle). A warp hits peak bandwidth only when every thread lands in a different bank (or all hit the same address, which hardware can broadcast). If two threads target different addresses inside the same bank during the same cycle, the hardware must serialize them, referred to as a bank conflict.\nBeyond coalescing, our tile layout also sidesteps these conflicts. Because b_shared[k, threadIdx.x] maps each thread to a distinct bank (while a_shared[threadIdx.y, k] is broadcast-friendly), all 32 memory ports stay busy with zero serialization.\n\n\nChoosing the Right Tile Size\nWhile the current puzzle selects \\(TPB=3\\) with tile size \\(TPBxTPB\\), choosing the tile size is a balancing act.\nExact numbers vary with GPU, kernel, and precision [[17]][18].\nI’m still learning the dark art of GPU perf tuning, so I’ll save the details for a future post once I’ve had more time to experiment.\nTLDR: For each tile, we will sync (barrier), compute, shift to next tile, repeat. But this is just the baseline - there’s always a deeper optimization rabbit hole!\n\n\n\nLayoutTensor\nWhile the manual tiling approach works, it suffers from indexing complexity that obscures the algorithm’s intent and creates opportunities for bugs. Mojo’s LayoutTensor API provides an elegant solution that maintains performance while dramatically improving code clarity.\n\nThe Pain of Manual Indexing\nThe manual implementation requires careful coordinate arithmetic:\n\nNested index calculations like tile * TPB + local_col that can easily introduce off-by-one errors\nSeparate bounds checking for each matrix load operation\nExplicit management of tile boundaries and edge cases\nCode that prioritizes performance over readability\n\nLayoutTensor provides a tile() method that creates zero-copy [7] views into sub-regions of tensors [19]. This eliminates manual indexing gymnastics while keeping identical performance.\nA LayoutTensor.tile[tile_height, tile_width](block_row, block_col) call returns a view of the specified tile without copying data, at no cost!\nThe transformation from manual indexing to LayoutTensor simplifies the loading logic:\n# Load elements of A into shared mem\n- if global_row &lt; size and (tile * TPB + local_col) &lt; size:\n-     shared_a[local_row, local_col] = a[\n-         global_row, tile * TPB + local_col\n-     ]\n- \n- # Load elements of B into shared mem  \n- if global_col &lt; size and (tile * TPB + local_row) &lt; size:\n-     shared_b[local_row, local_col] = b[\n-         tile * TPB + local_row, global_col\n-     ]\n# Create tile views (zero-copy)\n+ a_tile = a.tile[TPB, TPB](block_idx.y, idx)\n+ b_tile = b.tile[TPB, TPB](idx, block_idx.x)\n\n# Asynchronous copy to shared memory\n+ copy_dram_to_sram_async[thread_layout=load_a_layout](a_shared, a_tile)\n+ copy_dram_to_sram_async[thread_layout=load_b_layout](b_shared, b_tile)\n\n# Synchronize all async copies\n+ async_copy_wait_all()\nFull solution looks as follows:\n\n\nLayoutTensor Tiled Matmul\n\n\n\np14_matmul_layout_tensor.mojo\n\nalias SIZE_TILED = 9\nalias BLOCKS_PER_GRID_TILED = (3, 3)  # each block covers 3x3 elements\nalias THREADS_PER_BLOCK_TILED = (TPB, TPB)\nalias layout_tiled = Layout.row_major(SIZE_TILED, SIZE_TILED)\n\n\nfn matmul_tiled[\n    layout: Layout, size: Int\n](\n    output: LayoutTensor[mut=True, dtype, layout],\n    a: LayoutTensor[mut=False, dtype, layout],\n    b: LayoutTensor[mut=False, dtype, layout],\n):\n    # LayoutTensor APIs\n    out_tile = output.tile[TPB, TPB](block_idx.y, block_idx.x)\n    a_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n    b_shared = tb[dtype]().row_major[TPB, TPB]().shared().alloc()\n    local_row = thread_idx.y\n    local_col = thread_idx.x\n\n    var local_sum: output.element_type = 0.0\n\n    alias load_a_layout = Layout.row_major[1, TPB]()\n    alias load_b_layout = Layout.row_major[TPB, 1]()\n    \n    @parameter\n    for idx in range((size + TPB - 1) // TPB):\n        a_tile = a.tile[TPB, TPB](block_idx.y, idx)\n        b_tile = b.tile[TPB, TPB](idx, block_idx.x)\n\n        copy_dram_to_sram_async[thread_layout=load_a_layout](a_shared, a_tile)\n        copy_dram_to_sram_async[thread_layout=load_b_layout](b_shared, b_tile)\n\n        async_copy_wait_all()\n\n        @parameter\n        for k in range(min(TPB, size - idx * TPB)):\n            local_sum += a_shared[local_row, k] * b_shared[k, local_col]\n\n        barrier()\n\n    # Store result after all tiles processed\n    if (\n        block_idx.y * TPB + local_row &lt; size\n        and block_idx.x * TPB + local_col &lt; size\n    ):\n        out_tile[local_row, local_col] = local_sum\n\n\n\n\nSynchronization and Memory Hierarchy\nThe copy_dram_to_sram_async() function [20] enables asynchronous memory transfers from global to shared memory, while async_copy_wait_all() [21] provides a synchronization barrier that ensures all pending transfers complete before computation proceeds.\nThis pattern allows the GPU to:\n\nOverlap memory transfers with other computations using dedicated copy engines\nUtilize specialized hardware for efficient data movement\nMaintain correct execution ordering across thread blocks\nBypass intermediate registers for improved memory hierarchy efficiency\n\nImportant: async_copy_wait_all() only synchronizes the asynchronous copy operations—threads still need explicit barriers (barrier()) to ensure all threads in a block see the shared memory data before computation begins."
  }
]